{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search patterns right in TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/master/Search_patterns_right_in_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MAIN"
      ]
    },
    {
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ { output-height: 800, form-width: \"300px\", display-mode: \"form\" }\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"protocols-2\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "def _import_elmo():\n",
        "  \"\"\"\n",
        "  ACHTUNG!! this method is called later by ElmoEmbedder\n",
        "  \"\"\"\n",
        "\n",
        "  elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz',\n",
        "                    trainable=False)  # news\n",
        "  #   elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-twitter_2013-01_2018-04_600k_steps.tar.gz',\n",
        "  #                     trainable=False)  # twitter\n",
        "  print('‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è DONE (re)importing Tensorflow hub.Module ')\n",
        "  print('Tensorflow version is', tf.__version__)\n",
        "\n",
        "  return elmo\n",
        "\n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(_import_elmo(), tf, 'elmo', _import_elmo)\n",
        "\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "# AZ:-Init chartes context-----------------------------------------------------------------------------------\n",
        "def _init_charters():\n",
        "  if 'CharterAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Charters-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  _init_embedder()  # PRECONDITION\n",
        "  from charter_patterns import CharterPatternFactory\n",
        "  from charter_parser import CharterDocumentParser\n",
        "  CPF = CharterPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "  GLOBALS__['CharterAnlysingContext'] = CharterDocumentParser(CPF)\n",
        "  print('‚ù§Ô∏è DONE initing Charters-related tools and models ')\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from transaction_values import ValueConstraint\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  import matplotlib.pyplot as plt\n",
        "  \n",
        "  from renderer import AbstractRenderer, HtmlRenderer, head_types_colors\n",
        "  from renderer import to_multicolor_text, as_headline_3, as_offset\n",
        "  from renderer import as_msg, as_quote, as_c_quote\n",
        "  from renderer import as_error_html, known_subjects_dict, v_color_map\n",
        "  from transaction_values import ValueConstraint\n",
        "  from parsing import head_types_dict, head_types\n",
        "  from legal_docs import PatternSearchResults, ConstraintsSearchResult, PatternSearchResult, CharterDocument\n",
        "  \n",
        "  import numpy as np\n",
        "  \n",
        "  from charter_patterns import known_subjects\n",
        "  from patterns import AV_SOFT, AV_PREFIX\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  def _as_smaller(txt):\n",
        "    return f'<div font-size:12px\">{txt}</div>'\n",
        " \n",
        "  \n",
        "  import html as escaper\n",
        "# >>> html.escape('x > 2 && x < 7')\n",
        "      \n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      print('render_color_text')\n",
        "      html = self.to_color_text( [escaper.escape(t) for t in tokens ] , weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "                \n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      if len(tokens) == 0:\n",
        "        return \" - empty -\"\n",
        "      if len(weights) != len(tokens):\n",
        "        raise ValueError(\"number of weights differs weights={} tokens={}\".format(len(weights), len(tokens)))\n",
        "\n",
        "      #   if()\n",
        "      vmin = weights.min()\n",
        "      vmax = weights.max()\n",
        "\n",
        "      if _range is not None:\n",
        "        vmin = _range[0]\n",
        "        vmax = _range[1]\n",
        "\n",
        "      if print_debug:\n",
        "        print(vmin, vmax)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmin=vmin - 0.5, vmax=vmax)\n",
        "      html = \"\"\n",
        "      cmap = mpl.cm.get_cmap(colormap)\n",
        "\n",
        "      for d in range(0, len(weights)):\n",
        "        word = tokens[d]\n",
        "        if word == ' ':\n",
        "          word = '&nbsp;_ '\n",
        "\n",
        "        html += '<span title=\"{} {:.4f}\" style=\"background-color:{}\">{} </span>'.format(\n",
        "          d,\n",
        "          weights[d],\n",
        "          mpl.colors.to_hex(cmap(norm(weights[d]))),\n",
        "          word)\n",
        "\n",
        "        #     html+='<span style=\"background-color:' +mpl.colors.to_hex(cmap(norm(weights[d]) ))+ '\">' + str(tokens[d]) + \" </span>\"\n",
        "        if tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    ''' AZ:-Rendering CHARITYüî•-----üí∏------üí∏-------üí∏------------------------------'''\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "    \n",
        "\n",
        "    ''' AZ:------üí∏------üí∏-------üí∏----------------------END--Rendering CHARITYüî•------'''\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    def render_subj(self, contract: ContractDocument3):\n",
        "      subjects: List[ProbableValue] = contract.subjects\n",
        "\n",
        "      if len(subjects) > 0:\n",
        "        sorted_ = [y for y in sorted(subjects, key=lambda x: -x.confidence)]\n",
        "        subject_kind = sorted_[0].value\n",
        "        confidence = sorted_[0].confidence\n",
        "      else:\n",
        "        subject_kind = ContractSubject.Other\n",
        "\n",
        "      if subject_kind in known_subjects_dict:\n",
        "        rendering_name = known_subjects_dict[subject_kind]\n",
        "      else:\n",
        "        rendering_name = '–ø—Ä–æ—á–µ–µ'\n",
        "\n",
        "      display(\n",
        "        HTML(f'–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:'\n",
        "             f'<h3 style=\"margin:0\">{rendering_name}<sup> {subject_kind}</sup> </h3> '\n",
        "             f'confidence:{confidence:20,.2f}'))\n",
        "\n",
        "\n",
        "\n",
        "    def sign_to_text(self, sign: int):\n",
        "      if sign < 0: return \" &lt; \"\n",
        "      if sign > 0: return \" &gt; \"\n",
        "      return ' = '\n",
        "\n",
        "    def probable_value_to_html(self, pv):\n",
        "      vc = pv.value\n",
        "      color = '#333333'\n",
        "      if vc.sign > 0:\n",
        "        color = '#993300'\n",
        "      elif vc.sign < 0:\n",
        "        color = '#009933'\n",
        "\n",
        "      return f'<b style=\"color:{color}\">{self.sign_to_text(vc.sign)} {vc.currency} {vc.value:20,.2f}' \\\n",
        "             f'<sup>confidence={pv.confidence:20,.2f}</sup></b> '\n",
        "\n",
        "    def render_contents(self, doc):\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for i in doc.structure.headline_indexes:\n",
        "        line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li> {line} <sup>line {i}</sup></li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_sections(self, sections):\n",
        "      from legal_docs import HeadlineMeta\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for section_type in sections:\n",
        "        section: HeadlineMeta = sections[section_type]\n",
        "        body = section.body.untokenize_cc()[:1000]\n",
        "        headline = section.subdoc.untokenize_cc()[:500]\n",
        "        #     line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li><h3> {headline} <sup>type: {section_type}</sup> </h3> <p>{body}</p> </li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_values(self, values):\n",
        "      if len(values) > 0:\n",
        "        for pv in values:\n",
        "          h = self.probable_value_to_html(pv)\n",
        "          display(HTML(h))\n",
        "      else:\n",
        "        display(HTML('—Å—É–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'))\n",
        "\n",
        "    def render_value_section_details(self, value_section_info):\n",
        "      value_section = value_section_info.body\n",
        "      headline_doc = value_section_info.subdoc\n",
        "\n",
        "      headline = headline_doc.untokenize_cc()\n",
        "\n",
        "      v_names = {\n",
        "        'value_attention_vector',\n",
        "        'novalue_attention_vector',\n",
        "\n",
        "        'novalue_attention_vector_local_contrast',\n",
        "        'value_attention_vector_tuned'}\n",
        "\n",
        "      fig = plt.figure(figsize=(20, 6))\n",
        "      ax = plt.axes()\n",
        "      for vector_name in v_names:\n",
        "        ax.plot(value_section.distances_per_pattern_dict[vector_name], label=vector_name, alpha=0.4)\n",
        "\n",
        "      ax.plot(value_section.distances_per_pattern_dict['value_attention_vector_tuned'], label='value_attention result',\n",
        "              alpha=0.9, color='black')\n",
        "      plt.legend(loc='upper right')\n",
        "\n",
        "      text = self.to_color_text(value_section.tokens_cc,\n",
        "                                value_section.distances_per_pattern_dict['value_attention_vector_tuned'], _range=(0, 1))\n",
        "      html = f'{ as_headline_3(headline)} <div style=\"margin-left:4em; font-size=90%\">{text}</div>'\n",
        "      display(HTML(html))\n",
        "\n",
        "     \n",
        "    def render_charter_parsing_results_2(self, charter):\n",
        "      display(HTML(self.charter_parsing_results_to_html(charter)))\n",
        "      \n",
        "    def render_charter_parsing_results(self, doc, org, rz, charity_constraints):\n",
        "      WARN = '\\033[1;31m======== Dear Artem, ACHTUNG! üîû '\n",
        "      print (WARN+f\"use {self.render_charter_parsing_results} is deprecated\")\n",
        "      txt_html = self.to_color_text(org['tokens'], org['attention_vector'], _range=[0, 1])\n",
        "\n",
        "      html = '<div style=\"background:#eeeeff; padding:0.5em\"> recognized NE(s): <br><br> org type:<h3 style=\"margin:0\">  {} </h3>org full name:<h2 style=\"margin:0\">  {} </h2> <br>quote: <div style=\"font-size:90%; background:white\">{}</div> </div>'.format(\n",
        "        org['type_name'], org['name'], txt_html)\n",
        "      # html+=txt_html\n",
        "      html += self.render_constraint_values(doc, rz, charity_constraints)\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        " \n",
        "\n",
        "     \n",
        "\n",
        "    \n",
        " \n",
        "\n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  # AZ:----------PROTOCOLS RENDERER-------------------------\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  from renderer import as_headline_3, as_headline_4\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "\n",
        "  class ProtocolRenderer(DemoRenderer):\n",
        "\n",
        "    def winning_patterns_to_html(self, _tokens, ranges, winning_patterns, _range,\n",
        "                                 colormaps=['Reds', 'Purples', 'Blues', 'Greens', 'Greys']):\n",
        "      vmin = -ranges[1]\n",
        "      vmax = -ranges[0]\n",
        "\n",
        "      #     print(\"winning_patterns_to_html _range\", _range, \"min max=\", ranges)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmax=vmax, vmin=vmin)\n",
        "\n",
        "      cmaps = []\n",
        "\n",
        "      #     print (colormaps)\n",
        "      for n in colormaps:\n",
        "        cmap = mpl.cm.get_cmap(n)\n",
        "        cmaps.append(cmap)\n",
        "\n",
        "      html = \"\"\n",
        "\n",
        "      for d in _range:\n",
        "        winning_pattern_i = winning_patterns[d][0]\n",
        "        colormap = cmaps[winning_pattern_i % len(colormaps)]\n",
        "        normed = norm(-winning_patterns[d][1])\n",
        "        color = mpl.colors.to_hex(colormap(normed))\n",
        "        html += '<span title=\"' + '{} {:.2f}'.format(d, winning_patterns[d][\n",
        "          1]) + '\" style=\"background-color:' + color + '\">' + str(\n",
        "          _tokens[d]) + \" </span>\"\n",
        "        if _tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    def _render_doc_subject_fragments(self, doc):\n",
        "      #     print(doc.per_subject_distances)\n",
        "\n",
        "      _html = \"\"\n",
        "      if doc.per_subject_distances is not None:\n",
        "\n",
        "        type = \"–î–æ–≥–æ–≤–æ—Ä  –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\"\n",
        "        if doc.per_subject_distances[0] > doc.per_subject_distances[1]:\n",
        "          type = \"–î–æ–≥–æ–≤–æ—Ä –≤–æ–∑–º–µ–∑–¥–Ω–æ–≥–æ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥\"\n",
        "\n",
        "        _html += \"<h3>\" + type + \"</h3>\"\n",
        "\n",
        "        colormaps = ['PuRd'] * 5 + ['Blues'] * 7 + ['Greys']\n",
        "\n",
        "        _html += as_headline_4('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞')\n",
        "\n",
        "        for region in [doc.subj_range]:\n",
        "          _html += self.winning_patterns_to_html(_tokens=doc.tokens, ranges=doc.subj_ranges,\n",
        "                                                 winning_patterns=doc.winning_subj_patterns, _range=region,\n",
        "                                                 colormaps=colormaps)\n",
        "\n",
        "      return _html\n",
        "\n",
        "    def render_subject(self, counter):\n",
        "      html = as_headline_3('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (X):') + self.subject_type_weights_to_html(counter)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def print_results(self, _doc: LegalDocument, results=None):\n",
        "      values = sorted(_doc.values, key=lambda item: -item.confidence)\n",
        "\n",
        "      if values is None or len(values)==0:\n",
        "        display(HTML(as_warning('—Å—É–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')))\n",
        "        return \n",
        "\n",
        "\n",
        "      cc = 0  \n",
        "      for vl in values:\n",
        "\n",
        "        h = as_headline_3(self.probable_value_to_html(vl))\n",
        "        h += as_offset(self.to_color_text(vl.value.context.tokens, vl.value.context.attention, colormap='jet'))\n",
        "        if cc > 0:\n",
        "          h = as_offset(as_smaller(h))\n",
        "\n",
        "        display(HTML(h))\n",
        "\n",
        "        cc += 1\n",
        "    \n",
        "    def subject_type_weights_to_html(self, counter):\n",
        "      dict = {\n",
        "        't_dea': '–°–¥–µ–ª–∫–∞',\n",
        "        't_cha': '–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',\n",
        "        't_org': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è'\n",
        "      }\n",
        "\n",
        "      maxkey = \"None\"\n",
        "      for key in dict:\n",
        "        if counter[key] > counter[maxkey]:\n",
        "          maxkey = key\n",
        "\n",
        "      html = \"\"\n",
        "      for key in dict:\n",
        "        templ = \"<div>{}: {}</div>\"\n",
        "        if key == maxkey:\n",
        "          templ = '<b style=\"font-size:135%; color:maroon\">{}: {}</b>'\n",
        "        html += templ.format(counter[key], dict[key])\n",
        "\n",
        "      return html\n",
        "\n",
        "  GLOBALS__['ProtocolRenderer'] = ProtocolRenderer()\n",
        "\n",
        "  from demo_protocols import ProtocolAnlysingContext\n",
        "  GLOBALS__['ProtocolAnlysingContext'] = ProtocolAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['ProtocolRenderer'])\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Charters context====\n",
        "\n",
        "  def read_doc(fn):\n",
        "    import docx2txt, sys, os\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text\n",
        "\n",
        "  GLOBALS__['read_doc'] = read_doc\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "\n",
        "# AZ:-FINDING_VIOLATIONS--------------------------------------------------------\n",
        "def find_and_show_violations():\n",
        "  from IPython.core.display import display, HTML\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  from renderer import as_headline_2, as_error_html\n",
        "\n",
        "  print('–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π')\n",
        "\n",
        "  charterAnlysingContext: CharterAnlysingContext = GLOBALS__['CharterAnlysingContext']\n",
        "  contractAnlysingContext: ContractAnlysingContext = GLOBALS__['ContractAnlysingContext']\n",
        "\n",
        "  contract = contractAnlysingContext.contract\n",
        "  charter = charterAnlysingContext.doc\n",
        "  charter_constraints = charterAnlysingContext.constraints  # XXX: move to doc\n",
        "\n",
        "  renderer = GLOBALS__['renderer']\n",
        "  renderer.render_subj(contract)\n",
        "\n",
        "  import copy\n",
        "\n",
        "  def convert(v):\n",
        "    v_converted = copy.copy(v)\n",
        "    if v.currency in currency_converter:\n",
        "      v_converted.value = currency_converter[v.currency] * v.value\n",
        "      v_converted.currency = 'RUB'\n",
        "      return v_converted\n",
        "    else:\n",
        "      display(HTML(as_error_html(\n",
        "        f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "      return v\n",
        "\n",
        "  best_value = contractAnlysingContext.find_contract_best_value(convert)\n",
        "\n",
        "  # rendering:----------------------------\n",
        "\n",
        "  def _render_violations(ranges_by_group, best_value):\n",
        "    for group_key in ranges_by_group:\n",
        "      group = ranges_by_group[group_key]\n",
        "      display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "      for rk in group['ranges']:\n",
        "        r = group['ranges'][rk]\n",
        "        display(HTML(r.check_contract_value(best_value, convert, renderer)))\n",
        "\n",
        "  print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "  renderer.render_values([best_value])\n",
        "  renderer.render_color_text(best_value.value.context.tokens, best_value.value.context.attention, _range=[0, 1])\n",
        "\n",
        "  _render_violations(\n",
        "    charterAnlysingContext.find_ranges_by_group(charter_constraints, convert, verbose=False),\n",
        "    best_value)\n",
        "\n",
        "#   display(HTML(renderer.render_constraint_values(charter_constraints)))\n",
        "\n",
        "\n",
        "# AZ:--------------------------------------------------------FINDING_VIOLATIONS-\n",
        "\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhjI5YF61cpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 0. –ò–Ω–∏—Ç "
      ]
    },
    {
      "metadata": {
        "id": "6BnrLv2k1iVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## do preparation here\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        "# 2.\n",
        "_init_embedder()\n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n",
        "elmo=_import_elmo()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MSrrvHTtAlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TF\n"
      ]
    },
    {
      "metadata": {
        "id": "Xn3B8tQ6xwsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prepare patterns"
      ]
    },
    {
      "metadata": {
        "id": "hsnkO-zZxvn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "patterns_tokens = [\n",
        "    \"–º—ã–ª –ö—Ä–∏—à–Ω—É\".split(' '), \n",
        "    \"–º—ã–ª\".split(' '), \n",
        "    '–≤ –ï–¥–∏–Ω–æ–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–µ–µ—Å—Ç—Ä–µ'.split(' ')]\n",
        "\n",
        "patterns_tokens_lens=[len(x) for x in patterns_tokens]\n",
        "maxlen =  max( patterns_tokens_lens)\n",
        "\n",
        "#padding:\n",
        "patterns_tokens_ext=[ x+[' ']*(maxlen-len(x)) for x in patterns_tokens ]\n",
        "print(patterns_tokens_ext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEEXXURPxz-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prepare model\n",
        "\n",
        "1. populate array of indexes: token index X pattern index, pattern length "
      ]
    },
    {
      "metadata": {
        "id": "0nGrg6los9_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "def build_everything(words, patterns_tokens, patterns_tokens_lens_):\n",
        "  TOKENS_N = len(words[0])\n",
        "  PATTERN_N = len(patterns_tokens)\n",
        "\n",
        "  padding = max(patterns_tokens_lens_) - 1\n",
        "  words_with_padding = words[0] + [''] * padding\n",
        "\n",
        "  indexes = []\n",
        "  # Populate array of indexes: token index X pattern index, pattern length \n",
        "  for pi in range(PATTERN_N):\n",
        "    for ti in range(TOKENS_N):\n",
        "      indexes.append(np.array([ti, pi, patterns_tokens_lens_[pi]]))\n",
        "\n",
        "  indexes = np.array(indexes, dtype=np.int32)\n",
        "\n",
        "\n",
        "\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  with tf.Session(config=config) as sess:\n",
        "\n",
        "    IDX = tf.constant(indexes)\n",
        "\n",
        "\n",
        "    # 1. text embedding\n",
        "    text_embeddings = elmo(\n",
        "      inputs={\n",
        "        \"tokens\": [words_with_padding],\n",
        "        \"sequence_len\": [len(words_with_padding)]\n",
        "      },\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)[\"elmo\"][0]\n",
        "\n",
        "    \n",
        "    # 2. patterns embedding\n",
        "    patterns_embeddings = elmo(\n",
        "      inputs={\n",
        "        \"tokens\": patterns_tokens,\n",
        "        \"sequence_len\": patterns_tokens_lens_\n",
        "      },\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)[\"elmo\"]\n",
        "\n",
        "    \n",
        "    # 3. calc distances per pattern    \n",
        "    def do_at_index(emb_pat_len):\n",
        "      \n",
        "      # emb_pat_len = [ token index, pattern index, pattern length ]\n",
        "      \n",
        "      token_index =  emb_pat_len[0] #for each token\n",
        "      pattern_indx = emb_pat_len[1] #for each pattern\n",
        "      pattern_len =  emb_pat_len[2]\n",
        "\n",
        "      pattern_at_index = patterns_embeddings[pattern_indx] #getting pattern embeddings at index\n",
        "      pattern_at_index = pattern_at_index[0:pattern_len] #trimming its embedding to the original length of the pattern\n",
        "      #TODO: for better ELMO embedding pattern may have prefix and postfix, remove prefix and postfix\n",
        "\n",
        "      p_center = tf.reduce_mean(pattern_at_index, axis=0) #calc mean embedding vector for the pattern\n",
        "\n",
        "      slice_end = token_index + pattern_len\n",
        "\n",
        "      e_center = tf.reduce_mean(text_embeddings[token_index:slice_end], axis=0) #calc mean embedding vector for the window in the words embeddings    \n",
        "\n",
        "      p_center = tf.nn.l2_normalize(p_center, 0) # normalizing is kinda required if we want cosine return [0..1] range \n",
        "      e_center = tf.nn.l2_normalize(e_center, 0) # DO WE?\n",
        "\n",
        "      return tf.losses.cosine_distance(e_center, p_center, axis=0) ##TODO: how on Earth Cosine could be > 1????\n",
        "\n",
        "    cosine_distances = tf.map_fn(lambda i: do_at_index(i), IDX, dtype=tf.float32)\n",
        "    cosine_similarities = 1.0 - cosine_distances\n",
        "    # --------------------------------------------\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    cosine_similarities_by_pattern = tf.reshape(cosine_similarities, [PATTERN_N, -1])\n",
        "    out = sess.run(cosine_similarities_by_pattern)\n",
        "#   del elmo\n",
        "  return out\n",
        "\n",
        "##test\n",
        "# test_tokens = [\"–ú–∞–º–∞ –º—ã–ª–∞ –†–∞–º—É, –†–∞–º–∞ –º—ã–ª –ö—Ä–∏—à–Ω—É, –ö—Ä–∏—à–Ω–∞ –º—ã–ª –ë—Ä–∞—Ö–º—É. –ë—Ä–∞—Ö–º–∞ —á–∏—Å—Ç. –í –æ–∫–Ω–µ –∑–∞–∫—Ä—ã—Ç–æ–º –Ω–µ—Ç —Ç–µ–±—è\".split(' ')]\n",
        "# out = build_everything(test_tokens, patterns_tokens_ext, patterns_tokens_lens)\n",
        "# tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbvfU97hGrUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = interactive_upload('Protocol')[0]\n",
        "\n",
        "\n",
        "\n",
        "from text_tools import tokenize_text\n",
        "\n",
        "patterns_tokens = [\n",
        "    tokenize_text(\"–∫—Ç–æ-—Ç–æ –∏–º–µ–Ω—É–µ–º—ã–π –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å\"), \n",
        "    tokenize_text(\"–∑–∞–∫–ª—é—á–∏–ª ' ' –¥–æ–≥–æ–≤—Ä\"), \n",
        "    tokenize_text('–≤ –ï–¥–∏–Ω–æ–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–µ–µ—Å—Ç—Ä–µ')]\n",
        "\n",
        "patterns_tokens_lens=[len(x) for x in patterns_tokens]\n",
        "maxlen =  max( patterns_tokens_lens)\n",
        "\n",
        "#padding:\n",
        "patterns_tokens_ext=[ x+[' ']*(maxlen-len(x)) for x in patterns_tokens ]\n",
        "print(patterns_tokens_ext)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TOKENS=tokenize_text(uploaded)\n",
        "out = build_everything([TOKENS], patterns_tokens_ext, patterns_tokens_lens)\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text( TOKENS, out[1]+out[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}