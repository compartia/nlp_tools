{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search patterns right in TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/master/Search_patterns_right_in_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MAIN"
      ]
    },
    {
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "ec7cf725-1a3a-4b1a-e3f7-64a1964c4b43"
      },
      "cell_type": "code",
      "source": [
        "# @title –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ { output-height: 800, form-width: \"300px\", display-mode: \"form\" }\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"protocols-2\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "def _import_elmo():\n",
        "  \"\"\"\n",
        "  ACHTUNG!! this method is called later by ElmoEmbedder\n",
        "  \"\"\"\n",
        "\n",
        "  elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz',\n",
        "                    trainable=False)  # news\n",
        "  #   elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-twitter_2013-01_2018-04_600k_steps.tar.gz',\n",
        "  #                     trainable=False)  # twitter\n",
        "  print('‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è DONE (re)importing Tensorflow hub.Module ')\n",
        "  print('Tensorflow version is', tf.__version__)\n",
        "\n",
        "  return elmo\n",
        "\n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(_import_elmo(), tf, 'elmo', _import_elmo)\n",
        "\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "# AZ:-Init chartes context-----------------------------------------------------------------------------------\n",
        "def _init_charters():\n",
        "  if 'CharterAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Charters-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  _init_embedder()  # PRECONDITION\n",
        "  from charter_patterns import CharterPatternFactory\n",
        "  from charter_parser import CharterDocumentParser\n",
        "  CPF = CharterPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "  GLOBALS__['CharterAnlysingContext'] = CharterDocumentParser(CPF)\n",
        "  print('‚ù§Ô∏è DONE initing Charters-related tools and models ')\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from transaction_values import ValueConstraint\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  import matplotlib.pyplot as plt\n",
        "  \n",
        "  from renderer import AbstractRenderer, HtmlRenderer, head_types_colors\n",
        "  from renderer import to_multicolor_text, as_headline_3, as_offset\n",
        "  from renderer import as_msg, as_quote, as_c_quote\n",
        "  from renderer import as_error_html, known_subjects_dict, v_color_map\n",
        "  from transaction_values import ValueConstraint\n",
        "  from parsing import head_types_dict, head_types\n",
        "  from legal_docs import PatternSearchResults, ConstraintsSearchResult, PatternSearchResult, CharterDocument\n",
        "  \n",
        "  import numpy as np\n",
        "  \n",
        "  from charter_patterns import known_subjects\n",
        "  from patterns import AV_SOFT, AV_PREFIX\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  def _as_smaller(txt):\n",
        "    return f'<div font-size:12px\">{txt}</div>'\n",
        " \n",
        "  \n",
        "  import html as escaper\n",
        "# >>> html.escape('x > 2 && x < 7')\n",
        "      \n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      print('render_color_text')\n",
        "      html = self.to_color_text( [escaper.escape(t) for t in tokens ] , weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "                \n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      if len(tokens) == 0:\n",
        "        return \" - empty -\"\n",
        "      if len(weights) != len(tokens):\n",
        "        raise ValueError(\"number of weights differs weights={} tokens={}\".format(len(weights), len(tokens)))\n",
        "\n",
        "      #   if()\n",
        "      vmin = weights.min()\n",
        "      vmax = weights.max()\n",
        "\n",
        "      if _range is not None:\n",
        "        vmin = _range[0]\n",
        "        vmax = _range[1]\n",
        "\n",
        "      if print_debug:\n",
        "        print(vmin, vmax)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmin=vmin - 0.5, vmax=vmax)\n",
        "      html = \"\"\n",
        "      cmap = mpl.cm.get_cmap(colormap)\n",
        "\n",
        "      for d in range(0, len(weights)):\n",
        "        word = tokens[d]\n",
        "        if word == ' ':\n",
        "          word = '&nbsp;_ '\n",
        "\n",
        "        html += '<span title=\"{} {:.4f}\" style=\"background-color:{}\">{} </span>'.format(\n",
        "          d,\n",
        "          weights[d],\n",
        "          mpl.colors.to_hex(cmap(norm(weights[d]))),\n",
        "          word)\n",
        "\n",
        "        #     html+='<span style=\"background-color:' +mpl.colors.to_hex(cmap(norm(weights[d]) ))+ '\">' + str(tokens[d]) + \" </span>\"\n",
        "        if tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    ''' AZ:-Rendering CHARITYüî•-----üí∏------üí∏-------üí∏------------------------------'''\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "    \n",
        "\n",
        "    ''' AZ:------üí∏------üí∏-------üí∏----------------------END--Rendering CHARITYüî•------'''\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    def render_subj(self, contract: ContractDocument3):\n",
        "      subjects: List[ProbableValue] = contract.subjects\n",
        "\n",
        "      if len(subjects) > 0:\n",
        "        sorted_ = [y for y in sorted(subjects, key=lambda x: -x.confidence)]\n",
        "        subject_kind = sorted_[0].value\n",
        "        confidence = sorted_[0].confidence\n",
        "      else:\n",
        "        subject_kind = ContractSubject.Other\n",
        "\n",
        "      if subject_kind in known_subjects_dict:\n",
        "        rendering_name = known_subjects_dict[subject_kind]\n",
        "      else:\n",
        "        rendering_name = '–ø—Ä–æ—á–µ–µ'\n",
        "\n",
        "      display(\n",
        "        HTML(f'–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:'\n",
        "             f'<h3 style=\"margin:0\">{rendering_name}<sup> {subject_kind}</sup> </h3> '\n",
        "             f'confidence:{confidence:20,.2f}'))\n",
        "\n",
        "\n",
        "\n",
        "    def sign_to_text(self, sign: int):\n",
        "      if sign < 0: return \" &lt; \"\n",
        "      if sign > 0: return \" &gt; \"\n",
        "      return ' = '\n",
        "\n",
        "    def probable_value_to_html(self, pv):\n",
        "      vc = pv.value\n",
        "      color = '#333333'\n",
        "      if vc.sign > 0:\n",
        "        color = '#993300'\n",
        "      elif vc.sign < 0:\n",
        "        color = '#009933'\n",
        "\n",
        "      return f'<b style=\"color:{color}\">{self.sign_to_text(vc.sign)} {vc.currency} {vc.value:20,.2f}' \\\n",
        "             f'<sup>confidence={pv.confidence:20,.2f}</sup></b> '\n",
        "\n",
        "    def render_contents(self, doc):\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for i in doc.structure.headline_indexes:\n",
        "        line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li> {line} <sup>line {i}</sup></li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_sections(self, sections):\n",
        "      from legal_docs import HeadlineMeta\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for section_type in sections:\n",
        "        section: HeadlineMeta = sections[section_type]\n",
        "        body = section.body.untokenize_cc()[:1000]\n",
        "        headline = section.subdoc.untokenize_cc()[:500]\n",
        "        #     line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li><h3> {headline} <sup>type: {section_type}</sup> </h3> <p>{body}</p> </li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_values(self, values):\n",
        "      if len(values) > 0:\n",
        "        for pv in values:\n",
        "          h = self.probable_value_to_html(pv)\n",
        "          display(HTML(h))\n",
        "      else:\n",
        "        display(HTML('—Å—É–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'))\n",
        "\n",
        "    def render_value_section_details(self, value_section_info):\n",
        "      value_section = value_section_info.body\n",
        "      headline_doc = value_section_info.subdoc\n",
        "\n",
        "      headline = headline_doc.untokenize_cc()\n",
        "\n",
        "      v_names = {\n",
        "        'value_attention_vector',\n",
        "        'novalue_attention_vector',\n",
        "\n",
        "        'novalue_attention_vector_local_contrast',\n",
        "        'value_attention_vector_tuned'}\n",
        "\n",
        "      fig = plt.figure(figsize=(20, 6))\n",
        "      ax = plt.axes()\n",
        "      for vector_name in v_names:\n",
        "        ax.plot(value_section.distances_per_pattern_dict[vector_name], label=vector_name, alpha=0.4)\n",
        "\n",
        "      ax.plot(value_section.distances_per_pattern_dict['value_attention_vector_tuned'], label='value_attention result',\n",
        "              alpha=0.9, color='black')\n",
        "      plt.legend(loc='upper right')\n",
        "\n",
        "      text = self.to_color_text(value_section.tokens_cc,\n",
        "                                value_section.distances_per_pattern_dict['value_attention_vector_tuned'], _range=(0, 1))\n",
        "      html = f'{ as_headline_3(headline)} <div style=\"margin-left:4em; font-size=90%\">{text}</div>'\n",
        "      display(HTML(html))\n",
        "\n",
        "     \n",
        "    def render_charter_parsing_results_2(self, charter):\n",
        "      display(HTML(self.charter_parsing_results_to_html(charter)))\n",
        "      \n",
        "    def render_charter_parsing_results(self, doc, org, rz, charity_constraints):\n",
        "      WARN = '\\033[1;31m======== Dear Artem, ACHTUNG! üîû '\n",
        "      print (WARN+f\"use {self.render_charter_parsing_results} is deprecated\")\n",
        "      txt_html = self.to_color_text(org['tokens'], org['attention_vector'], _range=[0, 1])\n",
        "\n",
        "      html = '<div style=\"background:#eeeeff; padding:0.5em\"> recognized NE(s): <br><br> org type:<h3 style=\"margin:0\">  {} </h3>org full name:<h2 style=\"margin:0\">  {} </h2> <br>quote: <div style=\"font-size:90%; background:white\">{}</div> </div>'.format(\n",
        "        org['type_name'], org['name'], txt_html)\n",
        "      # html+=txt_html\n",
        "      html += self.render_constraint_values(doc, rz, charity_constraints)\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        " \n",
        "\n",
        "     \n",
        "\n",
        "    \n",
        " \n",
        "\n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  # AZ:----------PROTOCOLS RENDERER-------------------------\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  from renderer import as_headline_3, as_headline_4\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "\n",
        "  class ProtocolRenderer(DemoRenderer):\n",
        "\n",
        "    def winning_patterns_to_html(self, _tokens, ranges, winning_patterns, _range,\n",
        "                                 colormaps=['Reds', 'Purples', 'Blues', 'Greens', 'Greys']):\n",
        "      vmin = -ranges[1]\n",
        "      vmax = -ranges[0]\n",
        "\n",
        "      #     print(\"winning_patterns_to_html _range\", _range, \"min max=\", ranges)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmax=vmax, vmin=vmin)\n",
        "\n",
        "      cmaps = []\n",
        "\n",
        "      #     print (colormaps)\n",
        "      for n in colormaps:\n",
        "        cmap = mpl.cm.get_cmap(n)\n",
        "        cmaps.append(cmap)\n",
        "\n",
        "      html = \"\"\n",
        "\n",
        "      for d in _range:\n",
        "        winning_pattern_i = winning_patterns[d][0]\n",
        "        colormap = cmaps[winning_pattern_i % len(colormaps)]\n",
        "        normed = norm(-winning_patterns[d][1])\n",
        "        color = mpl.colors.to_hex(colormap(normed))\n",
        "        html += '<span title=\"' + '{} {:.2f}'.format(d, winning_patterns[d][\n",
        "          1]) + '\" style=\"background-color:' + color + '\">' + str(\n",
        "          _tokens[d]) + \" </span>\"\n",
        "        if _tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    def _render_doc_subject_fragments(self, doc):\n",
        "      #     print(doc.per_subject_distances)\n",
        "\n",
        "      _html = \"\"\n",
        "      if doc.per_subject_distances is not None:\n",
        "\n",
        "        type = \"–î–æ–≥–æ–≤–æ—Ä  –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\"\n",
        "        if doc.per_subject_distances[0] > doc.per_subject_distances[1]:\n",
        "          type = \"–î–æ–≥–æ–≤–æ—Ä –≤–æ–∑–º–µ–∑–¥–Ω–æ–≥–æ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥\"\n",
        "\n",
        "        _html += \"<h3>\" + type + \"</h3>\"\n",
        "\n",
        "        colormaps = ['PuRd'] * 5 + ['Blues'] * 7 + ['Greys']\n",
        "\n",
        "        _html += as_headline_4('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞')\n",
        "\n",
        "        for region in [doc.subj_range]:\n",
        "          _html += self.winning_patterns_to_html(_tokens=doc.tokens, ranges=doc.subj_ranges,\n",
        "                                                 winning_patterns=doc.winning_subj_patterns, _range=region,\n",
        "                                                 colormaps=colormaps)\n",
        "\n",
        "      return _html\n",
        "\n",
        "    def render_subject(self, counter):\n",
        "      html = as_headline_3('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (X):') + self.subject_type_weights_to_html(counter)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def print_results(self, _doc: LegalDocument, results=None):\n",
        "      values = sorted(_doc.values, key=lambda item: -item.confidence)\n",
        "\n",
        "      if values is None or len(values)==0:\n",
        "        display(HTML(as_warning('—Å—É–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')))\n",
        "        return \n",
        "\n",
        "\n",
        "      cc = 0  \n",
        "      for vl in values:\n",
        "\n",
        "        h = as_headline_3(self.probable_value_to_html(vl))\n",
        "        h += as_offset(self.to_color_text(vl.value.context.tokens, vl.value.context.attention, colormap='jet'))\n",
        "        if cc > 0:\n",
        "          h = as_offset(as_smaller(h))\n",
        "\n",
        "        display(HTML(h))\n",
        "\n",
        "        cc += 1\n",
        "    \n",
        "    def subject_type_weights_to_html(self, counter):\n",
        "      dict = {\n",
        "        't_dea': '–°–¥–µ–ª–∫–∞',\n",
        "        't_cha': '–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',\n",
        "        't_org': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è'\n",
        "      }\n",
        "\n",
        "      maxkey = \"None\"\n",
        "      for key in dict:\n",
        "        if counter[key] > counter[maxkey]:\n",
        "          maxkey = key\n",
        "\n",
        "      html = \"\"\n",
        "      for key in dict:\n",
        "        templ = \"<div>{}: {}</div>\"\n",
        "        if key == maxkey:\n",
        "          templ = '<b style=\"font-size:135%; color:maroon\">{}: {}</b>'\n",
        "        html += templ.format(counter[key], dict[key])\n",
        "\n",
        "      return html\n",
        "\n",
        "  GLOBALS__['ProtocolRenderer'] = ProtocolRenderer()\n",
        "\n",
        "  from demo_protocols import ProtocolAnlysingContext\n",
        "  GLOBALS__['ProtocolAnlysingContext'] = ProtocolAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['ProtocolRenderer'])\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Charters context====\n",
        "\n",
        "  def read_doc(fn):\n",
        "    import docx2txt, sys, os\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text\n",
        "\n",
        "  GLOBALS__['read_doc'] = read_doc\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "\n",
        "# AZ:-FINDING_VIOLATIONS--------------------------------------------------------\n",
        "def find_and_show_violations():\n",
        "  from IPython.core.display import display, HTML\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  from renderer import as_headline_2, as_error_html\n",
        "\n",
        "  print('–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π')\n",
        "\n",
        "  charterAnlysingContext: CharterAnlysingContext = GLOBALS__['CharterAnlysingContext']\n",
        "  contractAnlysingContext: ContractAnlysingContext = GLOBALS__['ContractAnlysingContext']\n",
        "\n",
        "  contract = contractAnlysingContext.contract\n",
        "  charter = charterAnlysingContext.doc\n",
        "  charter_constraints = charterAnlysingContext.constraints  # XXX: move to doc\n",
        "\n",
        "  renderer = GLOBALS__['renderer']\n",
        "  renderer.render_subj(contract)\n",
        "\n",
        "  import copy\n",
        "\n",
        "  def convert(v):\n",
        "    v_converted = copy.copy(v)\n",
        "    if v.currency in currency_converter:\n",
        "      v_converted.value = currency_converter[v.currency] * v.value\n",
        "      v_converted.currency = 'RUB'\n",
        "      return v_converted\n",
        "    else:\n",
        "      display(HTML(as_error_html(\n",
        "        f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "      return v\n",
        "\n",
        "  best_value = contractAnlysingContext.find_contract_best_value(convert)\n",
        "\n",
        "  # rendering:----------------------------\n",
        "\n",
        "  def _render_violations(ranges_by_group, best_value):\n",
        "    for group_key in ranges_by_group:\n",
        "      group = ranges_by_group[group_key]\n",
        "      display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "      for rk in group['ranges']:\n",
        "        r = group['ranges'][rk]\n",
        "        display(HTML(r.check_contract_value(best_value, convert, renderer)))\n",
        "\n",
        "  print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "  renderer.render_values([best_value])\n",
        "  renderer.render_color_text(best_value.value.context.tokens, best_value.value.context.attention, _range=[0, 1])\n",
        "\n",
        "  _render_violations(\n",
        "    charterAnlysingContext.find_ranges_by_group(charter_constraints, convert, verbose=False),\n",
        "    best_value)\n",
        "\n",
        "#   display(HTML(renderer.render_constraint_values(charter_constraints)))\n",
        "\n",
        "\n",
        "# AZ:--------------------------------------------------------FINDING_VIOLATIONS-\n",
        "\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0427 00:45:54.436706 140421857974144 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YhjI5YF61cpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 0. –ò–Ω–∏—Ç "
      ]
    },
    {
      "metadata": {
        "id": "6BnrLv2k1iVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1135
        },
        "outputId": "65992d20-fc21-4164-ff9a-fc2f065bcb18"
      },
      "cell_type": "code",
      "source": [
        "## do preparation here\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        "# 2.\n",
        "_init_embedder()\n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n",
        "elmo=_import_elmo()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fetching code from GitHub.....protocols-2\n",
            "\n",
            "ü¶ä GIT revision:\n",
            "386\n",
            "* protocols-2\n",
            "Created using Colaboratory\n",
            "Created using Colaboratory\n",
            "Add found_sum property for compatibility\n",
            "\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "code imported OK üëç\n",
            "installing antiword...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  antiword\n",
            "0 upgraded, 1 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 128 kB of archives.\n",
            "After this operation, 633 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 antiword amd64 0.37-11build1 [128 kB]\n",
            "Fetched 128 kB in 0s (718 kB/s)\n",
            "Selecting previously unselected package antiword.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../antiword_0.37-11build1_amd64.deb ...\n",
            "Unpacking antiword (0.37-11build1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Setting up antiword (0.37-11build1) ...\n",
            "\n",
            "installing docx2txt...\n",
            "Collecting docx2txt\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/02/74bdcdad3f8840cb0192010e02fd086bb37ca79bfacdda4112e3dcfb7af0/docx2txt-0.7.tar.gz\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py): started\n",
            "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/93/ec/aad2da3de09080ee7eea600d6ed1df763748ef70b099a134ed\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.7\n",
            "\n",
            "‚ù§Ô∏è DONE importing Code fro GitHub\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0427 00:46:22.456096 140421857974144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è DONE (re)importing Tensorflow hub.Module \n",
            "Tensorflow version is 1.13.1\n",
            "‚ù§Ô∏è DONE creating words embedding model\n",
            "<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x7fb604399cf8>\n",
            "‚ù§Ô∏è DONE initializing the code\n",
            "‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è DONE (re)importing Tensorflow hub.Module \n",
            "Tensorflow version is 1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7MSrrvHTtAlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TF\n"
      ]
    },
    {
      "metadata": {
        "id": "Xn3B8tQ6xwsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prepare patterns"
      ]
    },
    {
      "metadata": {
        "id": "hsnkO-zZxvn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "f560986e-63a2-4033-d343-11d066118ed8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "patterns_tokens = [\n",
        "    \"–º—ã–ª –ö—Ä–∏—à–Ω—É\".split(' '), \n",
        "    \"–º—ã–ª\".split(' '), \n",
        "    '–≤ –ï–¥–∏–Ω–æ–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–µ–µ—Å—Ç—Ä–µ'.split(' ')]\n",
        "\n",
        "patterns_tokens_lens=[len(x) for x in patterns_tokens]\n",
        "maxlen =  max( patterns_tokens_lens)\n",
        "\n",
        "#padding:\n",
        "patterns_tokens_ext=[ x+[' ']*(maxlen-len(x)) for x in patterns_tokens ]\n",
        "print(patterns_tokens_ext)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['–º—ã–ª', '–ö—Ä–∏—à–Ω—É', ' ', ' '], ['–º—ã–ª', ' ', ' ', ' '], ['–≤', '–ï–¥–∏–Ω–æ–º', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º', '—Ä–µ–µ—Å—Ç—Ä–µ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jEEXXURPxz-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prepare model\n",
        "\n",
        "1. populate array of indexes: token index X pattern index, pattern length "
      ]
    },
    {
      "metadata": {
        "id": "0nGrg6los9_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# returs\n",
        "from text_tools import Tokens\n",
        "\n",
        "\n",
        "def build_everything(words, patterns_tokens:List[Tokens], patterns_tokens_lens_:List[int]):\n",
        "  \"\"\"\n",
        "  \n",
        "  :param words: tokenized text\n",
        "  :param patterns_tokens: list of list\n",
        "  :param patterns_tokens_lens_: \n",
        "  :return: matrix of shape [M , N] where M is the number of patterns, and N is the number of words  \n",
        "  \"\"\"\n",
        "  TOKENS_N = len(words[0])\n",
        "  PATTERN_N = len(patterns_tokens)\n",
        "\n",
        "  padding = max(patterns_tokens_lens_) - 1\n",
        "  words_with_padding = words[0] + ['\\n'] * padding\n",
        "\n",
        "  indexes = []\n",
        "  # Populate array of indexes: token index X pattern index, pattern length \n",
        "  for pi in range(PATTERN_N):\n",
        "    for ti in range(TOKENS_N):\n",
        "      indexes.append(np.array([ti, pi, patterns_tokens_lens_[pi]]))\n",
        "\n",
        "  indexes = np.array(indexes, dtype=np.int32)\n",
        "\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  with tf.Session(config=config) as sess:\n",
        "\n",
        "    IDX = tf.constant(indexes)\n",
        "\n",
        "    # 1. text embedding\n",
        "    text_embeddings = elmo(\n",
        "      inputs={\n",
        "        \"tokens\": [words_with_padding],\n",
        "        \"sequence_len\": [len(words_with_padding)]\n",
        "      },\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)[\"elmo\"][0]\n",
        "\n",
        "    # 2. patterns embedding\n",
        "    patterns_embeddings = elmo(\n",
        "      inputs={\n",
        "        \"tokens\": patterns_tokens,\n",
        "        \"sequence_len\": patterns_tokens_lens_\n",
        "      },\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)[\"elmo\"]\n",
        "\n",
        "    # 3. calc distances per pattern    \n",
        "    def do_at_index(emb_pat_len):\n",
        "      # emb_pat_len = [ token index, pattern index, pattern length ]\n",
        "\n",
        "      token_index = emb_pat_len[0]  # for each token\n",
        "      pattern_indx = emb_pat_len[1]  # for each pattern\n",
        "      pattern_len = emb_pat_len[2]\n",
        "\n",
        "      pattern_at_index = patterns_embeddings[pattern_indx]  # getting pattern embeddings at index\n",
        "      pattern_at_index = pattern_at_index[0:pattern_len]  # trimming its embedding to the original length of the pattern\n",
        "      # TODO: for better ELMO embedding pattern may have prefix and postfix, remove prefix and postfix\n",
        "\n",
        "      p_center = tf.reduce_mean(pattern_at_index, axis=0)  # calc mean embedding vector for the pattern\n",
        "      #todo reduce sum or reduce mean\n",
        "\n",
        "      slice_end = token_index + pattern_len\n",
        "\n",
        "      e_center = tf.reduce_mean(text_embeddings[token_index:slice_end],\n",
        "                                axis=0)  # calc mean embedding vector for the window in the words embeddings    \n",
        "\n",
        "      p_center = tf.nn.l2_normalize(p_center, 0)  # normalizing is kinda required if we want cosine return [0..1] range \n",
        "      e_center = tf.nn.l2_normalize(e_center, 0)  # DO WE?\n",
        "      \n",
        "      #todo scroll left by Len of pattern/2\n",
        "      #Because pattern \n",
        "\n",
        "      return tf.losses.cosine_distance(e_center, p_center, axis=0)  ##TODO: how on Earth Cosine could be > 1????\n",
        "\n",
        "    cosine_distances = tf.map_fn(lambda i: do_at_index(i), IDX, dtype=tf.float32)\n",
        "    cosine_similarities = 1.0 - cosine_distances\n",
        "    # --------------------------------------------\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    cosine_similarities_by_pattern = tf.reshape(cosine_similarities, [PATTERN_N, -1])\n",
        "    out = sess.run(cosine_similarities_by_pattern)\n",
        "  #   del elmo\n",
        "  return out\n",
        "\n",
        "##test\n",
        "# test_tokens = [\"–ú–∞–º–∞ –º—ã–ª–∞ –†–∞–º—É, –†–∞–º–∞ –º—ã–ª –ö—Ä–∏—à–Ω—É, –ö—Ä–∏—à–Ω–∞ –º—ã–ª –ë—Ä–∞—Ö–º—É. –ë—Ä–∞—Ö–º–∞ —á–∏—Å—Ç. –í –æ–∫–Ω–µ –∑–∞–∫—Ä—ã—Ç–æ–º –Ω–µ—Ç —Ç–µ–±—è\".split(' ')]\n",
        "# out = build_everything(test_tokens, patterns_tokens_ext, patterns_tokens_lens)\n",
        "# tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbvfU97hGrUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "78f71652-2ab6-4f08-cc67-57eaf499eebb"
      },
      "cell_type": "code",
      "source": [
        "#uploaded = interactive_upload('Protocol')[0]\n",
        "uploaded = \"\"\"\n",
        "–ë–µ—Ä–ª–∏–Ω—Å–∫–∞—è –¥–∞–¥–∞-—è—Ä–º–∞—Ä–∫–∞ –∏ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞ –≤ –ø–∞—Ä–∏–∂¬≠—Å–∫–æ–π –≥–∞–ª–µ—Ä–µ–µ ¬´–ò–∑—è—â–Ω—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞¬ª –≤ 1938 –≥–æ–¥—É —Å—Ç–∞–ª–∏ –≤—ã—Å—à–∏–º–∏ —Ç–æ—á–∫–∞–º–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–≤—É—Ö –¥–≤–∏–∂–µ–Ω–∏–π –∏ –ø–æ–¥–≤–µ–ª–∏ –∏–º –∏—Ç–æ–≥. –ù–∞ ¬´–°—é—Ä—Ä–µ–∞–ª–∏—Å¬≠—Ç–∏—á–µ—Å–∫–æ–π —É–ª–∏—Ü–µ¬ª, –∑–∞ –º–∞–Ω–µ–∫–µ–Ω–∞–º–∏, –≤—ã—Å—Ç—Ä–æ–∏–≤—à–∏–º–∏—Å—è –≤ –ø—Ä–æ—Ö–æ–¥–µ –≤ –≥–ª–∞–≤–Ω—ã–π –∑–∞–ª, —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å –ø–ª–∞–∫–∞—Ç—ã, –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è, –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –æ—Ç—Å—ã–ª–∞—é—â–∏–µ –∫ —Ä–∞–Ω–Ω–∏–º —ç—Ç–∞–ø–∞–º —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞. –í –≥–ª–∞–≤–Ω–æ–º –∑–∞–ª–µ, –∑–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Ç–≤–µ—á–∞–ª –ú–∞—Ä—Å–µ–ª—å –î—é—à–∞–Ω‚Äâ\n",
        " \n",
        ", –∞ –∑–∞ –æ—Å–≤–µ—â–µ¬≠–Ω–∏–µ ‚Äî –ú–∞–Ω –†—ç–π‚Äâ\n",
        " \n",
        ", –∫–∞—Ä—Ç–∏–Ω—ã 1920-—Ö –≥–æ–¥–æ–≤ –≤–∏—Å–µ–ª–∏ —Ä—è–¥–æ–º —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–ª–æ —Ä–∞–∑–≤–∏—Ç–∏\n",
        "–µ —Å—é—Ä—Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ ¬´–∏–Ω—Ç–µ—Ä–Ω–∞—Ü–∏¬≠–æ–Ω–∞¬≠–ª–∞¬ª. –ó–∞—Ä–æ–¥–∏–≤—à–∏—Å—å –∫–∞–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ —Ç–µ—á–µ–Ω–∏–µ, –∫ –∫–æ–Ω—Ü—É 1930-—Ö –≥–æ–¥–æ–≤ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º —É–∂–µ –æ–∫–æ–ª–æ 15 –ª–µ—Ç –≥–æ—Å–ø–æ–¥—Å—Ç–≤–æ–≤–∞–ª –≤ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∞–≤–∞–Ω–≥–∞—Ä–¥–µ –ü–∞—Ä–∏–∂–∞. –ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–π—Ç–∏ –Ω–∞ —Å–ø–∞–¥ —Å –Ω–∞—á–∞–ª–æ–º\n",
        "–í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –æ–Ω —Å—Ç–∞–ª —á–∞—Å—Ç—å—é —Å–≤–µ—Ç—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –ü–∞—Ä–∏–∂–∞ –∏ –¥–∞–∂–µ –¥–æ –Ω–µ–∫–æ—Ç–æ–†–æ–π\n",
        "—Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏—Å—è–≥–Ω—É–ª –≤—ã—Å–æ–∫–æ–π –º–æ–¥–µ, –ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É –∫–∞–∫ —Ä—É—Å—Å–∫–∏–π –∞–≤–∞–Ω–≥–∞—Ä–¥ ‚Äî –ø—É—Å—Ç—å —Å–æ–≤—Å–µ–º –∏–Ω–∞—á–µ ‚Äî –ø—Ä–∏—Å—è–≥–Ω—É \n",
        "–≤ —Å–≤–æ–µ –≤—Ä–µ–º—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏. –ò–∑—è—â–µ—Å—Ç–≤–æ —Å—Ç–∏–ª—è, —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—é—Ä—Ä–µ–ª–∏–∑–º—É, —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–∞–ª–æ —ç—Ç–æ–º—É \n",
        "—Å–±–ª–∏–∂–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, —É–ø—Ä–æ—á–∏–ª–æ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –æ–±—â–µ—Å—Ç–≤–µ. \n",
        "–û–¥–Ω–∞–∫–æ –ø–æ–Ω–∞—á–∞–ª—É –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤-–±—É–Ω—Ç–∞—Ä–µ–π, –Ω–∏—á—É—Ç—å –Ω–µ —Å—Ç—Ä–µ–º–∏–≤—à–∏—Ö—Å—è –∫ —Å–æ—Ü–∏–∞–ª—å¬≠–Ω–æ–º—É —É—Å–ø–µ—Ö—É, \n",
        "–±—ã–ª–∞ –∫—É–¥–∞ –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–≤—è–∑—å —Å –¥–∞–¥–∞–∏–∑–º–æ–º\"\"\"\n",
        "\n",
        "\n",
        "from text_tools import tokenize_text\n",
        "\n",
        "patterns_tokens = [\n",
        "    tokenize_text(\"–∫—Ç–æ-—Ç–æ –∏–º–µ–Ω—É–µ–º—ã–π –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å\"), \n",
        "    tokenize_text(\"–∑–∞–∫–ª—é—á–∏–ª ' ' –¥–æ–≥–æ–≤—Ä\"), \n",
        "    tokenize_text('–≤ –ï–¥–∏–Ω–æ–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–µ–µ—Å—Ç—Ä–µ –°—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞')]\n",
        "\n",
        "patterns_tokens_lens=[len(x) for x in patterns_tokens]\n",
        "maxlen =  max( patterns_tokens_lens)\n",
        "\n",
        "#padding:\n",
        "patterns_tokens_ext=[ x+[' ']*(maxlen-len(x)) for x in patterns_tokens ]\n",
        "#print(patterns_tokens_ext)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TOKENS=tokenize_text(uploaded)\n",
        "out = build_everything([TOKENS], patterns_tokens_ext, patterns_tokens_lens)\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text( TOKENS, out[1]+out[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0427 00:56:24.685903 140421857974144 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0427 00:56:26.251779 140421857974144 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "render_color_text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span title=\"0 0.4785\" style=\"background-color:#f59c7d\">\n",
              " </span><br><span title=\"1 0.4003\" style=\"background-color:#f7b497\">–ë–µ—Ä–ª–∏–Ω—Å–∫–∞—è </span><span title=\"2 0.3598\" style=\"background-color:#f5c0a7\">–¥–∞–¥–∞-—è—Ä–º–∞—Ä–∫–∞ </span><span title=\"3 0.3572\" style=\"background-color:#f5c0a7\">–∏ </span><span title=\"4 0.3572\" style=\"background-color:#f5c0a7\">–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è </span><span title=\"5 0.3356\" style=\"background-color:#f4c6af\">–≤—ã—Å—Ç–∞–≤–∫–∞ </span><span title=\"6 0.3250\" style=\"background-color:#f3c8b2\">—Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞ </span><span title=\"7 0.3724\" style=\"background-color:#f6bda2\">–≤ </span><span title=\"8 0.3908\" style=\"background-color:#f7b79b\">–ø–∞—Ä–∏–∂¬≠—Å–∫–æ–π </span><span title=\"9 0.3885\" style=\"background-color:#f7b89c\">–≥–∞–ª–µ—Ä–µ–µ </span><span title=\"10 0.4731\" style=\"background-color:#f59d7e\">¬´ </span><span title=\"11 0.4557\" style=\"background-color:#f6a385\">–ò–∑—è—â–Ω—ã–µ </span><span title=\"12 0.4559\" style=\"background-color:#f6a385\">–∏—Å–∫—É—Å—Å—Ç–≤–∞ </span><span title=\"13 0.4954\" style=\"background-color:#f39577\">¬ª </span><span title=\"14 0.4822\" style=\"background-color:#f49a7b\">–≤ </span><span title=\"15 0.4091\" style=\"background-color:#f7b194\">1938 </span><span title=\"16 0.3494\" style=\"background-color:#f5c2aa\">–≥–æ–¥—É </span><span title=\"17 0.2600\" style=\"background-color:#ead5c9\">—Å—Ç–∞–ª–∏ </span><span title=\"18 0.2142\" style=\"background-color:#dfdbd9\">–≤—ã—Å—à–∏–º–∏ </span><span title=\"19 0.2174\" style=\"background-color:#e0dbd8\">—Ç–æ—á–∫–∞–º–∏ </span><span title=\"20 0.2618\" style=\"background-color:#ead5c9\">—Ä–∞–∑–≤–∏—Ç–∏—è </span><span title=\"21 0.3526\" style=\"background-color:#f5c1a9\">–¥–≤—É—Ö </span><span title=\"22 0.4677\" style=\"background-color:#f59f80\">–¥–≤–∏–∂–µ–Ω–∏–π </span><span title=\"23 0.5565\" style=\"background-color:#eb7d62\">–∏ </span><span title=\"24 0.6655\" style=\"background-color:#d24b40\">–ø–æ–¥–≤–µ–ª–∏ </span><span title=\"25 0.7254\" style=\"background-color:#c12b30\">–∏–º </span><span title=\"26 0.6930\" style=\"background-color:#cb3e38\">–∏—Ç–æ–≥ </span><span title=\"27 0.5273\" style=\"background-color:#f08a6c\">. </span><span title=\"28 0.4024\" style=\"background-color:#f7b497\">–ù–∞ </span><span title=\"29 0.3352\" style=\"background-color:#f4c6af\">¬´ </span><span title=\"30 0.3124\" style=\"background-color:#f2cbb7\">–°—é—Ä—Ä–µ–∞–ª–∏—Å¬≠—Ç–∏—á–µ—Å–∫–æ–π </span><span title=\"31 0.3170\" style=\"background-color:#f2cab5\">—É–ª–∏—Ü–µ </span><span title=\"32 0.2770\" style=\"background-color:#edd2c3\">¬ª </span><span title=\"33 0.2546\" style=\"background-color:#e9d5cb\">, </span><span title=\"34 0.2451\" style=\"background-color:#e6d7cf\">–∑–∞ </span><span title=\"35 0.2468\" style=\"background-color:#e7d7ce\">–º–∞–Ω–µ–∫–µ–Ω–∞–º–∏ </span><span title=\"36 0.2368\" style=\"background-color:#e5d8d1\">, </span><span title=\"37 0.2741\" style=\"background-color:#ecd3c5\">–≤—ã—Å—Ç—Ä–æ–∏–≤—à–∏–º–∏—Å—è </span><span title=\"38 0.2706\" style=\"background-color:#ebd3c6\">–≤ </span><span title=\"39 0.2549\" style=\"background-color:#e9d5cb\">–ø—Ä–æ—Ö–æ–¥–µ </span><span title=\"40 0.2759\" style=\"background-color:#ecd3c5\">–≤ </span><span title=\"41 0.3232\" style=\"background-color:#f2c9b4\">–≥–ª–∞–≤–Ω—ã–π </span><span title=\"42 0.3117\" style=\"background-color:#f2cbb7\">–∑–∞–ª </span><span title=\"43 0.3213\" style=\"background-color:#f2c9b4\">, </span><span title=\"44 0.2979\" style=\"background-color:#efcebd\">—Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å </span><span title=\"45 0.3114\" style=\"background-color:#f1ccb8\">–ø–ª–∞–∫–∞—Ç—ã </span><span title=\"46 0.2976\" style=\"background-color:#efcebd\">, </span><span title=\"47 0.3150\" style=\"background-color:#f2cbb7\">–ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è </span><span title=\"48 0.2810\" style=\"background-color:#edd1c2\">, </span><span title=\"49 0.3149\" style=\"background-color:#f2cbb7\">–æ–±—ä—è–≤–ª–µ–Ω–∏—è </span><span title=\"50 0.3359\" style=\"background-color:#f4c6af\">–∏ </span><span title=\"51 0.3289\" style=\"background-color:#f3c8b2\">—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ </span><span title=\"52 0.3129\" style=\"background-color:#f2cbb7\">, </span><span title=\"53 0.3226\" style=\"background-color:#f2c9b4\">–æ—Ç—Å—ã–ª–∞—é—â–∏–µ </span><span title=\"54 0.3465\" style=\"background-color:#f5c4ac\">–∫ </span><span title=\"55 0.4791\" style=\"background-color:#f49a7b\">—Ä–∞–Ω–Ω–∏–º </span><span title=\"56 0.6416\" style=\"background-color:#d95847\">—ç—Ç–∞–ø–∞–º </span><span title=\"57 0.5810\" style=\"background-color:#e67259\">—Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞ </span><span title=\"58 0.4639\" style=\"background-color:#f5a081\">. </span><span title=\"59 0.3774\" style=\"background-color:#f7bca1\">–í </span><span title=\"60 0.2813\" style=\"background-color:#edd1c2\">–≥–ª–∞–≤–Ω–æ–º </span><span title=\"61 0.2713\" style=\"background-color:#ebd3c6\">–∑–∞–ª–µ </span><span title=\"62 0.2963\" style=\"background-color:#efcebd\">, </span><span title=\"63 0.3290\" style=\"background-color:#f3c8b2\">–∑–∞ </span><span title=\"64 0.3348\" style=\"background-color:#f4c6af\">–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ </span><span title=\"65 0.3426\" style=\"background-color:#f5c4ac\">–∫–æ—Ç–æ—Ä–æ–≥–æ </span><span title=\"66 0.3705\" style=\"background-color:#f6bda2\">–æ—Ç–≤–µ—á–∞–ª </span><span title=\"67 0.3725\" style=\"background-color:#f6bda2\">–ú–∞—Ä—Å–µ–ª—å </span><span title=\"68 0.4097\" style=\"background-color:#f7b194\">–î—é—à–∞–Ω </span><span title=\"69 0.4278\" style=\"background-color:#f7ac8e\">\n",
              " </span><br><span title=\"70 0.4743\" style=\"background-color:#f59d7e\">\n",
              " </span><br><span title=\"71 0.4618\" style=\"background-color:#f5a081\">, </span><span title=\"72 0.4977\" style=\"background-color:#f39475\">–∞ </span><span title=\"73 0.4982\" style=\"background-color:#f39475\">–∑–∞ </span><span title=\"74 0.4514\" style=\"background-color:#f6a586\">–æ—Å–≤–µ—â–µ¬≠–Ω–∏–µ </span><span title=\"75 0.4081\" style=\"background-color:#f7b396\">‚Äî </span><span title=\"76 0.3843\" style=\"background-color:#f7b99e\">–ú–∞–Ω </span><span title=\"77 0.3906\" style=\"background-color:#f7b89c\">–†—ç–π </span><span title=\"78 0.3048\" style=\"background-color:#f1cdba\">\n",
              " </span><br><span title=\"79 0.2700\" style=\"background-color:#ebd3c6\">\n",
              " </span><br><span title=\"80 0.2401\" style=\"background-color:#e5d8d1\">, </span><span title=\"81 0.2363\" style=\"background-color:#e4d9d2\">–∫–∞—Ä—Ç–∏–Ω—ã </span><span title=\"82 0.2953\" style=\"background-color:#efcebd\">1920-—Ö </span><span title=\"83 0.3084\" style=\"background-color:#f1ccb8\">–≥–æ–¥–æ–≤ </span><span title=\"84 0.2437\" style=\"background-color:#e6d7cf\">–≤–∏—Å–µ–ª–∏ </span><span title=\"85 0.1980\" style=\"background-color:#dcdddd\">—Ä—è–¥–æ–º </span><span title=\"86 0.1368\" style=\"background-color:#cdd9ec\">—Å </span><span title=\"87 0.1873\" style=\"background-color:#d9dce1\">–±–æ–ª–µ–µ </span><span title=\"88 0.2487\" style=\"background-color:#e7d7ce\">—Ä–∞–Ω–Ω–∏–º–∏ </span><span title=\"89 0.3301\" style=\"background-color:#f3c7b1\">—Ä–∞–±–æ—Ç–∞–º–∏ </span><span title=\"90 0.4131\" style=\"background-color:#f7b093\">, </span><span title=\"91 0.4461\" style=\"background-color:#f7a688\">—á—Ç–æ </span><span title=\"92 0.4115\" style=\"background-color:#f7b194\">–ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–ª–æ </span><span title=\"93 0.3760\" style=\"background-color:#f7bca1\">—Ä–∞–∑–≤–∏—Ç–∏ </span><span title=\"94 0.3564\" style=\"background-color:#f5c0a7\">\n",
              " </span><br><span title=\"95 0.3574\" style=\"background-color:#f5c0a7\">–µ </span><span title=\"96 0.3855\" style=\"background-color:#f7b99e\">—Å—é—Ä—Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ </span><span title=\"97 0.5184\" style=\"background-color:#f18d6f\">¬´ </span><span title=\"98 0.5917\" style=\"background-color:#e46e56\">–∏–Ω—Ç–µ—Ä–Ω–∞—Ü–∏¬≠–æ–Ω–∞¬≠–ª–∞ </span><span title=\"99 0.6007\" style=\"background-color:#e36b54\">¬ª </span><span title=\"100 0.5292\" style=\"background-color:#ef886b\">. </span><span title=\"101 0.4379\" style=\"background-color:#f7a98b\">–ó–∞—Ä–æ–¥–∏–≤—à–∏—Å—å </span><span title=\"102 0.3982\" style=\"background-color:#f7b599\">–∫–∞–∫ </span><span title=\"103 0.4005\" style=\"background-color:#f7b497\">–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ </span><span title=\"104 0.4186\" style=\"background-color:#f7af91\">—Ç–µ—á–µ–Ω–∏–µ </span><span title=\"105 0.3929\" style=\"background-color:#f7b79b\">, </span><span title=\"106 0.4090\" style=\"background-color:#f7b194\">–∫ </span><span title=\"107 0.4258\" style=\"background-color:#f7ad90\">–∫–æ–Ω—Ü—É </span><span title=\"108 0.4205\" style=\"background-color:#f7af91\">1930-—Ö </span><span title=\"109 0.4080\" style=\"background-color:#f7b396\">–≥–æ–¥–æ–≤ </span><span title=\"110 0.3961\" style=\"background-color:#f7b599\">—Å—é—Ä—Ä–µ–∞–ª–∏–∑–º </span><span title=\"111 0.3415\" style=\"background-color:#f4c5ad\">—É–∂–µ </span><span title=\"112 0.3668\" style=\"background-color:#f6bea4\">–æ–∫–æ–ª–æ </span><span title=\"113 0.3703\" style=\"background-color:#f6bda2\">15 </span><span title=\"114 0.3127\" style=\"background-color:#f2cbb7\">–ª–µ—Ç </span><span title=\"115 0.2924\" style=\"background-color:#efcfbf\">–≥–æ—Å–ø–æ–¥—Å—Ç–≤–æ–≤–∞–ª </span><span title=\"116 0.3178\" style=\"background-color:#f2cab5\">–≤ </span><span title=\"117 0.4131\" style=\"background-color:#f7b093\">—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º </span><span title=\"118 0.5459\" style=\"background-color:#ec8165\">–∞–≤–∞–Ω–≥–∞—Ä–¥–µ </span><span title=\"119 0.5805\" style=\"background-color:#e67259\">–ü–∞—Ä–∏–∂–∞ </span><span title=\"120 0.5527\" style=\"background-color:#ec7f63\">. </span><span title=\"121 0.4925\" style=\"background-color:#f39577\">–ü—Ä–µ–∂–¥–µ </span><span title=\"122 0.4521\" style=\"background-color:#f6a586\">—á–µ–º </span><span title=\"123 0.4420\" style=\"background-color:#f7a889\">–ø–æ–π—Ç–∏ </span><span title=\"124 0.4257\" style=\"background-color:#f7ad90\">–Ω–∞ </span><span title=\"125 0.4569\" style=\"background-color:#f6a385\">—Å–ø–∞–¥ </span><span title=\"126 0.3870\" style=\"background-color:#f7b89c\">—Å </span><span title=\"127 0.3603\" style=\"background-color:#f6bfa6\">–Ω–∞—á–∞–ª–æ–º </span><span title=\"128 0.3548\" style=\"background-color:#f5c1a9\">\n",
              " </span><br><span title=\"129 0.3838\" style=\"background-color:#f7b99e\">–í—Ç–æ—Ä–æ–π </span><span title=\"130 0.4752\" style=\"background-color:#f59c7d\">–º–∏—Ä–æ–≤–æ–π </span><span title=\"131 0.5336\" style=\"background-color:#ee8669\">–≤–æ–π–Ω—ã </span><span title=\"132 0.4785\" style=\"background-color:#f59c7d\">, </span><span title=\"133 0.4250\" style=\"background-color:#f7ad90\">–æ–Ω </span><span title=\"134 0.3538\" style=\"background-color:#f5c1a9\">—Å—Ç–∞–ª </span><span title=\"135 0.3275\" style=\"background-color:#f3c8b2\">—á–∞—Å—Ç—å—é </span><span title=\"136 0.3717\" style=\"background-color:#f6bda2\">—Å–≤–µ—Ç—Å–∫–æ–π </span><span title=\"137 0.4325\" style=\"background-color:#f7aa8c\">–∫—É–ª—å—Ç—É—Ä—ã </span><span title=\"138 0.4451\" style=\"background-color:#f7a688\">–ü–∞—Ä–∏–∂–∞ </span><span title=\"139 0.4491\" style=\"background-color:#f6a586\">–∏ </span><span title=\"140 0.4858\" style=\"background-color:#f4987a\">–¥–∞–∂–µ </span><span title=\"141 0.4719\" style=\"background-color:#f59d7e\">–¥–æ </span><span title=\"142 0.4796\" style=\"background-color:#f49a7b\">–Ω–µ–∫–æ—Ç–æ–†–æ–π </span><span title=\"143 0.4349\" style=\"background-color:#f7a98b\">\n",
              " </span><br><span title=\"144 0.3915\" style=\"background-color:#f7b79b\">—Å—Ç–µ–ø–µ–Ω–∏ </span><span title=\"145 0.3918\" style=\"background-color:#f7b79b\">–ø—Ä–∏—Å—è–≥–Ω—É–ª </span><span title=\"146 0.3496\" style=\"background-color:#f5c2aa\">–≤—ã—Å–æ–∫–æ–π </span><span title=\"147 0.4139\" style=\"background-color:#f7b093\">–º–æ–¥–µ </span><span title=\"148 0.4662\" style=\"background-color:#f59f80\">, </span><span title=\"149 0.5021\" style=\"background-color:#f29274\">–ø–æ–¥–æ–±–Ω–æ </span><span title=\"150 0.5566\" style=\"background-color:#eb7d62\">—Ç–æ–º—É </span><span title=\"151 0.6102\" style=\"background-color:#e16751\">–∫–∞–∫ </span><span title=\"152 0.6085\" style=\"background-color:#e16751\">—Ä—É—Å—Å–∫–∏–π </span><span title=\"153 0.5841\" style=\"background-color:#e67259\">–∞–≤–∞–Ω–≥–∞—Ä–¥ </span><span title=\"154 0.4889\" style=\"background-color:#f39778\">‚Äî </span><span title=\"155 0.4958\" style=\"background-color:#f39577\">–ø—É—Å—Ç—å </span><span title=\"156 0.4817\" style=\"background-color:#f49a7b\">—Å–æ–≤—Å–µ–º </span><span title=\"157 0.5108\" style=\"background-color:#f18f71\">–∏–Ω–∞—á–µ </span><span title=\"158 0.4856\" style=\"background-color:#f4987a\">‚Äî </span><span title=\"159 0.4242\" style=\"background-color:#f7ad90\">–ø—Ä–∏—Å—è–≥–Ω—É </span><span title=\"160 0.4186\" style=\"background-color:#f7af91\">\n",
              " </span><br><span title=\"161 0.4093\" style=\"background-color:#f7b194\">–≤ </span><span title=\"162 0.4766\" style=\"background-color:#f59c7d\">—Å–≤–æ–µ </span><span title=\"163 0.5339\" style=\"background-color:#ee8669\">–≤—Ä–µ–º—è </span><span title=\"164 0.4720\" style=\"background-color:#f59d7e\">—Ä–µ–≤–æ–ª—é—Ü–∏–∏ </span><span title=\"165 0.4991\" style=\"background-color:#f39475\">. </span><span title=\"166 0.3775\" style=\"background-color:#f7bca1\">–ò–∑—è—â–µ—Å—Ç–≤–æ </span><span title=\"167 0.3426\" style=\"background-color:#f5c4ac\">—Å—Ç–∏–ª—è </span><span title=\"168 0.3625\" style=\"background-color:#f6bfa6\">, </span><span title=\"169 0.3721\" style=\"background-color:#f6bda2\">—Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ </span><span title=\"170 0.3816\" style=\"background-color:#f7ba9f\">—Å—é—Ä—Ä–µ–ª–∏–∑–º—É </span><span title=\"171 0.3958\" style=\"background-color:#f7b599\">, </span><span title=\"172 0.3695\" style=\"background-color:#f6bda2\">—Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–∞–ª–æ </span><span title=\"173 0.3985\" style=\"background-color:#f7b599\">—ç—Ç–æ–º—É </span><span title=\"174 0.4343\" style=\"background-color:#f7aa8c\">\n",
              " </span><br><span title=\"175 0.4509\" style=\"background-color:#f6a586\">—Å–±–ª–∏–∂–µ–Ω–∏—é </span><span title=\"176 0.4368\" style=\"background-color:#f7a98b\">, </span><span title=\"177 0.3922\" style=\"background-color:#f7b79b\">–∫–æ—Ç–æ—Ä–æ–µ </span><span title=\"178 0.3484\" style=\"background-color:#f5c2aa\">, </span><span title=\"179 0.3377\" style=\"background-color:#f4c6af\">–≤ </span><span title=\"180 0.3338\" style=\"background-color:#f4c6af\">—Å–≤–æ—é </span><span title=\"181 0.3972\" style=\"background-color:#f7b599\">–æ—á–µ—Ä–µ–¥—å </span><span title=\"182 0.4016\" style=\"background-color:#f7b497\">, </span><span title=\"183 0.3625\" style=\"background-color:#f6bfa6\">—É–ø—Ä–æ—á–∏–ª–æ </span><span title=\"184 0.3300\" style=\"background-color:#f3c7b1\">–ø–æ–ª–æ–∂–µ–Ω–∏–µ </span><span title=\"185 0.3534\" style=\"background-color:#f5c1a9\">–º–Ω–æ–≥–∏—Ö </span><span title=\"186 0.3891\" style=\"background-color:#f7b89c\">–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π </span><span title=\"187 0.5134\" style=\"background-color:#f18f71\">–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è </span><span title=\"188 0.6704\" style=\"background-color:#d1493f\">–≤ </span><span title=\"189 0.7638\" style=\"background-color:#b40426\">–æ–±—â–µ—Å—Ç–≤–µ </span><span title=\"190 0.7656\" style=\"background-color:#b40426\">. </span><span title=\"191 0.6806\" style=\"background-color:#cf453c\">\n",
              " </span><br><span title=\"192 0.5149\" style=\"background-color:#f18d6f\">–û–¥–Ω–∞–∫–æ </span><span title=\"193 0.3859\" style=\"background-color:#f7b99e\">–ø–æ–Ω–∞—á–∞–ª—É </span><span title=\"194 0.3118\" style=\"background-color:#f2cbb7\">–¥–ª—è </span><span title=\"195 0.2696\" style=\"background-color:#ebd3c6\">–ª–∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤ </span><span title=\"196 0.2660\" style=\"background-color:#ead4c8\">–∏ </span><span title=\"197 0.2983\" style=\"background-color:#f0cdbb\">—Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤-–±—É–Ω—Ç–∞—Ä–µ–π </span><span title=\"198 0.2749\" style=\"background-color:#ecd3c5\">, </span><span title=\"199 0.2797\" style=\"background-color:#edd2c3\">–Ω–∏—á—É—Ç—å </span><span title=\"200 0.2993\" style=\"background-color:#f0cdbb\">–Ω–µ </span><span title=\"201 0.3041\" style=\"background-color:#f1cdba\">—Å—Ç—Ä–µ–º–∏–≤—à–∏—Ö—Å—è </span><span title=\"202 0.3057\" style=\"background-color:#f1cdba\">–∫ </span><span title=\"203 0.3771\" style=\"background-color:#f7bca1\">—Å–æ—Ü–∏–∞–ª—å¬≠–Ω–æ–º—É </span><span title=\"204 0.4471\" style=\"background-color:#f7a688\">—É—Å–ø–µ—Ö—É </span><span title=\"205 0.4590\" style=\"background-color:#f6a283\">, </span><span title=\"206 0.4171\" style=\"background-color:#f7b093\">\n",
              " </span><br><span title=\"207 0.4107\" style=\"background-color:#f7b194\">–±—ã–ª–∞ </span><span title=\"208 0.4498\" style=\"background-color:#f6a586\">–∫—É–¥–∞ </span><span title=\"209 0.4632\" style=\"background-color:#f5a081\">–±–æ–ª–µ–µ </span><span title=\"210 0.4923\" style=\"background-color:#f39577\">–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π </span><span title=\"211 0.5011\" style=\"background-color:#f29274\">—Å–≤—è–∑—å </span><span title=\"212 0.4437\" style=\"background-color:#f7a688\">—Å </span><span title=\"213 0.4182\" style=\"background-color:#f7af91\">–¥–∞–¥–∞–∏–∑–º–æ–º </span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}