{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 test demo no UI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGPbyfYp6nIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title –ù–∞—Å—Ç—Ä–æ–π–∫–∏ { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "USD_to_RUB = 20034.02 #@param {type:\"number\"}\n",
        "RUB_to_USD = 1.0/USD_to_RUB\n",
        "\n",
        "# print('USD_to_RUB=',USD_to_RUB)\n",
        "# print('RUB_to_USD=',RUB_to_USD)\n",
        "\n",
        "\n",
        "\n",
        "currency_converter = {\n",
        "  'USD': USD_to_RUB,\n",
        "  'RUB': 1.0\n",
        "}\n",
        "\n",
        "print(currency_converter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MAIN"
      ]
    },
    {
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ { output-height: 800, form-width: \"300px\", display-mode: \"form\" }\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"contracts-subj\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "def _import_elmo():\n",
        "  \"\"\"\n",
        "  ACHTUNG!! this method is called later by ElmoEmbedder\n",
        "  \"\"\"\n",
        "\n",
        "  elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz',\n",
        "                    trainable=False)  # news\n",
        "  #   elmo = hub.Module('https://storage.googleapis.com/az-nlp/elmo_ru-twitter_2013-01_2018-04_600k_steps.tar.gz',\n",
        "  #                     trainable=False)  # twitter\n",
        "  print('‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è DONE (re)importing Tensorflow hub.Module ')\n",
        "  print('Tensorflow version is', tf.__version__)\n",
        "\n",
        "  return elmo\n",
        "\n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(_import_elmo(), tf, 'elmo', _import_elmo)\n",
        "\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "# AZ:-Init chartes context-----------------------------------------------------------------------------------\n",
        "def _init_charters():\n",
        "  if 'CharterAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Charters-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  _init_embedder()  # PRECONDITION\n",
        "  from charter_patterns import CharterPatternFactory\n",
        "  from charter_parser import CharterDocumentParser\n",
        "  CPF = CharterPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "  GLOBALS__['CharterAnlysingContext'] = CharterDocumentParser(CPF)\n",
        "  print('‚ù§Ô∏è DONE initing Charters-related tools and models ')\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from transaction_values import ValueConstraint\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  import matplotlib.pyplot as plt\n",
        "  \n",
        "  from renderer import AbstractRenderer, HtmlRenderer, head_types_colors\n",
        "  from renderer import to_multicolor_text, as_headline_3, as_offset\n",
        "  from renderer import as_msg, as_quote, as_c_quote\n",
        "  from renderer import as_error_html, known_subjects_dict, v_color_map\n",
        "  from transaction_values import ValueConstraint\n",
        "  from parsing import head_types_dict, head_types\n",
        "  from legal_docs import PatternSearchResults, ConstraintsSearchResult, PatternSearchResult, CharterDocument\n",
        "  \n",
        "  import numpy as np\n",
        "  \n",
        "  from charter_patterns import known_subjects\n",
        "  from patterns import AV_SOFT, AV_PREFIX\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  def _as_smaller(txt):\n",
        "    return f'<div font-size:12px\">{txt}</div>'\n",
        " \n",
        "  \n",
        "      \n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      print('render_color_text')\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "                \n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      if len(tokens) == 0:\n",
        "        return \" - empty -\"\n",
        "      if len(weights) != len(tokens):\n",
        "        raise ValueError(\"number of weights differs weights={} tokens={}\".format(len(weights), len(tokens)))\n",
        "\n",
        "      #   if()\n",
        "      vmin = weights.min()\n",
        "      vmax = weights.max()\n",
        "\n",
        "      if _range is not None:\n",
        "        vmin = _range[0]\n",
        "        vmax = _range[1]\n",
        "\n",
        "      if print_debug:\n",
        "        print(vmin, vmax)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmin=vmin - 0.5, vmax=vmax)\n",
        "      html = \"\"\n",
        "      cmap = mpl.cm.get_cmap(colormap)\n",
        "\n",
        "      for d in range(0, len(weights)):\n",
        "        word = tokens[d]\n",
        "        if word == ' ':\n",
        "          word = '&nbsp;_ '\n",
        "\n",
        "        html += '<span title=\"{} {:.4f}\" style=\"background-color:{}\">{} </span>'.format(\n",
        "          d,\n",
        "          weights[d],\n",
        "          mpl.colors.to_hex(cmap(norm(weights[d]))),\n",
        "          word)\n",
        "\n",
        "        #     html+='<span style=\"background-color:' +mpl.colors.to_hex(cmap(norm(weights[d]) ))+ '\">' + str(tokens[d]) + \" </span>\"\n",
        "        if tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    ''' AZ:-Rendering CHARITYüî•-----üí∏------üí∏-------üí∏------------------------------'''\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "    \n",
        "\n",
        "    ''' AZ:------üí∏------üí∏-------üí∏----------------------END--Rendering CHARITYüî•------'''\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    def render_subj(self, contract: ContractDocument3):\n",
        "      subjects: List[ProbableValue] = contract.subjects\n",
        "\n",
        "      if len(subjects) > 0:\n",
        "        sorted_ = [y for y in sorted(subjects, key=lambda x: -x.confidence)]\n",
        "        subject_kind = sorted_[0].value\n",
        "        confidence = sorted_[0].confidence\n",
        "      else:\n",
        "        subject_kind = ContractSubject.Other\n",
        "\n",
        "      if subject_kind in known_subjects_dict:\n",
        "        rendering_name = known_subjects_dict[subject_kind]\n",
        "      else:\n",
        "        rendering_name = '–ø—Ä–æ—á–µ–µ'\n",
        "\n",
        "      display(\n",
        "        HTML(f'–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:'\n",
        "             f'<h3 style=\"margin:0\">{rendering_name}<sup> {subject_kind}</sup> </h3> '\n",
        "             f'confidence:{confidence:20,.2f}'))\n",
        "\n",
        "\n",
        "\n",
        "    def sign_to_text(self, sign: int):\n",
        "      if sign < 0: return \" &lt; \"\n",
        "      if sign > 0: return \" &gt; \"\n",
        "      return ' = '\n",
        "\n",
        "    def probable_value_to_html(self, pv):\n",
        "      vc = pv.value\n",
        "      color = '#333333'\n",
        "      if vc.sign > 0:\n",
        "        color = '#993300'\n",
        "      elif vc.sign < 0:\n",
        "        color = '#009933'\n",
        "\n",
        "      return f'<b style=\"color:{color}\">{self.sign_to_text(vc.sign)} {vc.currency} {vc.value:20,.2f} confidence={pv.confidence:20,.2f}</b> '\n",
        "\n",
        "    def render_contents(self, doc):\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for i in doc.structure.headline_indexes:\n",
        "        line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li> {line} <sup>line {i}</sup></li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_sections(self, sections):\n",
        "      from legal_docs import HeadlineMeta\n",
        "      html = as_headline_3('–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞')\n",
        "      html += \"<ul>\"\n",
        "      for section_type in sections:\n",
        "        section: HeadlineMeta = sections[section_type]\n",
        "        body = section.body.untokenize_cc()[:1000]\n",
        "        headline = section.subdoc.untokenize_cc()[:500]\n",
        "        #     line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "        html += f'<li><h3> {headline} <sup>type: {section_type}</sup> </h3> <p>{body}</p> </li>'\n",
        "      html += \"</ul>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        "    def render_values(self, values):\n",
        "      if len(values) > 0:\n",
        "        for pv in values:\n",
        "          h = self.probable_value_to_html(pv)\n",
        "          display(HTML(h))\n",
        "      else:\n",
        "        display(HTML('—Å—É–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'))\n",
        "\n",
        "    def render_value_section_details(self, value_section_info):\n",
        "      value_section = value_section_info.body\n",
        "      headline_doc = value_section_info.subdoc\n",
        "\n",
        "      headline = headline_doc.untokenize_cc()\n",
        "\n",
        "      v_names = {\n",
        "        'value_attention_vector',\n",
        "        'novalue_attention_vector',\n",
        "\n",
        "        'novalue_attention_vector_local_contrast',\n",
        "        'value_attention_vector_tuned'}\n",
        "\n",
        "      fig = plt.figure(figsize=(20, 6))\n",
        "      ax = plt.axes()\n",
        "      for vector_name in v_names:\n",
        "        ax.plot(value_section.distances_per_pattern_dict[vector_name], label=vector_name, alpha=0.4)\n",
        "\n",
        "      ax.plot(value_section.distances_per_pattern_dict['value_attention_vector_tuned'], label='value_attention result',\n",
        "              alpha=0.9, color='black')\n",
        "      plt.legend(loc='upper right')\n",
        "\n",
        "      text = self.to_color_text(value_section.tokens_cc,\n",
        "                                value_section.distances_per_pattern_dict['value_attention_vector_tuned'], _range=(0, 1))\n",
        "      html = f'{ as_headline_3(headline)} <div style=\"margin-left:4em; font-size=90%\">{text}</div>'\n",
        "      display(HTML(html))\n",
        "\n",
        "     \n",
        "    def render_charter_parsing_results_2(self, charter):\n",
        "      display(HTML(self.charter_parsing_results_to_html(charter)))\n",
        "      \n",
        "    def render_charter_parsing_results(self, doc, org, rz, charity_constraints):\n",
        "      WARN = '\\033[1;31m======== Dear Artem, ACHTUNG! üîû '\n",
        "      print (WARN+f\"use {self.render_charter_parsing_results} is deprecated\")\n",
        "      txt_html = self.to_color_text(org['tokens'], org['attention_vector'], _range=[0, 1])\n",
        "\n",
        "      html = '<div style=\"background:#eeeeff; padding:0.5em\"> recognized NE(s): <br><br> org type:<h3 style=\"margin:0\">  {} </h3>org full name:<h2 style=\"margin:0\">  {} </h2> <br>quote: <div style=\"font-size:90%; background:white\">{}</div> </div>'.format(\n",
        "        org['type_name'], org['name'], txt_html)\n",
        "      # html+=txt_html\n",
        "      html += self.render_constraint_values(doc, rz, charity_constraints)\n",
        "\n",
        "      display(HTML(html))\n",
        "\n",
        " \n",
        "\n",
        "     \n",
        "\n",
        "    \n",
        " \n",
        "\n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  # AZ:----------PROTOCOLS RENDERER-------------------------\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "\n",
        "  import matplotlib as mpl\n",
        "  from IPython.core.display import display, HTML\n",
        "  from renderer import as_headline_3, as_headline_4\n",
        "  \n",
        "  class ProtocolRenderer(DemoRenderer):\n",
        "\n",
        "    def winning_patterns_to_html(self, _tokens, ranges, winning_patterns, _range,\n",
        "                                 colormaps=['Reds', 'Purples', 'Blues', 'Greens', 'Greys']):\n",
        "      vmin = -ranges[1]\n",
        "      vmax = -ranges[0]\n",
        "\n",
        "      #     print(\"winning_patterns_to_html _range\", _range, \"min max=\", ranges)\n",
        "\n",
        "      norm = mpl.colors.Normalize(vmax=vmax, vmin=vmin)\n",
        "\n",
        "      cmaps = []\n",
        "\n",
        "      #     print (colormaps)\n",
        "      for n in colormaps:\n",
        "        cmap = mpl.cm.get_cmap(n)\n",
        "        cmaps.append(cmap)\n",
        "\n",
        "      html = \"\"\n",
        "\n",
        "      for d in _range:\n",
        "        winning_pattern_i = winning_patterns[d][0]\n",
        "        colormap = cmaps[winning_pattern_i % len(colormaps)]\n",
        "        normed = norm(-winning_patterns[d][1])\n",
        "        color = mpl.colors.to_hex(colormap(normed))\n",
        "        html += '<span title=\"' + '{} {:.2f}'.format(d, winning_patterns[d][\n",
        "          1]) + '\" style=\"background-color:' + color + '\">' + str(\n",
        "          _tokens[d]) + \" </span>\"\n",
        "        if _tokens[d] == '\\n':\n",
        "          html += \"<br>\"\n",
        "\n",
        "      return html\n",
        "\n",
        "    def _render_doc_subject_fragments(self, doc):\n",
        "      #     print(doc.per_subject_distances)\n",
        "\n",
        "      _html = \"\"\n",
        "      if doc.per_subject_distances is not None:\n",
        "\n",
        "        type = \"–î–æ–≥–æ–≤–æ—Ä  –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\"\n",
        "        if doc.per_subject_distances[0] > doc.per_subject_distances[1]:\n",
        "          type = \"–î–æ–≥–æ–≤–æ—Ä –≤–æ–∑–º–µ–∑–¥–Ω–æ–≥–æ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥\"\n",
        "\n",
        "        _html += \"<h3>\" + type + \"</h3>\"\n",
        "\n",
        "        colormaps = ['PuRd'] * 5 + ['Blues'] * 7 + ['Greys']\n",
        "\n",
        "        _html += as_headline_4('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞')\n",
        "\n",
        "        for region in [doc.subj_range]:\n",
        "          _html += self.winning_patterns_to_html(_tokens=doc.tokens, ranges=doc.subj_ranges,\n",
        "                                                 winning_patterns=doc.winning_subj_patterns, _range=region,\n",
        "                                                 colormaps=colormaps)\n",
        "\n",
        "      return _html\n",
        "\n",
        "    def render_subject(self, counter):\n",
        "      html = as_headline_3('–ü—Ä–µ–¥–º–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (X):') + self.subject_type_weights_to_html(counter)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def print_results(self, _doc: LegalDocument, results=None):\n",
        "\n",
        "      if results is None:\n",
        "        results = _doc.found_sum\n",
        "\n",
        "      result, (start, end), sentence, meta = results\n",
        "\n",
        "      html = \"<hr>\"\n",
        "\n",
        "      html += self._render_doc_subject_fragments(_doc)\n",
        "\n",
        "      if result is None:\n",
        "        html += '<h2 style=\"color:red\">–°–£–ú–ú–ê –ù–ï –ù–ê–ô–î–ï–ù–ê</h2>'\n",
        "      else:\n",
        "        html += '<h2>' + str(result[0]) + ' ' + str(result[1]) + '</h2>'\n",
        "\n",
        "      for key in meta.keys():\n",
        "        html += '<div style=\"font-size:9px\">' + str(key) + \" = \" + str(meta[key]) + \"</div>\"\n",
        "\n",
        "      display(HTML(html))\n",
        "      self.render_color_text(_doc.tokens[start:end], _doc.sums[start:end])\n",
        "\n",
        "    def subject_type_weights_to_html(self, counter):\n",
        "      dict = {\n",
        "        't_dea': '–°–¥–µ–ª–∫–∞',\n",
        "        't_cha': '–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',\n",
        "        't_org': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è'\n",
        "      }\n",
        "\n",
        "      maxkey = \"None\"\n",
        "      for key in dict:\n",
        "        if counter[key] > counter[maxkey]:\n",
        "          maxkey = key\n",
        "\n",
        "      html = \"\"\n",
        "      for key in dict:\n",
        "        templ = \"<div>{}: {}</div>\"\n",
        "        if key == maxkey:\n",
        "          templ = '<b style=\"font-size:135%; color:maroon\">{}: {}</b>'\n",
        "        html += templ.format(counter[key], dict[key])\n",
        "\n",
        "      return html\n",
        "\n",
        "  GLOBALS__['ProtocolRenderer'] = ProtocolRenderer()\n",
        "\n",
        "  from demo_protocols import ProtocolAnlysingContext\n",
        "  GLOBALS__['ProtocolAnlysingContext'] = ProtocolAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['ProtocolRenderer'])\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Charters context====\n",
        "\n",
        "  def read_doc(fn):\n",
        "    import docx2txt, sys, os\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text\n",
        "\n",
        "  GLOBALS__['read_doc'] = read_doc\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "\n",
        "# AZ:-FINDING_VIOLATIONS--------------------------------------------------------\n",
        "def find_and_show_violations():\n",
        "  from IPython.core.display import display, HTML\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  from renderer import as_headline_2, as_error_html\n",
        "\n",
        "  print('–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π')\n",
        "\n",
        "  charterAnlysingContext: CharterAnlysingContext = GLOBALS__['CharterAnlysingContext']\n",
        "  contractAnlysingContext: ContractAnlysingContext = GLOBALS__['ContractAnlysingContext']\n",
        "\n",
        "  contract = contractAnlysingContext.contract\n",
        "  charter = charterAnlysingContext.doc\n",
        "  charter_constraints = charterAnlysingContext.constraints  # XXX: move to doc\n",
        "\n",
        "  renderer = GLOBALS__['renderer']\n",
        "  renderer.render_subj(contract)\n",
        "\n",
        "  import copy\n",
        "\n",
        "  def convert(v):\n",
        "    v_converted = copy.copy(v)\n",
        "    if v.currency in currency_converter:\n",
        "      v_converted.value = currency_converter[v.currency] * v.value\n",
        "      v_converted.currency = 'RUB'\n",
        "      return v_converted\n",
        "    else:\n",
        "      display(HTML(as_error_html(\n",
        "        f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "      return v\n",
        "\n",
        "  best_value = contractAnlysingContext.find_contract_best_value(convert)\n",
        "\n",
        "  # rendering:----------------------------\n",
        "\n",
        "  def _render_violations(ranges_by_group, best_value):\n",
        "    for group_key in ranges_by_group:\n",
        "      group = ranges_by_group[group_key]\n",
        "      display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "      for rk in group['ranges']:\n",
        "        r = group['ranges'][rk]\n",
        "        display(HTML(r.check_contract_value(best_value, convert, renderer)))\n",
        "\n",
        "  print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "  renderer.render_values([best_value])\n",
        "  renderer.render_color_text(best_value.value.context.tokens, best_value.value.context.attention, _range=[0, 1])\n",
        "\n",
        "  _render_violations(\n",
        "    charterAnlysingContext.find_ranges_by_group(charter_constraints, convert, verbose=False),\n",
        "    best_value)\n",
        "\n",
        "#   display(HTML(renderer.render_constraint_values(charter_constraints)))\n",
        "\n",
        "\n",
        "# AZ:--------------------------------------------------------FINDING_VIOLATIONS-\n",
        "\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhjI5YF61cpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 0. –ò–Ω–∏—Ç –≤—Å–µ–≥–∞"
      ]
    },
    {
      "metadata": {
        "id": "6BnrLv2k1iVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## do preparation here\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        "# 2.\n",
        "_init_embedder()\n",
        "# 3.\n",
        "_init_the_code()\n",
        "4.\n",
        "_init_charters()\n",
        "# 5.\n",
        "_init_contracts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zNNi8ZkbzLwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 1. –£—Å—Ç–∞—Ñ—Ñ"
      ]
    },
    {
      "metadata": {
        "id": "z2rA1VotgOSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = interactive_upload('–£—Å—Ç–∞—Ñ—Ñ, –æ–Ω –ª—ë—Ö—Ö—Ö, –Ω–æ –ø–æ–¥—É–º–∞—Ñ—Ñ –æ–Ω –æ—Å–æ–∑–Ω–∞–ª, —á—Ç–æ –æ–Ω –Ω–µ —Ç–∞–∫–æ–π —É–∂ –∏ –õ—ë—Ö—Ö')\n",
        "\n",
        "_CTX = GLOBALS__['CharterAnlysingContext']\n",
        "_CTX.verbosity_level=2\n",
        "_CTX.analyze_charter(uploaded[0], True)\n",
        "\n",
        "\n",
        "GLOBALS__['renderer'].render_charter_parsing_results_2(_CTX.charter)\n",
        "GLOBALS__['renderer'].render_contents(_CTX.doc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olVwh_GqMs52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GLOBALS__['renderer'].render_charter_parsing_results(_CTX.doc, _CTX.org, _CTX.constraints, _CTX.charity_constraints)\n",
        "# _CTX.charter._constraints[0].__dict__\n",
        "for c in GLOBALS__['CharterAnlysingContext'].charter._constraints:\n",
        "  print(c.__dict__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OFOIGEs5pZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## NEW\n",
        "# del(GLOBALS__['_init_the_code'] )\n",
        "_init_the_code(True)\n",
        "GLOBALS__['renderer'].render_charter_parsing_results_2(_CTX.charter)\n",
        "# GLOBALS__['renderer'].render_contents(_CTX.doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slIYoTxBAkNN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['renderer'].render_charter_parsing_results(_CTX.doc, _CTX.org, _CTX.constraints, _CTX.charity_constraints)\n",
        "# GLOBALS__['renderer'].render_contents(_CTX.doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fV5mnSXciKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['CharterAnlysingContext'].constraints['head.directors'][0].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnsM7Qzwci1J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### tests, experiments"
      ]
    },
    {
      "metadata": {
        "id": "ycJrZCWAFEXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  GLOBALS__['CharterAnlysingContext'].verbosity_level=2\n",
        "  org, rz = GLOBALS__['CharterAnlysingContext'].analyze_charter(text, True)\n",
        "  doc = GLOBALS__['CharterAnlysingContext'].doc\n",
        "  charity_constraints = GLOBALS__['CharterAnlysingContext'].charity_constraints\n",
        "  org = GLOBALS__['CharterAnlysingContext'].org\n",
        "\n",
        "\n",
        "  GLOBALS__['renderer'].render_contents(doc)\n",
        "  GLOBALS__['renderer'].render_charter_parsing_results(doc, org, rz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejjvjf3I4SWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "str(_CTX.constraints['head.all'][0].subdoc.subject_mapping[\"subj\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_CqKEG514Xf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVPCRaSAmYHt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## üëû Constraint type detection"
      ]
    },
    {
      "metadata": {
        "id": "RpYvYObHBAxc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Factory extension (careful)"
      ]
    },
    {
      "metadata": {
        "id": "ZJxb5ytdYI_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1r41cS5RCxcv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_lawsuit_patterns(factory):\n",
        "  def cp(name, tuples):\n",
        "    return factory.create_pattern(name, tuples)\n",
        "\n",
        "#   cp('x_lawsuit_4', ('–Ω–∞—á–∞–ª–æ/—É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—é–±—ã—Ö', '—Å—É–¥–µ–±–Ω—ã—Ö', \n",
        "#                 '—Å–ø–æ—Ä–æ–≤, –ø–æ–¥–ø–∏—Å–∞–Ω–∏–µ –º–∏—Ä–æ–≤–æ–≥–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è, –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ –∏—Å–∫–∞, –æ—Ç–∫–∞–∑ –æ—Ç –∏—Å–∫–∞, –∞ —Ç–∞–∫–∂–µ –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ  ',\n",
        "#                       ))\n",
        "\n",
        "#   cp('x_lawsuit_3', ('—Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤, —Ü–µ–Ω–∞ ',\n",
        "#                        '–∏—Å–∫–∞',\n",
        "#                        '–ø–æ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–µ–≤—ã—à–∞–µ—Ç'))\n",
        "  cp('x_lawsuit_6', ( '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ–± –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ —Ä–µ—à–µ–Ω–∏–π , —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ', '—Å–ø–æ—Ä–∞–º–∏', '–û–±—â–µ—Å—Ç–≤–∞ —Å –û—Ä–≥–∞–Ω–∞–º–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π –≤–ª–∞—Å—Ç–∏ , –Ω–∞ —Å—É–º–º—ã '  ))\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  \n",
        "build_lawsuit_patterns(GLOBALS__['CharterAnlysingContext'].pattern_factory)\n",
        "GLOBALS__['CharterAnlysingContext'].pattern_factory.embedd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YY3e0XMpYL-V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raise sure?\n",
        "\n",
        "# def build_realestate_patterns(factory):\n",
        "#   def cp(name, tuples):\n",
        "#     return factory.create_pattern(name, tuples)\n",
        "\n",
        "#   cp('x_realestate_1', ('–æ—Ç—á—É–∂–¥–µ–Ω–∏—é –∞–∫—Ç–∏–≤–æ–≤ –æ–±—â–µ—Å—Ç–≤–∞ ( –≤–∫–ª—é—á–∞—è', '–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '',))\n",
        "\n",
        "#   cp('x_realestate_2', ('—Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤, —Ü–µ–Ω–∞ ',\n",
        "#                        '–∏—Å–∫–∞',\n",
        "#                        '–ø–æ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–µ–≤—ã—à–∞–µ—Ç'))\n",
        " \n",
        "  \n",
        "  \n",
        "# build_realestate_patterns(GLOBALS__['CharterAnlysingContext'].pattern_factory)\n",
        "# GLOBALS__['CharterAnlysingContext'].pattern_factory.embedd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDgF5nNkLZ88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def build_incl_patterns(factory):\n",
        "#   def cp(name, tuples):\n",
        "#     return factory.create_pattern(name, tuples)\n",
        "\n",
        "#   cp('x_exclusive_1', ('', '–∏—Å–∫–ª—é—á–∞—è', '—Å–¥–µ–ª–∫–∏'))\n",
        "\n",
        "#   cp('x_inclusive_1', ('', '–≤–∫–ª—é—á–∞—è', '—Å–¥–µ–ª–∫–∏'))\n",
        "  \n",
        "# build_incl_patterns(GLOBALS__['CharterAnlysingContext'].pattern_factory)\n",
        "# GLOBALS__['CharterAnlysingContext'].pattern_factory.embedd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAreL6ew6Nh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_init_the_code(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6wc-fUn0aek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "CH_CTX=GLOBALS__['CharterAnlysingContext']\n",
        "\n",
        "from charter_patterns import find_sentences_by_pattern_prefix\n",
        "\n",
        "a:dict = find_sentences_by_pattern_prefix(  CH_CTX.pattern_factory, CH_CTX._get_head_sections(), 'x_charity_')\n",
        "b:dict = find_sentences_by_pattern_prefix(  CH_CTX.pattern_factory, CH_CTX._get_head_sections(), 'sum__')\n",
        "lawsuits = find_sentences_by_pattern_prefix(  CH_CTX.pattern_factory, CH_CTX._get_head_sections(), 'x_lawsuit_')\n",
        "  \n",
        "def merge_quotes_by_head_type (a, b):\n",
        "  res={}\n",
        "  for head in a:\n",
        "    res[head] = a[head]+b[head]\n",
        "  return res\n",
        "  \n",
        "merged=merge_quotes_by_head_type(a, b)\n",
        "merged=merge_quotes_by_head_type(merged, lawsuits)\n",
        "# soft$.$at_sum__\n",
        "for head in merged:\n",
        "  print(CH_CTX.doc.sections[head].body.distances_per_pattern_dict.keys())\n",
        "  html = GLOBALS__['renderer'].html_charity_constraints_by_head(merged[head] )\n",
        "  display(HTML(html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S70aM1xD2zLN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "_init_the_code(True)\n",
        "CH_CTX=GLOBALS__['CharterAnlysingContext']\n",
        "\n",
        "from charter_patterns import find_sentences_by_pattern_prefix\n",
        "realestates = find_sentences_by_pattern_prefix( CH_CTX.pattern_factory, CH_CTX._get_head_sections(), 'sum__')\n",
        "  \n",
        "\n",
        "for head in lawsuits:\n",
        "  html = GLOBALS__['renderer'].html_charity_constraints_by_head(realestates[head] )\n",
        "  display(HTML(html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JQWRoJf_OfS7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "CH_CTX=GLOBALS__['CharterAnlysingContext']\n",
        "\n",
        "from charter_patterns import find_sentences_by_pattern_prefix\n",
        "lawsuits = find_sentences_by_pattern_prefix( CH_CTX.pattern_factory, CH_CTX._get_head_sections(), 'x_ContractSubject.RealEstate')\n",
        "  \n",
        " \n",
        "  \n",
        "# soft$.$at_sum__\n",
        "for head in lawsuits:\n",
        "  print(CH_CTX.doc.sections[head].body.distances_per_pattern_dict.keys())\n",
        "  html = GLOBALS__['renderer'].html_charity_constraints_by_head(lawsuits[head] )\n",
        "  display(HTML(html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6C5LhlUp0Jln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from legal_docs import ConstraintsSearchResult, PatternSearchResult\n",
        "from IPython.core.display import display, HTML\n",
        "from renderer import as_quote, to_multicolor_text\n",
        " \n",
        "\n",
        "CH_CTX=GLOBALS__['CharterAnlysingContext']\n",
        "\n",
        "\n",
        "def estimate_confidence(vector):\n",
        "  sum_ = sum(vector)\n",
        "  _max = np.max( np.nonzero(vector) )\n",
        "  nonzeros_count = len(np.nonzero(vector)[0])\n",
        "  confidence = 0\n",
        "\n",
        "  if nonzeros_count > 0:\n",
        "    confidence = sum_ / nonzeros_count\n",
        "    \n",
        "  return confidence, sum_, nonzeros_count,  _max\n",
        "\n",
        "for constraints in CH_CTX.constraints['head.directors']:\n",
        "  html =GLOBALS__['renderer']._render_sentence(constraints)\n",
        "  display(HTML(html))\n",
        "  context:PatternSearchResult = constraints.subdoc\n",
        "  print(context.confidence)\n",
        "  print(context.pattern_prefix)\n",
        "  print(context.tokens)\n",
        "#   'soft$.$at_x_realestate_'\n",
        "  confidence, sum_, nonzeros_count, _max = estimate_confidence(context.get_attention())\n",
        "  print(f'confidence={confidence:.4f} \\t sum_{sum_} \\t nonzeros_count={nonzeros_count} \\t_max={_max:.4f}')\n",
        "  \n",
        "  GLOBALS__['renderer'].render_color_text(context.tokens, context.get_attention('soft$.$at_x_realestate_'), _range=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XtXmlhVD8uIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2qD6gSJ_1rQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from legal_docs import calculate_distances_per_pattern\n",
        "from ml_tools import *\n",
        "from patterns import *\n",
        "\n",
        "#XXX: renamed from find_charity_sentences\n",
        "\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "###-----test\n",
        "CH_CTX=GLOBALS__['CharterAnlysingContext']\n",
        "renderer = GLOBALS__['renderer']\n",
        "charter = CH_CTX.doc\n",
        "section = charter.sections['head.directors'].body\n",
        "\n",
        "\n",
        "slices1, attention_v_name_consent = find_sentences_by_pattern_prefix (section, CH_CTX.pattern_factory, pattern_prefix='d_order_consent')\n",
        "slices2, attention_v_name_charity = find_sentences_by_pattern_prefix (section, CH_CTX.pattern_factory, pattern_prefix='x_charity')\n",
        "slices3, attention_v_name_law = find_sentences_by_pattern_prefix (section, CH_CTX.pattern_factory, pattern_prefix='x_lawsuit_')\n",
        "slices4, attention_v_currency = find_sentences_by_pattern_prefix (section, CH_CTX.pattern_factory, pattern_prefix='currency')\n",
        "\n",
        "\n",
        "attention_incl,_ = make_attention_vector(section, CH_CTX.pattern_factory, 'x_inclusive')\n",
        "attention_excl,_ = make_attention_vector(section, CH_CTX.pattern_factory, 'x_exclusive')\n",
        "\n",
        "attention_excl=momentum(attention_excl, 0.9)\n",
        "\n",
        "colormap = {\n",
        "    'red':(1,0,0),\n",
        "    'red1':(1,0.4,0),\n",
        "    'green':(0,1,0),\n",
        "    'blue':(0,0,1),\n",
        "    'cyan':(0,0.7,1),\n",
        "    'yellow':(0.9,0.8,0)\n",
        "}\n",
        "\n",
        "alltogether={}\n",
        "def merge_slices(slices1):\n",
        "  for _slice in slices1:\n",
        "    alltogether[_slice [0].start] = _slice\n",
        "    ss=section.subdoc_slice( _slice [0] )\n",
        "\n",
        "merge_slices(slices1)\n",
        "merge_slices(slices3)\n",
        "merge_slices(slices2)\n",
        "merge_slices(slices4)\n",
        "\n",
        "ssss=[]\n",
        "for k in alltogether:\n",
        "  _slice=alltogether[k]\n",
        "#   print(_slice)\n",
        "  ss=section.subdoc_slice( _slice [0] )\n",
        "  vectors = {\n",
        "    'red':ss.distances_per_pattern_dict['soft$.'+attention_v_name_law],\n",
        "    'cyan':ss.distances_per_pattern_dict['soft$.'+attention_v_name_charity],\n",
        "#     'green':ss.distances_per_pattern_dict['soft$.'+attention_v_name_consent],\n",
        "    'red1':ss.distances_per_pattern_dict['soft$.'+attention_v_currency],\n",
        "      \n",
        "    'yellow':attention_incl[_slice [0] ],\n",
        "    'green':attention_excl[_slice [0] ],\n",
        "  }\n",
        "  ssss.append(ss)\n",
        "#   print (ss.tokens_cc)\n",
        "  renderer.render_multicolor_text(ss.tokens_cc, vectors, colormap=colormap)\n",
        "#   renderer.render_color_text(ss.tokens, vectors['red'])\n",
        "del ssss\n",
        "# print (slices)\n",
        "# print(section.distances_per_pattern_dict)\n",
        "# vectors['red']\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('ddd'))\n",
        "GLOBALS__['renderer'].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0XY6kIFS2X8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# contract value: 5\n",
        "  \n",
        "# directors:{\n",
        "    \n",
        "#     [4, 6]: True\n",
        "#     [6, 20]: False\n",
        "#     [2, inf]: True\n",
        "#     [-inf, 4]: False\n",
        "    \n",
        "# } ==> True | False | True | False ===> True\n",
        "  \n",
        "  \n",
        "  \n",
        "# all:{    \n",
        "#     [6, inf]: False\n",
        "#     [6, 20]: False\n",
        "#     [2, inf]: True\n",
        "#     [-inf, 4]: False\n",
        "    \n",
        "# } ==> True | False | True | False ===> True\n",
        "  \n",
        "  \n",
        "# for level in OrgLevel:\n",
        "#   for constraint in OrgLevel.constraints:\n",
        "#     if contract.value in range(constraint.min, constraint.max) \n",
        "#       if constraint.subj == contract.subj\n",
        "#         raise RedFlag('–∞ —á–æ –ø—Ä–æ—Ç–æ–∫–æ–ª-—Ç–æ –µ—Å—Ç—å?')\n",
        "\n",
        "# if contract.value > max ( every constraint.max)\n",
        "#    raise RedFlag('—á—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEM5XvFcA_J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Charitiy finder"
      ]
    },
    {
      "metadata": {
        "id": "PhCbZyLYiBnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_charity_patterns(factory):\n",
        "  def cp(name, tuples):\n",
        "    return factory.create_pattern(name, tuples)\n",
        "\n",
        "  cp('x_charity_1', ('–¥–æ–≥–æ–≤–æ—Ä',\n",
        "                     '–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ',\n",
        "                     '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è'))\n",
        "\n",
        "  cp('x_charity_1.1', ('–æ–¥–æ–±—Ä–µ–Ω–∏–µ –≤–Ω–µ—Å–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –∫–∞–∫–∏—Ö-–ª–∏–±–æ –≤–∫–ª–∞–¥–æ–≤ –∏–ª–∏ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–∏',\n",
        "                       '–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ',\n",
        "                       '—Ü–µ–ª–∏'))\n",
        "\n",
        "  cp('x_charity_1.2', ('–æ–¥–æ–±—Ä–µ–Ω–∏–µ –≤–Ω–µ—Å–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –∫–∞–∫–∏—Ö-–ª–∏–±–æ –≤–∫–ª–∞–¥–æ–≤ –∏–ª–∏',\n",
        "                       '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–π',\n",
        "                       '–Ω–∞ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏ '))\n",
        "\n",
        "  cp('x_charity_2', ('–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ',\n",
        "                     '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π',\n",
        "                     '–ø–æ–º–æ—â–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π')),\n",
        "\n",
        "  cp('x_charity_3', ('—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å–¥–µ–ª–æ–∫',\n",
        "                     ' –¥–∞—Ä–µ–Ω–∏—è ',\n",
        "                     ' '))\n",
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from legal_docs import calculate_distances_per_pattern\n",
        "from ml_tools import filter_values_by_key_prefix, max_exclusive_pattern, relu\n",
        "from patterns import improve_attention_vector\n",
        "from text_tools import get_sentence_bounds_at_index\n",
        "\n",
        "\n",
        "def find_charity_constraints(doc, factory, head_sections:dict ) -> dict:\n",
        "  charity_quotes_by_head_type = {}\n",
        "  for section_name in head_sections:\n",
        "\n",
        "    # section_name = section_prefix + head\n",
        "    # print(head, '->', section_name)\n",
        "    if section_name in doc.sections:\n",
        "      subdoc = doc.sections[section_name].body\n",
        "      # subdoc.calculate_distances_per_pattern(TFAA)\n",
        "\n",
        "      print(section_name)\n",
        "      bounds = find_charity_sentences(subdoc, factory)\n",
        "      charity_quotes_by_head_type[section_name] = bounds\n",
        "\n",
        "      # print('ok')\n",
        "  return charity_quotes_by_head_type\n",
        "\n",
        "\n",
        "def find_charity_sentences(subdoc, factory) -> List:\n",
        "  \"\"\"\n",
        "    returns list of tuples (slice, confidence, summa-of-attention-)\n",
        "  \"\"\"\n",
        "\n",
        "  calculate_distances_per_pattern(subdoc, factory, merge=True, pattern_prefix='x_charity_')\n",
        "\n",
        "  slices = []\n",
        "  vectors = filter_values_by_key_prefix(subdoc.distances_per_pattern_dict, 'x_charity_')\n",
        "  vectors_i = []\n",
        "  for v in vectors:\n",
        "    if max(v) > 0.6:\n",
        "      vector_i, _ = improve_attention_vector(subdoc.embeddings, v, relu_th=0.6, mix=0.9)\n",
        "      vectors_i.append(vector_i)\n",
        "    else:\n",
        "      vectors_i.append(v)\n",
        "\n",
        "  x = max_exclusive_pattern(vectors_i)\n",
        "  x = relu(x, 0.8)\n",
        "  subdoc.distances_per_pattern_dict['$at_x_charity_'] = x\n",
        "\n",
        "  dups = {}\n",
        "  for i in np.nonzero(x)[0]:\n",
        "    bounds = get_sentence_bounds_at_index(i, subdoc.tokens)\n",
        "\n",
        "    if bounds[0] not in dups:\n",
        "      sl = slice(bounds[0], bounds[1])\n",
        "      sum_ = sum(x[sl])\n",
        "      confidence = 'x'\n",
        "      #       confidence = np.mean( np.nonzero(x[sl]) )\n",
        "      nonzeros_count = len(np.nonzero(x[sl])[0])\n",
        "      print('nonzeros_count=', nonzeros_count)\n",
        "      confidence = 0\n",
        "\n",
        "      if nonzeros_count > 0:\n",
        "        confidence = sum_ / nonzeros_count\n",
        "      print('confidence=', confidence)\n",
        "      if confidence > 0.8:\n",
        "        # GLOBALS__['renderer'].render_color_text(subdoc.tokens_cc[sl],\n",
        "        #                                         subdoc.distances_per_pattern_dict['$at_x_charity_'][sl], _range=(0, 1))\n",
        "        print(i, sum_)\n",
        "\n",
        "        slices.append((sl, confidence, sum_))\n",
        "\n",
        "      dups[bounds[0]] = True\n",
        "\n",
        "  return slices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##---test\n",
        "\n",
        "# find_charity_constraints(self.doc, GLOBALS__['CharterAnlysingContext'].pattern_factory, self._get_head_sections())\n",
        "# GLOBALS__['CharterAnlysingContext'].\n",
        "\n",
        "charity_constraints = find_charity_constraints(doc, \n",
        "                                               GLOBALS__['CharterAnlysingContext'].pattern_factory, \n",
        "                                               GLOBALS__['CharterAnlysingContext']._get_head_sections())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QflbPNjUDQJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Render charity constraints"
      ]
    },
    {
      "metadata": {
        "id": "AsbjbSMmDOnN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#---CharterRenderer\n",
        "_CTX = GLOBALS__['CharterAnlysingContext']\n",
        "_cr = CharterRenderer()\n",
        "_cr.render_charter_parsing_results(_CTX.doc, _CTX.org, _CTX.constraints, _CTX.charity_constraints)\n",
        "# _cr.render_contents(_CTX.doc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLEZH3mDiRUm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # GLOBALS__['renderer'].render_color_text(subdoc.tokens_cc, subdoc.distances_per_pattern_dict['x_charity_1.2'], _range=(0,1))\n",
        "# # x = max_exclusive_pattern_by_prefix(subdoc.distances_per_pattern_dict, 'x_charity_')\n",
        "# import numpy as np\n",
        "# from text_tools import get_sentence_bounds_at_index\n",
        "# from ml_tools import filter_values_by_key_prefix,max_exclusive_pattern, momentum, smooth_safe, smooth, relu\n",
        "# from patterns import improve_attention_vector\n",
        "# vectors = filter_values_by_key_prefix(subdoc.distances_per_pattern_dict, 'x_charity_')\n",
        "# vectors_i=[]\n",
        "# for v in vectors:\n",
        "#   if max(v)>0.6:\n",
        "#     vector_i, _ = improve_attention_vector(subdoc.embeddings, v, relu_th=0.6, mix=0.9)\n",
        "#     vectors_i.append(vector_i)\n",
        "#   else:\n",
        "#     vectors_i.append(v)\n",
        "    \n",
        "# x = max_exclusive_pattern(vectors_i)\n",
        "# x=relu(x, 0.8)\n",
        "# i = np.argmax(x)\n",
        "# sl = slice( i-150, i+150)\n",
        "\n",
        "\n",
        "\n",
        "# dups={}\n",
        "# for i in np.nonzero(x)[0]:\n",
        "#   bounds = get_sentence_bounds_at_index( i, subdoc.tokens)\n",
        "  \n",
        "#   if bounds[0] not in dups:\n",
        "#     sl=slice(bounds[0], bounds[1])\n",
        "#     sum_ = sum(x[sl])\n",
        "\n",
        "#     if sum_ >2:\n",
        "#       GLOBALS__['renderer'].render_color_text(subdoc.tokens_cc[sl], x[sl], _range=(0,1))\n",
        "#       print(i, sum_)\n",
        "# #       yeld (bounds)\n",
        "\n",
        "#     dups[bounds[0]]=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oX4OH7NMBNUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Miscl\n"
      ]
    },
    {
      "metadata": {
        "id": "8h2m0krc_KM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  doc.embedd(GLOBALS__['ContractAnlysingContext'].hadlines_factory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUnmEKg0HE0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  # GLOBALS__['ContractAnlysingContext'].hadlines_factory.\n",
        "\n",
        "  doc.calculate_distances_per_pattern(GLOBALS__['CharterAnlysingContext'].hadlines_factory)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqnUuNcxHlmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(doc.distances_per_pattern_dict.keys())\n",
        "from legal_docs import rectifyed_sum_by_pattern_prefix\n",
        "\n",
        "from ml_tools import max_exclusive_pattern_by_prefix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vv = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'headline.name.1')\n",
        "print(vv[0:10])\n",
        "\n",
        "\n",
        "\n",
        "# GLOBALS__['renderer'].render_color_text(doc.tokens_cc, vv)\n",
        "\n",
        "for s in doc.sections:\n",
        "  print (s)\n",
        "  \n",
        "if 'head.directors' in doc.sections:\n",
        "  subdoc = doc.sections['head.directors'].body\n",
        "  for ky in subdoc.distances_per_pattern_dict:\n",
        "    print(ky)\n",
        "  GLOBALS__['renderer'].render_color_text(subdoc.tokens_cc, subdoc.distances_per_pattern_dict['deal_attention_vector'])\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Rv4ja_xLmxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " \n",
        "\n",
        "level_by_line = [ max ( i._possible_levels) for i in doc.structure.structure ]\n",
        " \n",
        "\n",
        "\n",
        "headlines_attention_vector=[]\n",
        "for i in doc.structure.structure:\n",
        "  l = i.span[1]-i.span[0]\n",
        "  headlines_attention_vector+=[level_by_line[ i.line_number ]]*l\n",
        "  \n",
        "# print (pv[0:100])\n",
        "headlines_attention_vector = normalize(headlines_attention_vector)\n",
        "headlines_attention_vector = relu(headlines_attention_vector, 0.4)\n",
        "\n",
        "# headlines_attention_vector = smooth(headlines_attention_vector, 20)\n",
        "\n",
        "av= relu(headlines_attention_vector/2 + vv, 0.6)\n",
        "av = momentum(av, 0.99)\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(doc.tokens_cc, av)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kT3NPG52fAjK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Meta-pattern! (Cool!!(!))\n",
        "—Ç–∏–ø–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ"
      ]
    },
    {
      "metadata": {
        "id": "xUcxb8lmc1Dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ---------\n",
        "from patterns import FuzzyPattern\n",
        "best_id=np.argmax(av)\n",
        "best_embedding_v = doc.embeddings[best_id]\n",
        "\n",
        "meta_pattern=FuzzyPattern('s-meta-na')\n",
        "meta_pattern.embeddings =  np.array([ best_embedding_v ])\n",
        "\n",
        "meta_pattern_attention = 1.0 - meta_pattern._eval_distances(doc.embeddings)\n",
        "meta_pattern_attention = relu(meta_pattern_attention,  0.7)\n",
        "\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(doc.tokens_cc, meta_pattern_attention, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uFctYf9efjFt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "ax = plt.axes()\n",
        "span=1400\n",
        "ax.plot(meta_pattern_attention[best_id-span:best_id+span], alpha=0.5, color='green', label='meta_pattern_attention');\n",
        "ax.plot(normalize(av[best_id-span:best_id+span]), alpha=0.5, color='red', label='av');\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsmnVkodzR7h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 2. –î–æ–≥–æ–≤–æ—Ä"
      ]
    },
    {
      "metadata": {
        "id": "ysQTF-wAzwtV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_init_the_code(True)\n",
        "\n",
        "uploaded = interactive_upload('–î–æ–≥–æ–≤–æ—Ä –≤ –∑–∞–∫–æ–Ω–µ')\n",
        "\n",
        "GLOBALS__['ContractAnlysingContext'].analyze_contract(uploaded[0])\n",
        "doc = GLOBALS__['ContractAnlysingContext'].contract\n",
        "\n",
        "GLOBALS__['renderer'].render_subj(doc)\n",
        "GLOBALS__['renderer'].render_contents(doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHF4mlGBAEAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_init_the_code(True)\n",
        "GLOBALS__['renderer'].render_subj(doc)\n",
        "GLOBALS__['renderer'].render_contents(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGv3HNDn5sFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ext"
      ]
    },
    {
      "metadata": {
        "id": "J1iHjpFi5trH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[(y.confidence, y.value) for y in sorted(GLOBALS__['ContractAnlysingContext'].contract.subjects, key=lambda x:-x.confidence )]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K5oq9UKV6GjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['ContractAnlysingContext'].contract.subjects[0].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gp_RaSmD6Jkx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['ContractAnlysingContext'].contract.subjects[1].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvJhXqHE6NuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['ContractAnlysingContext'].contract.subjects[2].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "li0QqB4C7OiN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['CharterAnlysingContext'].charter.subjects[2].__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GnyR3dSIzUyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 3. –ü–æ–∏—Å–∫ –≤—Ä–µ–¥–∞"
      ]
    },
    {
      "metadata": {
        "id": "3ob6Mmo4z5BZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "find_and_show_violations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qo8nIebSl7rE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### subj correlations"
      ]
    },
    {
      "metadata": {
        "id": "KWXN-Ecjl_GZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(GLOBALS__['ContractAnlysingContext'].contract.sections['subj'].body.untokenize_cc())\n",
        "print(\"-\"*20)\n",
        "print(GLOBALS__['CharterAnlysingContext'].constraints['head.directors']['sentences'][1]['subdoc'].untokenize_cc())\n",
        "\n",
        "\n",
        "docA=GLOBALS__['ContractAnlysingContext'].contract.sections['subj'].body\n",
        "docB=GLOBALS__['CharterAnlysingContext'].constraints['head.directors']['sentences'][1]['subdoc']\n",
        "\n",
        "\n",
        "GLOBALS__['CharterAnlysingContext'].constraints['head.directors']['sentences'][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TboOY5t3plhK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GLOBALS__['CharterAnlysingContext'].doc.constraints\n",
        "_init_the_code(reset=True)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PGcbxq5poBH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from patterns import dist_mean_cosine as DF\n",
        "from text_tools import untokenize\n",
        "\n",
        "#\n",
        "# docA.embedd( GLOBALS__['CharterAnlysingContext'].pattern_factory )\n",
        "# docB.embeddings[0:10]\n",
        "# docA.embeddings = None\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def match_contract_to_charter_constraints(contract, charter, charter_constraints, charity_constraints):\n",
        "  \"\"\"\n",
        "  find best constraint to apply to Contract\n",
        "  \"\"\"\n",
        "\n",
        "  r_quotes = []\n",
        "  r_vector = []\n",
        "\n",
        "  quote_slice = slice(0, 17)\n",
        "\n",
        "  if 'subj' not in contract.sections:\n",
        "    raise ValueError(\"contract has no subject section\")\n",
        "\n",
        "  subj = contract.sections['subj'].body\n",
        "  print(subj.untokenize_cc())\n",
        "  print('------')\n",
        "  if subj.embeddings is None:\n",
        "    print(\"Subj embeddings are gone, restoring...\")\n",
        "    subj.embeddings = contract.embeddings[subj.start:subj.end]\n",
        "    #     subj.tokens = doc.tokens[subj.start:subj.end]\n",
        "    #     subj.tokens_cc = doc.tokens_cc[subj.start:subj.end]\n",
        "    #     subj.embedd( GLOBALS__['CharterAnlysingContext'].pattern_factory )\n",
        "    print('\\t\\t sample:', subj.embeddings[0][1:10])\n",
        "\n",
        "  for head_type in charter_constraints:\n",
        "\n",
        "    ##charity:\n",
        "    if head_type in charity_constraints:\n",
        "      print(f'{head_type} has charity constrinats')\n",
        "      \n",
        "      charity_constraints_by_head = charity_constraints[head_type]\n",
        "      charity_constraints_by_head_new = []\n",
        "      \n",
        "      charity_constraints['new.'+head_type] = charity_constraints_by_head_new\n",
        "      \n",
        "      for i in range(len(charity_constraints_by_head)):\n",
        "        _tuple = charity_constraints_by_head[i] \n",
        "#       for cc in charity_constraints[head_type]:\n",
        "        _slice = _tuple[0]\n",
        "        emb_charter = charter.sections[head_type].body.embeddings[_slice]\n",
        "        \n",
        "        distance = 1 - DF(emb_charter,  subj.embeddings[5:])\n",
        "        \n",
        "#         cc.add['subj_correlation'] = distance\n",
        "        \n",
        "#         detupling\n",
        "        charity_constraints_by_head_new.append ( {\n",
        "            'slice':_slice,\n",
        "            'subj_correlation': distance,\n",
        "            'confidence': _tuple[1],\n",
        "            'sum': _tuple[2]\n",
        "        })\n",
        "  \n",
        "        print('\\t'*4, 'cc=', charity_constraints_by_head_new[i])\n",
        "        \n",
        "        #         print('\\t\\t---CC', cc[0])\n",
        "        \n",
        "\n",
        "    #       GLOBALS__['CharterAnlysingContext'].doc.sections['head.directors'].body.embeddings[_slice]\n",
        "\n",
        "    ##------------------------charity end\n",
        "    print(f'measuring {head_type} constraints...'.upper())\n",
        "    cc = charter_constraints[head_type]\n",
        "    quotes = cc['sentences']\n",
        "    for quote in quotes:\n",
        "      print()\n",
        "      _q = untokenize(quote['subdoc'].tokens_cc[quote_slice])\n",
        "      print(_q)\n",
        "\n",
        "      distance = 1 - DF(quote['subdoc'].embeddings[quote_slice],\n",
        "                        subj.embeddings[5:])\n",
        "\n",
        "      quote['subj_correlation'] = distance\n",
        "\n",
        "      print(f'distance = {distance:.4f}')\n",
        "\n",
        "      r_quotes.append(_q)\n",
        "      r_vector.append(distance)\n",
        "      r_quotes.append('\\n')\n",
        "      r_vector.append(distance)\n",
        "\n",
        "  GLOBALS__['renderer'].render_color_text(r_quotes, r_vector)\n",
        "  print(r_vector)\n",
        "  print(r_quotes)\n",
        "  \n",
        "  \n",
        "match_contract_to_charter_constraints(GLOBALS__['ContractAnlysingContext'].contract,\n",
        "                                      GLOBALS__['CharterAnlysingContext'].doc,\n",
        "                                      GLOBALS__['CharterAnlysingContext'].constraints,\n",
        "                                      GLOBALS__['CharterAnlysingContext'].charity_constraints\n",
        "                                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hiOWiOKX9MEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from ml_tools import ProbableValue, np, TokensWithAttention\n",
        "from renderer import as_warning, as_offset, as_error_html, as_msg, as_quote, as_currency\n",
        "from text_tools import untokenize\n",
        "from transaction_values import ValueConstraint\n",
        "\n",
        "# //copy\n",
        "class ViolationsFinder:\n",
        "\n",
        "  def find_ranges_by_group(self, charter_constraints, m_convert, verbose=False):\n",
        "    ranges_by_group = {}\n",
        "    for head_group in charter_constraints:\n",
        "      #     print('-' * 20)\n",
        "      group_c = charter_constraints[head_group]\n",
        "      data = self._combine_constraints_in_group(group_c, m_convert, verbose)\n",
        "      ranges_by_group[head_group] = data\n",
        "    return ranges_by_group\n",
        "\n",
        "  @staticmethod\n",
        "  def _combine_constraints_in_group(group_c, m_convert, verbose=False):\n",
        "    # print(group_c)\n",
        "    # print(group_c['section'])\n",
        "\n",
        "    data = {\n",
        "      'name': group_c['section'],\n",
        "      'ranges': {}\n",
        "    }\n",
        "\n",
        "    sentences = group_c['sentences']\n",
        "    #   print (charter_constraints[head_group]['sentences'])\n",
        "    sentence_id = 0\n",
        "    for sentence in sentences:\n",
        "      constraint_low = None\n",
        "      constraint_up = None\n",
        "\n",
        "      sentence_id += 1\n",
        "      #     print (sentence['constraints'])\n",
        "\n",
        "      s_constraints = sentence['constraints']\n",
        "      # –±–æ–ª—å—à–∏–µ –∏—â–µ–º\n",
        "      maximals = [x for x in s_constraints if x.value.sign > 0]\n",
        "\n",
        "      if len(maximals) > 0:\n",
        "        constraint_low = min(maximals, key=lambda item: m_convert(item.value).value)\n",
        "        # if verbose:\n",
        "        #   print(\"all maximals:\")\n",
        "        #   self.renderer.render_values(maximals)\n",
        "        #   print('\\t\\t\\t constraint_low', constraint_low.value.value)\n",
        "        #   self.renderer.render_values([constraint_low])\n",
        "\n",
        "      minimals = [x for x in s_constraints if x.value.sign <= 0]\n",
        "      if len(minimals) > 0:\n",
        "        constraint_up = min(minimals, key=lambda item: m_convert(item.value).value)\n",
        "        # if verbose:\n",
        "        #   print(\"all: minimals\")\n",
        "        #   self.renderer.render_values(minimals)\n",
        "        #   print('\\t\\t\\t constraint_upper', constraint_up.value.value)\n",
        "        #   self.renderer.render_values([constraint_up])\n",
        "        #   print(\"----X\")\n",
        "\n",
        "      if constraint_low is not None or constraint_up is not None:\n",
        "        data['ranges'][sentence_id] = VConstraint(constraint_low, constraint_up, group_c)\n",
        "\n",
        "    return data\n",
        "    # ==================================================================VIOLATIONS\n",
        "\n",
        "\n",
        "class VConstraint:\n",
        "  def __init__(self, lower, upper, head_group):\n",
        "    _emp = TokensWithAttention([''], [0])\n",
        "    self.lower = ProbableValue(ValueConstraint(0, 'RUB', +1, context=_emp), 0)\n",
        "    self.upper = ProbableValue(ValueConstraint(np.inf, 'RUB', -1, context=_emp), 0)\n",
        "\n",
        "    if lower is not None:\n",
        "      self.lower = lower\n",
        "\n",
        "    if upper is not None:\n",
        "      self.upper = upper\n",
        "\n",
        "    self.head_group = head_group\n",
        "\n",
        "  @staticmethod\n",
        "  def maybe_convert(v: ValueConstraint, convet_m):\n",
        "    html = \"\"\n",
        "    v_converted = v\n",
        "    if v.currency != 'RUB':\n",
        "      v_converted = convet_m(v)\n",
        "      html += as_warning(f\"–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–∞–ª—é—Ç {as_currency(v)} --> RUB \")\n",
        "      html += as_offset(as_warning(f\"–ø—Ä–∏–º–µ—Ä–Ω–æ: {as_currency(v)} ~~  {as_currency(v_converted)}  \"))\n",
        "    return v, v_converted, html\n",
        "\n",
        "  def check_contract_value(self, _v: ProbableValue, convet_m, renderer):\n",
        "    greather_lower = False\n",
        "    greather_upper = False\n",
        "\n",
        "    if _v is None:\n",
        "      return as_error_html(\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞\")\n",
        "    v: ValueConstraint = _v.value\n",
        "\n",
        "    if v is None:\n",
        "      return as_error_html(\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞\")\n",
        "\n",
        "    if v.value is None:\n",
        "      return as_error_html(f\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞ {v.currency}\")\n",
        "    ###----\n",
        "\n",
        "    lower_v = None\n",
        "    upper_v = None\n",
        "    if self.lower is not None:\n",
        "      lower_v: ValueConstraint = self.lower.value\n",
        "    if self.upper is not None:\n",
        "      upper_v: ValueConstraint = self.upper.value\n",
        "\n",
        "    html = as_msg(f\"–î–∏–∞–ø–∞–∑–æ–Ω: {as_currency(lower_v)} < ..... < {as_currency(upper_v)}\")\n",
        "\n",
        "    v, v_converted, h = self.maybe_convert(v, convet_m)\n",
        "    html += h\n",
        "\n",
        "    if self.lower is not None:\n",
        "      lower_v: ValueConstraint = self.lower.value\n",
        "      lower_v, lower_converted, h = self.maybe_convert(lower_v, convet_m)\n",
        "      html += h\n",
        "\n",
        "      if v_converted.value >= lower_converted.value:\n",
        "        greather_lower = True\n",
        "        html += as_warning(\"—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ...\".upper())\n",
        "        html += as_warning(\n",
        "          f\"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)}  –ë–û–õ–¨–®–ï –Ω–∏–∂–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(lower_converted)} \")\n",
        "        html += as_quote(untokenize(lower_v.context.tokens))\n",
        "\n",
        "    if self.upper is not None:\n",
        "\n",
        "      upper_v: ValueConstraint = self.upper.value\n",
        "      upper_v, upper_converted, h = self.maybe_convert(upper_v, convet_m)\n",
        "      html += h\n",
        "\n",
        "      if v_converted.value >= upper_converted.value:\n",
        "\n",
        "        html += as_error_html(\n",
        "          f\"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)} –ë–û–õ–¨–®–ï –≤–µ—Ä—Ö–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(upper_converted)} \")\n",
        "\n",
        "      elif greather_lower:\n",
        "        head_name = self.head_group['section']\n",
        "        html += as_error_html(f'—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã \"{head_name.upper()}\"')\n",
        "\n",
        "        if lower_v.context is not None:\n",
        "          html += as_quote(renderer.to_color_text(lower_v.context.tokens, lower_v.context.attention, _range=[0, 1]))\n",
        "\n",
        "        if upper_v.context is not None:\n",
        "          html += '<br>'\n",
        "          html += as_quote(renderer.to_color_text(upper_v.context.tokens, upper_v.context.attention, _range=[0, 1]))\n",
        "\n",
        "    return html \n",
        "  \n",
        "  \n",
        "  \n",
        "  # AZ:-FINDING_VIOLATIONS--------------------------------------------------------\n",
        "def find_and_show_violations():\n",
        "  from IPython.core.display import display, HTML\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  from renderer import as_headline_2, as_error_html\n",
        "\n",
        "  charterAnlysingContext: CharterAnlysingContext = GLOBALS__['CharterAnlysingContext']\n",
        "  contractAnlysingContext: ContractAnlysingContext = GLOBALS__['ContractAnlysingContext']\n",
        "\n",
        "  contract = contractAnlysingContext.contract\n",
        "  charter = charterAnlysingContext.doc\n",
        "  \n",
        "  charter_constraints = charterAnlysingContext.constraints  # XXX: move to doc\n",
        "  charity_constraints = charterAnlysingContext.charity_constraints  # XXX: move to doc\n",
        "\n",
        "  \n",
        "\n",
        "  import copy\n",
        "\n",
        "  def convert(v):\n",
        "    v_converted = copy.copy(v)\n",
        "    if v.currency in currency_converter:\n",
        "      v_converted.value = currency_converter[v.currency] * v.value\n",
        "      v_converted.currency = 'RUB'\n",
        "      return v_converted\n",
        "    else:\n",
        "      display(HTML(as_error_html(\n",
        "        f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "      return v\n",
        "\n",
        "  best_value = contractAnlysingContext.find_contract_best_value(convert)\n",
        "\n",
        "  # rendering:----------------------------\n",
        "\n",
        "  def _render_violations(ranges_by_group, best_value):\n",
        "    \n",
        "    for group_key in ranges_by_group:\n",
        "      group = ranges_by_group[group_key]\n",
        "      display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "      for rk in group['ranges']:\n",
        "        r = group['ranges'][rk]\n",
        "        display(HTML(r.check_contract_value(best_value, convert, renderer)))\n",
        "\n",
        "        \n",
        "  renderer = GLOBALS__['renderer']\n",
        "  renderer.render_subj(contract)\n",
        "  \n",
        "  print('---')\n",
        "  print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "  renderer.render_values([best_value])\n",
        "  renderer.render_color_text(best_value.value.context.tokens, best_value.value.context.attention, _range=[0, 1])\n",
        "\n",
        "  violations_finder = ViolationsFinder()####//XXXX: remplaced\n",
        "  \n",
        "  \n",
        "  _render_violations(    \n",
        "    violations_finder.find_ranges_by_group(charter_constraints, convert, verbose=False),\n",
        "    best_value)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "find_and_show_violations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xOiDjj3KQN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contract= GLOBALS__['ContractAnlysingContext'].contract\n",
        "charter = GLOBALS__['CharterAnlysingContext'].doc\n",
        "# if( GLOBALS__['ContractAnlysingContext'].contract.subject[0] == 'charity'):\n",
        "#   print('charity')\n",
        "\n",
        "charity_constraints =  GLOBALS__['CharterAnlysingContext'].charity_constraints\n",
        "for cc in charity_constraints:\n",
        "  if cc[:4]=='new.':\n",
        "    print(cc[4:].upper() )\n",
        "    constraints = charity_constraints[cc]\n",
        "    for constraint in constraints:\n",
        "#       print(constraint)\n",
        "      if constraint['subj_correlation'] > 0.5:\n",
        "        GLOBALS__['ContractAnlysingContext'].renderer.render_subj(contract)\n",
        "        print(contract.sections['subj'].body.untokenize_cc())\n",
        "#         GLOBALS__['ContractAnlysingContext'].renderer.render_color_text(contract.sections['subj'].tokens, contract.sections['subj'].)\n",
        "\n",
        "        print (f\"\\n–ö–æ—Ä–µ–ª—è—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —É—Å—Ç–∞–≤–∞: {constraint['subj_correlation']:.4f}\\n\\n\"  )\n",
        "        print(constraint['slice'])\n",
        "\n",
        "# contract.sections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uZVU4M1osVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIST_FUNC(docA.embeddings,  docB.embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQMHGw2NzZU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 4. –ü—Ä–æ—Ç–æ–∫–æ–ª\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "85aO_7B1zKFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = interactive_upload('–ü—Ä–æ—Ç–æ–∫–æ–ª, –ø—Ä–æ—Ç–æ–∫—É—é –∏ –±—É–¥—É –ø—Ä–æ—Ç–æ–∫–æ–≤–∞—Ç—å')\n",
        "GLOBALS__['ProtocolAnlysingContext'].process(uploaded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKaWFEoWcK_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## do preparation here   \n",
        "\n",
        "#1.\n",
        "_init_import_code_from_gh()\n",
        "#2.\n",
        "_init_embedder()\n",
        "#3.\n",
        "_init_the_code()\n",
        "#4. \n",
        "_init_charters()\n",
        "#5. \n",
        "# _init_contracts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzn0JxepKxei",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sample Charter"
      ]
    },
    {
      "metadata": {
        "id": "BiDo5o2YcRkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yi0yZHbPsXHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SAMPLE=\"\"\"\n",
        " \n",
        "\n",
        "–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "–£—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª\n",
        "\n",
        "\n",
        "xxvii –£—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª –û–±—â–µ—Å—Ç–≤–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∏–∑ –Ω–æ–º–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–æ–ª–µ–π –µ–≥–æ –£—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç  6 734 244 615 (–®–µ—Å—Ç—å –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Å–µ–º—å—Å–æ—Ç —Ç—Ä–∏–¥—Ü–∞—Ç—å —á–µ—Ç—ã—Ä–µ –º–∏–ª–ª–∏–æ–Ω–∞ –¥–≤–µ—Å—Ç–∏ —Å–æ—Ä–æ–∫ —á–µ—Ç—ã—Ä–µ —Ç—ã—Å—è—á–∏ —à–µ—Å—Ç—å—Å–æ—Ç –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å) —Ä—É–±–ª–µ–π.\n",
        "\n",
        "\n",
        "\n",
        "xxxv –û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–µ –≤–ø—Ä–∞–≤–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–∏–±—ã–ª–∏ –û–±—â–µ—Å—Ç–≤–∞ –º–µ–∂–¥—É –£—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –û–±—â–µ—Å—Ç–≤–∞:\n",
        "\n",
        "       ‚Ä¢ –¥–æ –ø–æ–ª–Ω–æ–π –æ–ø–ª–∞—Ç—ã –≤—Å–µ–≥–æ —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ –û–±—â–µ—Å—Ç–≤–∞;\n",
        "\n",
        "     \n",
        "\n",
        "–û—Ä–≥–∞–Ω—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
        "\n",
        "\n",
        "lxxxiv –û—Ä–≥–∞–Ω–∞–º–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞ —è–≤–ª—è—é—Ç—Å—è:\n",
        "\n",
        "\n",
        "       ‚Ä¢ –û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞;\n",
        "\n",
        "\n",
        "       ‚Ä¢ –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ‚Äì –ö–æ–ª–ª–µ–≥–∏–∞–ª—å–Ω—ã–π –æ—Ä–≥–∞–Ω —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è;\n",
        "\n",
        "\n",
        "       ‚Ä¢ –ü—Ä–∞–≤–ª–µ–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–∞ - –ö–æ–ª–ª–µ–≥–∏–∞–ª—å–Ω—ã–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ä–≥–∞–Ω;\n",
        "\n",
        "\n",
        "       ‚Ä¢ –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä - –ï–¥–∏–Ω–æ–ª–∏—á–Ω—ã–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ä–≥–∞–Ω.\n",
        "\n",
        "\n",
        "–û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞\n",
        "\n",
        "\n",
        "lxxxvi –í—ã—Å—à–∏–º –æ—Ä–≥–∞–Ω–æ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞. –û—á–µ—Ä–µ–¥–Ω–æ–µ –û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞  —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∏ —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –û–±—â–µ—Å—Ç–≤–∞ –∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –≥–æ–¥ –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ –ø–µ—Ä–∏–æ–¥ —Å 1 –º–∞—Ä—Ç–∞ –ø–æ 30 –∞–ø—Ä–µ–ª—è. –ü—Ä–æ–≤–æ–¥–∏–º—ã–µ –ø–æ–º–∏–º–æ –æ—á–µ—Ä–µ–¥–Ω–æ–≥–æ –û–±—â–∏–µ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ —è–≤–ª—è—é—Ç—Å—è –≤–Ω–µ–æ—á–µ—Ä–µ–¥–Ω—ã–º–∏. –í–Ω–µ–æ—á–µ—Ä–µ–¥–Ω—ã–µ –û–±—â–∏–µ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è –≤ —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ —ç—Ç–æ–≥–æ —Ç—Ä–µ–±—É—é—Ç –∏–Ω—Ç–µ—Ä–µ—Å—ã –û–±—â–µ—Å—Ç–≤–∞.\n",
        "\n",
        "\n",
        "lxxxvii –ö –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n",
        "\n",
        "    \n",
        "\n",
        "       12) —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞, —Å–æ—Å—Ç–∞–≤–∞, —Ñ–æ—Ä–º—ã –∏ –ø–æ—Ä—è–¥–∫–∞ –≤–Ω–µ—Å–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ç–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –≤–Ω–µ—Å–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–≤ –£—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –û–±—â–µ—Å—Ç–≤–∞;\n",
        "       13) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö —Å–¥–µ–ª–æ–∫, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ–º, –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ–º –∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç—á—É–∂–¥–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –ø—Ä—è–º–æ –∏–ª–∏ –∫–æ—Å–≤–µ–Ω–Ω–æ –∏–º—É—â–µ—Å—Ç–≤–∞, —Ü–µ–Ω–∞ –∏–ª–∏ –±–∞–ª–∞–Ω—Å–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–≥–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 25 (–î–≤–∞–¥—Ü–∞—Ç—å –ø—è—Ç—å) –∏ –±–æ–ª–µ–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –±–∞–ª–∞–Ω—Å–æ–≤–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–º—É—â–µ—Å—Ç–≤–∞ –û–±—â–µ—Å—Ç–≤–∞ –ø–æ –¥–∞–Ω–Ω—ã–º –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—Ç—á–µ—Ç–Ω—É—é –¥–∞—Ç—É;\n",
        "       14) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫, –≤ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –∏–º–µ–µ—Ç—Å—è –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ —Ü–µ–Ω–∞ –∏–ª–∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–º—É—â–µ—Å—Ç–≤–∞, —è–≤–ª—è—é—â–µ–≥–æ—Å—è –ø—Ä–µ–¥–º–µ—Ç–æ–º —Å–¥–µ–ª–∫–∏,  –ø—Ä–µ–≤—ã—à–∞–µ—Ç 10 (–¥–µ—Å—è—Ç—å) –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –±–∞–ª–∞–Ω—Å–æ–≤–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤  –±–∞–ª–∞–Ω—Å–æ–≤–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–º—É—â–µ—Å—Ç–≤–∞ –û–±—â–µ—Å—Ç–≤–∞ –ø–æ –¥–∞–Ω–Ω—ã–º –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—Ç—á–µ—Ç–Ω—É—é –¥–∞—Ç—É.\n",
        "       15) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –ª–∏–∫–≤–∏–¥–∞—Ü–∏–∏ –û–±—â–µ—Å—Ç–≤–∞;\n",
        "       26) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫ –º–µ–Ω—ã, –¥–∞—Ä–µ–Ω–∏—è, –∏–Ω—ã—Ö —Å–¥–µ–ª–æ–∫, –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞—é—â–∏—Ö –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ –∏–º—É—â–µ—Å—Ç–≤–∞ –û–±—â–µ—Å—Ç–≤–∞ –ª–∏–±–æ –æ–ø–ª–∞—Ç—É (–≤—Å—Ç—Ä–µ—á–Ω–æ–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ) –≤ –Ω–µ–¥–µ–Ω–µ–∂–Ω–æ–π —Ñ–æ—Ä–º–µ,  –æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å–ø–æ–Ω—Å–æ—Ä—Å–∫–æ–≥–æ –∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å—É–º–º—ã —Å–¥–µ–ª–∫–∏, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —Ç–∞–∫–∏–µ —Å–¥–µ–ª–∫–∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–∏—è –æ—Ä–≥–∞–Ω–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞ –∫–∞–∫ –∫—Ä—É–ø–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –∏ —Å–¥–µ–ª–∫–∏ —Å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é;\n",
        "       27) —Ä–µ—à–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤, –æ—Ç–Ω–µ—Å–µ–Ω–Ω—ã—Ö –∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤, –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ –∏–∑–±—Ä–∞–Ω;\n",
        "       28) —Ä–µ—à–µ–Ω–∏–µ –∏–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –æ—Ç–Ω–µ—Å–µ–Ω–Ω—ã—Ö –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞.\n",
        "\n",
        "\n",
        "\n",
        "–°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –û–±—â–µ—Å—Ç–≤–∞\n",
        "\n",
        "\n",
        "      1. –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –æ–±—â–µ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –û–±—â–µ—Å—Ç–≤–∞, –∞ —Ç–∞–∫–∂–µ –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º —Ä–µ—à–µ–Ω–∏–π –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ –∏ –∏–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –µ–≥–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ä–≥–∞–Ω–∞.\n",
        "\n",
        "      2. –°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 4 (–ß–µ—Ç—ã—Ä–µ—Ö) —á–µ–ª–æ–≤–µ–∫.\n",
        "\n",
        "      3. –ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è:\n",
        "\n",
        "\n",
        "      14) —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ-—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –û–±—â–µ—Å—Ç–≤–∞, –≤ —Ç–æ–º —á–∏—Å–ª–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –≤–Ω—É—Ç—Ä–∏—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç—Ä–æ–ª–µ, –¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–∫–∞—Ö –∏ —Ä–µ–≤–∏–∑–∏—è—Ö;\n",
        "\n",
        "      15) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏–µ–º –¥–µ–Ω–µ–∂–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç –∏ (–∏–ª–∏) –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ–º, –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ–º –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç—á—É–∂–¥–µ–Ω–∏—è, –æ–±—Ä–µ–º–µ–Ω–µ–Ω–∏–µ–º –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—Ä–µ–º–µ–Ω–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –∏–º—É—â–µ—Å—Ç–≤–∞, –≤ —Ä–∞–º–∫–∞—Ö –∫–æ—Ç–æ—Ä—ã—Ö —Ü–µ–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ç—á—É–∂–¥–∞–µ–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è —Å–¥–µ–ª–∫–∏, –ø—Ä–µ–≤—ã—à–∞–µ—Ç –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–¥–µ–ª–∫–∏ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫, –∑–∞–∫–ª—é—á–µ–Ω–Ω—ã—Ö –≤ —Ç–µ—á–µ–Ω–∏–µ 6 (—à–µ—Å—Ç–∏) –º–µ—Å—è—Ü–µ–≤, –≤ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ 50 000 000 (–ü—è—Ç—å–¥–µ—Å—è—Ç –º–∏–ª–ª–∏–æ–Ω–æ–≤) —Ä—É–±–ª–µ–π –∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –¥–∞–Ω–Ω–æ–π —Å—É–º–º—ã –≤ –∏–Ω–æ–π –≤–∞–ª—é—Ç–µ, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —Ç–∞–∫–∏–µ —Å–¥–µ–ª–∫–∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–∏—è –æ—Ä–≥–∞–Ω–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞  –∫–∞–∫ –∫—Ä—É–ø–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –∏ —Å–¥–µ–ª–∫–∏ —Å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é, –∞ —Ç–∞–∫–∂–µ –¥–ª—è —Å–¥–µ–ª–æ–∫, –æ–¥–æ–±—Ä—è–µ–º—ã—Ö –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º–∏ 22)-26) –ø—É–Ω–∫—Ç–∞ 11.2 –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –£—Å—Ç–∞–≤–∞, –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º–∏ 17) ‚Äì22), 30) –ø—É–Ω–∫—Ç–∞ 12.3 –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –£—Å—Ç–∞–≤–∞;\n",
        "\n",
        "      16) –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –Ω–∞—á–∞–ª–µ/—É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ª—é–±—ã—Ö —Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤ –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤, —Ü–µ–Ω–∞ –∏—Å–∫–∞ –ø–æ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–µ–≤—ã—à–∞–µ—Ç 50 000 000 (–ü—è—Ç—å–¥–µ—Å—è—Ç –º–∏–ª–ª–∏–æ–Ω–æ–≤) —Ä—É–±–ª–µ–π (–∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –≤ –¥—Ä—É–≥–æ–π –≤–∞–ª—é—Ç–µ) –≤ —Ç–æ–º —á–∏—Å–ª–µ, –ø–æ–¥–ø–∏—Å–∞–Ω–∏–µ –º–∏—Ä–æ–≤–æ–≥–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è, –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ –∏—Å–∫–∞, –æ—Ç–∫–∞–∑ –æ—Ç –∏—Å–∫–∞, –∞ —Ç–∞–∫–∂–µ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∫—Ä–æ–º–µ —Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤, –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ç–Ω–µ—Å–µ–Ω–æ –∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞;\n",
        "\n",
        "\n",
        "–ü—Ä–∞–≤–ª–µ–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–∞\n",
        "\n",
        "\n",
        " 1. –ü—Ä–∞–≤–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –ö–æ–ª–ª–µ–≥–∏–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –æ—Ä–≥–∞–Ω–æ–º –û–±—â–µ—Å—Ç–≤–∞. –ü—Ä–∞–≤–ª–µ–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–∞ –∏–∑–±–∏—Ä–∞–µ—Ç—Å—è –û–±—â–∏–º —Å–æ–±—Ä–∞–Ω–∏–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ  2 (–î–≤—É—Ö) —á–µ–ª–æ–≤–µ–∫ ‚Äì —á–ª–µ–Ω–æ–≤ –ü—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞.\n",
        "\n",
        "\n",
        "12. –í —Å–ª—É—á–∞–µ –Ω–µ–ø—Ä–∏–Ω—è—Ç–∏—è –ü—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–µ—à–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–º—É –∏ —Ç–æ–º—É –∂–µ –≤–æ–ø—Ä–æ—Å—É –≤ —Ö–æ–¥–µ 2 (–î–≤—É—Ö) –Ω–∞–¥–ª–µ–∂–∞—â–µ —Å–æ–∑–≤–∞–Ω–Ω—ã—Ö –∑–∞—Å–µ–¥–∞–Ω–∏–π –ü—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ –ª—é–±—ã–º –ø—Ä–∏—á–∏–Ω–∞–º, –≤–∫–ª—é—á–∞—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–≤–æ—Ä—É–º–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –Ω–µ –±—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ, –≤—ã–Ω–æ—Å–∏—Ç—Å—è –Ω–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ (–≤–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–æ–≤–µ—Å—Ç–∫—É –¥–Ω—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ–∑—ã–≤–∞–µ–º–æ–≥–æ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤). –í–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ —Å–æ–º–Ω–µ–Ω–∏–π, —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å—Å—è –∫–∞–∫ –Ω–µ–ø—Ä–∏–Ω—è—Ç–æ–µ –¥–ª—è —Ü–µ–ª–µ–π –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –ø—É–Ω–∫—Ç–∞, –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –ø—Ä–∏–Ω—è—Ç–æ –≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ—Ç–∏–≤ –Ω–µ–≥–æ –ø—Ä–æ–≥–æ–ª–æ—Å–æ–≤–∞–ª–∏ –∏–ª–∏ –≤–æ–∑–¥–µ—Ä–∂–∞–ª–∏—Å—å –æ—Ç –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –≤—Å–µ —á–ª–µ–Ω—ã –ü—Ä–∞–≤–ª–µ–Ω–∏—è.\n",
        "\n",
        "13. –ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ü—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞ –æ—Ç–Ω–æ—Å—è—Ç—Å—è:\n",
        "\n",
        "\n",
        "\n",
        "         ‚Ä¢ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π (–∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Ñ–∏–ª–∏–∞–ª–æ–≤ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤) –û–±—â–µ—Å—Ç–≤–∞, –∞ —Ç–∞–∫–∂–µ —É—Å–ª–æ–≤–∏–π —Ç—Ä—É–¥–æ–≤—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ª–∏—Ü–∞–º–∏, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –æ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ª–∏—Ü;\n",
        "\n",
        "\n",
        "         ‚Ä¢ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏–µ–º –¥–µ–Ω–µ–∂–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç –∏ (–∏–ª–∏) –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ–º, –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ–º –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç—á—É–∂–¥–µ–Ω–∏—è, –æ–±—Ä–µ–º–µ–Ω–µ–Ω–∏–µ–º –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—Ä–µ–º–µ–Ω–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –∏–º—É—â–µ—Å—Ç–≤–∞, —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–≥–æ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–¥–µ–ª–∫–∏ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫ –≤ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç 1 000 000 (–û–¥–Ω–æ–≥–æ) –º–∏–ª–ª–∏–æ–Ω–∞ —Ä—É–±–ª–µ–π –¥–æ 50 000 000 (–ü—è—Ç—å–¥–µ—Å—è—Ç –º–∏–ª–ª–∏–æ–Ω–æ–≤) —Ä—É–±–ª–µ–π –∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –¥–∞–Ω–Ω–æ–π —Å—É–º–º—ã –≤ –∏–Ω–æ–π –≤–∞–ª—é—Ç–µ, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞  —Ç–∞–∫–∏–µ —Å–¥–µ–ª–∫–∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–∏—è –æ—Ä–≥–∞–Ω–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞  –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ó–∞–∫–æ–Ω–æ–º –∏ –£—Å—Ç–∞–≤–æ–º;\n",
        "\n",
        "\n",
        "         ‚Ä¢ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ –ø—Ä–∏–µ–º–∫–µ –∏ –æ–ø–ª–∞—Ç–µ –û–±—â–µ—Å—Ç–≤–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –ê–≥–µ–Ω—Ç—Å–∫–æ–º—É –¥–æ–≥–æ–≤–æ—Ä—É –Ω–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–∞ ¬´–Æ–∂–Ω–æ-–ü—Ä–∏–æ–±—Å–∫–∏–π –≥–∞–∑–æ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–≤–æ–¥ (–Æ–ü –ì–ü–ó). –ì–∞–∑–æ–∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä–Ω–∞—è —Å—Ç–∞–Ω—Ü–∏—è –Æ–õ–¢ –ü—Ä–∏–æ–±—Å–∫–æ–≥–æ –º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏—è¬ª ‚Ññ10-875 –æ—Ç 29.09.2010 –≥., –∑–∞–∫–ª—é—á–µ–Ω–Ω–æ–º—É –º–µ–∂–¥—É –û–±—â–µ—Å—Ç–≤–æ–º –∏ –û–û–û ¬´–ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–•–∞–Ω—Ç–æ—Å¬ª.\n",
        "\n",
        "\n",
        "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä –û–±—â–µ—Å—Ç–≤–∞\n",
        "\n",
        "\n",
        "cvi –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—É—â–µ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –û–±—â–µ—Å—Ç–≤–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –µ–¥–∏–Ω–æ–ª–∏—á–Ω—ã–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –æ—Ä–≥–∞–Ω–æ–º –û–±—â–µ—Å—Ç–≤–∞ - –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º. –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä –∏–∑–±–∏—Ä–∞–µ—Ç—Å—è –û–±—â–∏–º —Å–æ–±—Ä–∞–Ω–∏–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ —Å—Ä–æ–∫–æ–º –Ω–∞ 3 (–¢—Ä–∏) –≥–æ–¥–∞, –µ—Å–ª–∏ —Ä–µ—à–µ–Ω–∏–µ–º –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–µ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω –∏–Ω–æ–π —Å—Ä–æ–∫. –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–∏–∑–±—Ä–∞–Ω —Ä–µ—à–µ–Ω–∏–µ–º –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑. –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä –¥–µ–π—Å—Ç–≤—É–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –£—Å—Ç–∞–≤–∞ –û–±—â–µ—Å—Ç–≤–∞ –∏ —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞, –∑–∞–∫–ª—é—á–∞–µ–º–æ–≥–æ —Å –Ω–∏–º –û–±—â–µ—Å—Ç–≤–æ–º.\n",
        "\n",
        "\n",
        "       ‚Ä¢ —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ä–µ–≥—É–ª–∏—Ä—É—é—â–∏–µ —Ç–µ–∫—É—â—É—é (–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é) –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –û–±—â–µ—Å—Ç–≤–∞, –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã (–∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—â–∏—Ö —Å–∏—Å—Ç–µ–º—ã –æ–ø–ª–∞—Ç—ã —Ç—Ä—É–¥–∞ –∏ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ç–Ω–µ—Å–µ–Ω–æ –∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏–Ω—ã—Ö –æ—Ä–≥–∞–Ω–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–∞);\n",
        "\n",
        "       ‚Ä¢ –≤–Ω–æ—Å–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ –û–±—â–∏—Ö —Å–æ–±—Ä–∞–Ω–∏–π —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –û–±—â–µ—Å—Ç–≤–∞, –∑–∞—Å–µ–¥–∞–Ω–∏—è –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤;\n",
        "\n",
        "       ‚Ä¢ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –ø–æ –¥—Ä—É–≥–∏–º –≤–æ–ø—Ä–æ—Å–∞–º, –æ—Ç–Ω–æ—Å—è—â–∏–º—Å—è –∫ –µ–≥–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏.\n",
        "\n",
        "\n",
        "\n",
        "cxlv –ù–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ª—é–±–æ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –£—Å—Ç–∞–≤–∞ –Ω–µ –≤–ª–µ—á–µ—Ç –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏–π. –í —Å–ª—É—á–∞–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ —Å–∏–ª—É –Ω–æ–≤—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤, –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –£—Å—Ç–∞–≤–∞ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏, –∏ –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≤–Ω–µ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –£—Å—Ç–∞–≤, –£—á–∞—Å—Ç–Ω–∏–∫–∏ –æ–±—è–∑–∞–Ω—ã –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏—è –æ –≤–Ω–µ—Å–µ–Ω–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –Ω–∞—Å—Ç–æ—è—â–∏–π –£—Å—Ç–∞–≤.\n",
        "\n",
        "v –ü–æ–ª–Ω–æ–µ —Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:\n",
        "\n",
        "      –û–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é ¬´–Æ–∂–Ω–æ-–ü—Ä–∏–æ–±—Å–∫–∏–π –≥–∞–∑–æ–ø–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–≤–æ–¥¬ª.\n",
        "\n",
        "      –°–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:\n",
        "\n",
        "      –û–û–û ¬´–Æ–∂–Ω–æ-–ü—Ä–∏–æ–±—Å–∫–∏–π –ì–ü–ó¬ª.\n",
        "\n",
        "\n",
        "      –ü–æ–ª–Ω–æ–µ —Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ: Yuzhno-Priobsky Gaz Processing Plant Limited Liability Company.\n",
        "\n",
        "\n",
        "      –°–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ:  Yuzhno-Priobsky GPP LLC.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "GLOBALS__['CharterAnlysingContext'].analyze_charter(SAMPLE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_0s47A-ZhVN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'–ø–∏–∏–∏–∏—É—É—É'[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpYNZGtEJRmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.core.display import display, HTML\n",
        "doc = GLOBALS__['CharterAnlysingContext'].doc\n",
        "GLOBALS__['renderer'].render_contents(doc)\n",
        "\n",
        " \n",
        "GLOBALS__['renderer'].render_charter_parsing_results(GLOBALS__['CharterAnlysingContext'].org,GLOBALS__['CharterAnlysingContext'].constraints)\n",
        "\n",
        "\n",
        "def render_sections(sections):\n",
        "  html = '<h3>–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞</h4>'\n",
        "  html += \"<ul>\"\n",
        "  for section_type in sections:\n",
        "    section:HeadlineMeta = sections[section_type]\n",
        "    body = section.body.untokenize_cc()[:1000]\n",
        "    headline = section.subdoc.untokenize_cc()[:500]\n",
        "    #     line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "    html += f'<li><h3> {headline} <sup>type: {section_type}</sup> </h3> <p>{body}</p> </li>'\n",
        "  html += \"</ul>\"\n",
        "\n",
        "  display(HTML(html))\n",
        "  \n",
        "def render_contents(doc):\n",
        "  html = '<h3>–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞</h4>'\n",
        "  html += \"<ul>\"\n",
        "  for i in doc.structure.headline_indexes:\n",
        "    line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "    html += f'<li> {line} <sup>line {i}</sup></li>'\n",
        "  html += \"</ul>\"\n",
        "\n",
        "  display(HTML(html))\n",
        "  \n",
        "render_sections(doc.sections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tyMy_vPK8GQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TUp36Q8K8ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Upload charter"
      ]
    },
    {
      "metadata": {
        "id": "twCGd5TFcTKI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  uploaded = interactive_upload('Charter')  \n",
        "  org, rz = GLOBALS__['CharterAnlysingContext'] .analyze_charter(uploaded[0])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGRLbJ5kJuaO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_init_contracts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6c9siQi9k_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZVFmJtpK_fG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Upload contract"
      ]
    },
    {
      "metadata": {
        "id": "HEyYz8iPJ-4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = interactive_upload('–î–æ–≥–æ–≤–æ—Ä')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIOjRq7EKQA9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GLOBALS__['ContractAnlysingContext'].verbosity_level=4\n",
        "GLOBALS__['ContractAnlysingContext'].analyze_contract(uploaded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eb6EitqjLF3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Violations"
      ]
    },
    {
      "metadata": {
        "id": "9YWejFEKKUrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title –ù–∞—Å—Ç—Ä–æ–π–∫–∏ { run: \"auto\", display-mode: \"form\" }\n",
        "USD_to_RUB = 64.02 #@param {type:\"number\"}\n",
        "RUB_to_USD = 1.0/USD_to_RUB\n",
        "\n",
        "# print('USD_to_RUB=',USD_to_RUB)\n",
        "# print('RUB_to_USD=',RUB_to_USD)\n",
        "\n",
        "\n",
        "\n",
        "currency_converter = {\n",
        "  'USD': USD_to_RUB,\n",
        "  'RUB': 1.0\n",
        "}\n",
        "\n",
        "print(currency_converter)\n",
        "\n",
        "find_and_show_violations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8igUeguItG3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc=GLOBALS__['ContractAnlysingContext'].contract\n",
        "GLOBALS__['renderer'].render_contents(doc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhF8cq_PKKaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "h = GLOBALS__['renderer'].render_constraint_values(GLOBALS__['CharterAnlysingContext'].constraints)\n",
        "display(HTML(h))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2HHEc7cCZv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#violations (move and erase)"
      ]
    },
    {
      "metadata": {
        "id": "ffboeHGOCdcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from transaction_values import ValueConstraint\n",
        "\n",
        "print('–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π')\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "contract = GLOBALS__['ContractAnlysingContext'].contract\n",
        "charter = GLOBALS__['CharterAnlysingContext'].doc\n",
        "charter_constraints = GLOBALS__['CharterAnlysingContext'].constraints  # XXX: move to doc\n",
        "\n",
        "#       h = GLOBALS__['renderer'].render_constraint_values(GLOBALS__['CharterAnlysingContext'].constraints)\n",
        "#       display(HTML(h))\n",
        "\n",
        "\n",
        "# def max_constraint_val(constraints):\n",
        "renderer = GLOBALS__['renderer']\n",
        "renderer.render_subj(contract)\n",
        "print()\n",
        "# GLOBALS__['renderer'].render_values(contract.contract_values)\n",
        "\n",
        "print(\"----------\")\n",
        "\n",
        "# GLOBALS__['renderer'].render_values([most_confident_value])\n",
        "# GLOBALS__['renderer'].render_color_text (most_confident_value.value.context[0], most_confident_value.value.context[1], _range=[0,1])\n",
        "\n",
        "# for c in contract.contract_values:\n",
        "#   print(c.confidence)\n",
        "#   print(c.value.value)\n",
        "# #   if c.value.value > best_c.value.value:\n",
        "\n",
        "\n",
        "# #   print(c.value.context)\n",
        "#   GLOBALS__['renderer'].render_color_text (c.value.context[0], c.value.context[1], _range=[0,1])\n",
        "\n",
        "# GLOBALS__['renderer'].render_contents(contract)\n",
        "\n",
        "\n",
        "# -------------------charter\n",
        "\n",
        "charter_constraints = GLOBALS__['CharterAnlysingContext'].constraints  # XXX: move to doc\n",
        "\n",
        "\n",
        "from ml_tools import ProbableValue\n",
        "\n",
        "currency_converter = {\n",
        "  'USD': USD_to_RUB,\n",
        "  'RUB': 1.0\n",
        "\n",
        "}\n",
        "import copy\n",
        "\n",
        "\n",
        "def convert(v, currency_converter=currency_converter):\n",
        "  v_converted = copy.copy(v)\n",
        "  if v.currency in currency_converter:\n",
        "    v_converted.value = currency_converter[v.currency] * v.value\n",
        "    v_converted.currency = 'RUB'\n",
        "    return v_converted\n",
        "  else:\n",
        "    display(HTML(as_error_html(\n",
        "      f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "    return v\n",
        "\n",
        "\n",
        "from text_tools import untokenize\n",
        "import numpy as np\n",
        "\n",
        "class VConstraint:\n",
        "  def __init__(self, lower, upper, head_group):\n",
        "    self.lower = ProbableValue( ValueConstraint(0, 'RUB', +1), 0 )\n",
        "    self.upper = ProbableValue( ValueConstraint(np.inf, 'RUB', -1), 0 )\n",
        "    \n",
        "    if lower is not None:\n",
        "      self.lower = lower\n",
        "     \n",
        "\n",
        "    if upper is not None:\n",
        "      self.upper = upper\n",
        "     \n",
        "      \n",
        "    self.head_group = head_group\n",
        "\n",
        "  def maybe_convert(self, v: ValueConstraint, currency_converter):\n",
        "    html = \"\"\n",
        "    v_converted = v\n",
        "    if v.currency != 'RUB':\n",
        "      v_converted = convert(v, currency_converter)\n",
        "      html += as_warning(f\"–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–∞–ª—é—Ç {as_currency(v)} --> RUB \")\n",
        "      html += as_offset(as_warning(f\"–ø—Ä–∏–º–µ—Ä–Ω–æ: {as_currency(v)} ~~  {as_currency(v_converted)}  \"))\n",
        "    return v, v_converted, html\n",
        "\n",
        "  def check_contract_value(self, _v: ProbableValue, currency_converter):\n",
        "    greather_lower = False\n",
        "    greather_upper = False\n",
        "\n",
        "    if _v is None:\n",
        "      return as_error_html(\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –∏–∑–≤–µ—Å—Ç–Ω–∞\")\n",
        "    v: ValueConstraint = _v.value\n",
        "\n",
        "    if v is None:\n",
        "      return as_error_html(\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞\")\n",
        "\n",
        "    if v.value is None:\n",
        "      return as_error_html(f\"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞ {v.currency}\")\n",
        "    ###----\n",
        "\n",
        "    lower_v = None\n",
        "    upper_v = None\n",
        "    if self.lower is not None:\n",
        "      lower_v: ValueConstraint = self.lower.value\n",
        "    if self.upper is not None:\n",
        "      upper_v: ValueConstraint = self.upper.value\n",
        "\n",
        "    html = as_msg(f\"–¥–∏–∞–ø–∞–∑–æ–Ω: {as_currency(lower_v)} < ..... < {as_currency(upper_v)}\")\n",
        "\n",
        "    v, v_converted, h = self.maybe_convert(v, currency_converter)\n",
        "    html += h\n",
        "\n",
        "    if self.lower is not None:\n",
        "      lower_v: ValueConstraint = self.lower.value\n",
        "      lower_v, lower_converted, h = self.maybe_convert(lower_v, currency_converter)\n",
        "      html += h\n",
        "\n",
        "      if v_converted.value >= lower_converted.value:\n",
        "        greather_lower = True\n",
        "        html += as_warning(\"—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ...\".upper())\n",
        "        html += as_warning(\n",
        "          f\"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)}  –ë–û–õ–¨–®–ï –Ω–∏–∂–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(lower_converted)} \")\n",
        "        html += as_quote(untokenize(lower_v.context[0]))\n",
        "\n",
        "    if self.upper is not None:\n",
        "\n",
        "      upper_v: ValueConstraint = self.upper.value\n",
        "      upper_v, upper_converted, h = self.maybe_convert(upper_v, currency_converter)\n",
        "      html += h\n",
        "\n",
        "      if v_converted.value >= upper_converted.value:\n",
        "\n",
        "        html += as_error_html(\n",
        "          f\"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)} –ë–û–õ–¨–®–ï –≤–µ—Ä—Ö–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(upper_converted)} \")\n",
        "\n",
        "      elif greather_lower:\n",
        "        head_name = self.head_group['section']\n",
        "        html += as_error_html(f'—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã \"{head_name.upper()}\"')\n",
        "\n",
        "        if lower_v.context is not None:\n",
        "          html += as_quote(renderer.to_color_text(lower_v.context[0], lower_v.context[1], _range=[0, 1]))\n",
        "        if upper_v.context is not None:\n",
        "          html += '<br>'\n",
        "          html += as_quote(renderer.to_color_text(upper_v.context[0], upper_v.context[1], _range=[0, 1]))\n",
        "\n",
        "    return html\n",
        "\n",
        "\n",
        "# -----------\n",
        "\n",
        "\n",
        "def _combine_constraints_in_group(group_c, verbose=False):\n",
        "  # print(group_c)\n",
        "  # print(group_c['section'])\n",
        "\n",
        "  data = {\n",
        "    'name': group_c['section'],\n",
        "    'ranges': {}\n",
        "  }\n",
        "\n",
        "  sentences = group_c['sentences']\n",
        "  #   print (charter_constraints[head_group]['sentences'])\n",
        "  sentence_id = 0\n",
        "  for sentence in sentences:\n",
        "    constraint_low = None\n",
        "    constraint_up = None\n",
        "\n",
        "    sentence_id += 1\n",
        "    #     print (sentence['constraints'])\n",
        "\n",
        "    s_constraints = sentence['constraints']\n",
        "    # –±–æ–ª—å—à–∏–µ –∏—â–µ–º\n",
        "    maximals = [x for x in s_constraints if x.value.sign > 0]\n",
        "\n",
        "    if len(maximals) > 0:\n",
        "      constraint_low = min(maximals, key=lambda item: convert(item.value, currency_converter).value)\n",
        "      if verbose:\n",
        "        print(\"all maximals:\")\n",
        "        renderer.render_values(maximals)\n",
        "        print('\\t\\t\\t constraint_low', constraint_low.value.value)\n",
        "        renderer.render_values([constraint_low])\n",
        "\n",
        "    minimals = [x for x in s_constraints if x.value.sign <= 0]\n",
        "    if len(minimals) > 0:\n",
        "      constraint_up = min(minimals, key=lambda item: convert(item.value, currency_converter).value)\n",
        "      if verbose:\n",
        "        print(\"all: minimals\")\n",
        "        renderer.render_values(minimals)\n",
        "        print('\\t\\t\\t constraint_upper', constraint_up.value.value)\n",
        "        renderer.render_values([constraint_up])\n",
        "        print(\"----X\")\n",
        "\n",
        "    if constraint_low is not None or constraint_up is not None:\n",
        "      data['ranges'][sentence_id] = VConstraint(constraint_low, constraint_up, group_c)\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def find_ranges_by_group(charter_constraints):\n",
        "  ranges_by_group = {}\n",
        "  for head_group in charter_constraints:\n",
        "    #     print('-' * 20)\n",
        "    group_c = charter_constraints[head_group]\n",
        "    data = _combine_constraints_in_group(group_c)\n",
        "    ranges_by_group[head_group] = data\n",
        "  return ranges_by_group\n",
        "\n",
        "\n",
        "def select_most_confident_if_almost_equal(a: ProbableValue, alternative: ProbableValue, equality_range=0):\n",
        "  if abs(convert(a.value).value - convert(alternative.value).value) < equality_range:\n",
        "    if a.confidence > alternative.confidence:\n",
        "      return a\n",
        "    else:\n",
        "      return alternative\n",
        "  return a\n",
        "\n",
        "\n",
        "def find_contract_best_value(contract):\n",
        "  best_value: ProbableValue = max(contract.contract_values,\n",
        "                                  key=lambda item: convert(item.value, currency_converter).value)\n",
        "\n",
        "  most_confident_value = max(contract.contract_values, key=lambda item: item.confidence)\n",
        "  best_value = select_most_confident_if_almost_equal(best_value, most_confident_value, 20)\n",
        "\n",
        "  return best_value\n",
        "\n",
        "\n",
        "best_value = find_contract_best_value(contract)\n",
        "\n",
        "\n",
        "# rendering:----------------------------\n",
        "\n",
        "\n",
        "def _render_violations(ranges_by_group, best_value):\n",
        "  for group_key in ranges_by_group:\n",
        "    group = ranges_by_group[group_key]\n",
        "    #   print(group['name'])\n",
        "    display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "    for rk in group['ranges']:\n",
        "      r = group['ranges'][rk]\n",
        "      display(HTML(r.check_contract_value(best_value, currency_converter)))\n",
        "\n",
        "\n",
        "print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "renderer.render_values([best_value])\n",
        "renderer.render_color_text(best_value.value.context[0], best_value.value.context[1], _range=[0, 1])\n",
        "\n",
        "_render_violations(\n",
        "  find_ranges_by_group(charter_constraints),\n",
        "  best_value)\n",
        "\n",
        "display(HTML(renderer.render_constraint_values(charter_constraints)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdYrfpIZCnZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## —á–∏—Å—Ç–æ–≤–∏–∫"
      ]
    },
    {
      "metadata": {
        "id": "dvXylOnCCqDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from transaction_values import ValueConstraint\n",
        "\n",
        "print('–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π')\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "contract = GLOBALS__['ContractAnlysingContext'].contract\n",
        "charter = GLOBALS__['CharterAnlysingContext'].doc\n",
        "charter_constraints = GLOBALS__['CharterAnlysingContext'].constraints  # XXX: move to doc\n",
        "\n",
        " \n",
        "renderer = GLOBALS__['renderer']\n",
        "renderer.render_subj(contract)\n",
        "  \n",
        "\n",
        "charter_constraints = GLOBALS__['CharterAnlysingContext'].constraints  # XXX: move to doc\n",
        "\n",
        "\n",
        "from ml_tools import ProbableValue\n",
        "from text_tools import untokenize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def convert(v):\n",
        "  \n",
        "  v_converted = copy.copy(v)\n",
        "  if v.currency in currency_converter:\n",
        "    v_converted.value = currency_converter[v.currency] * v.value\n",
        "    v_converted.currency = 'RUB'\n",
        "    return v_converted\n",
        "  else:\n",
        "    display(HTML(as_error_html(\n",
        "      f\"–º—ã –Ω–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ (–ø–æ–∫–∞) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {v.currency} --> RUB. –≠—Ç–æ –≤–æ–æ–±—â–µ –≤–∞–ª—é—Ç–∞ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω—ã? –†—É–º—ã–Ω–∏–∏?\")))\n",
        "    return v\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "def _combine_constraints_in_group(group_c, verbose=False):\n",
        "  # print(group_c)\n",
        "  # print(group_c['section'])\n",
        "\n",
        "  data = {\n",
        "    'name': group_c['section'],\n",
        "    'ranges': {}\n",
        "  }\n",
        "\n",
        "  sentences = group_c['sentences']\n",
        "  #   print (charter_constraints[head_group]['sentences'])\n",
        "  sentence_id = 0\n",
        "  for sentence in sentences:\n",
        "    constraint_low = None\n",
        "    constraint_up = None\n",
        "\n",
        "    sentence_id += 1\n",
        "    #     print (sentence['constraints'])\n",
        "\n",
        "    s_constraints = sentence['constraints']\n",
        "    # –±–æ–ª—å—à–∏–µ –∏—â–µ–º\n",
        "    maximals = [x for x in s_constraints if x.value.sign > 0]\n",
        "\n",
        "    if len(maximals) > 0:\n",
        "      constraint_low = min(maximals, key=lambda item: convert(item.value, currency_converter).value)\n",
        "      if verbose:\n",
        "        print(\"all maximals:\")\n",
        "        renderer.render_values(maximals)\n",
        "        print('\\t\\t\\t constraint_low', constraint_low.value.value)\n",
        "        renderer.render_values([constraint_low])\n",
        "\n",
        "    minimals = [x for x in s_constraints if x.value.sign <= 0]\n",
        "    if len(minimals) > 0:\n",
        "      constraint_up = min(minimals, key=lambda item: convert(item.value, currency_converter).value)\n",
        "      if verbose:\n",
        "        print(\"all: minimals\")\n",
        "        renderer.render_values(minimals)\n",
        "        print('\\t\\t\\t constraint_upper', constraint_up.value.value)\n",
        "        renderer.render_values([constraint_up])\n",
        "        print(\"----X\")\n",
        "\n",
        "    if constraint_low is not None or constraint_up is not None:\n",
        "      data['ranges'][sentence_id] = VConstraint(constraint_low, constraint_up, group_c)\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def find_ranges_by_group(charter_constraints):\n",
        "  ranges_by_group = {}\n",
        "  for head_group in charter_constraints:\n",
        "    #     print('-' * 20)\n",
        "    group_c = charter_constraints[head_group]\n",
        "    data = _combine_constraints_in_group(group_c)\n",
        "    ranges_by_group[head_group] = data\n",
        "  return ranges_by_group\n",
        "\n",
        "\n",
        "def select_most_confident_if_almost_equal(a: ProbableValue, alternative: ProbableValue, equality_range=0):\n",
        "  if abs(convert(a.value).value - convert(alternative.value).value) < equality_range:\n",
        "    if a.confidence > alternative.confidence:\n",
        "      return a\n",
        "    else:\n",
        "      return alternative\n",
        "  return a\n",
        "\n",
        "\n",
        "def find_contract_best_value(contract):\n",
        "  best_value: ProbableValue = max(contract.contract_values,\n",
        "                                  key=lambda item: convert(item.value, currency_converter).value)\n",
        "\n",
        "  most_confident_value = max(contract.contract_values, key=lambda item: item.confidence)\n",
        "  best_value = select_most_confident_if_almost_equal(best_value, most_confident_value, 20)\n",
        "\n",
        "  return best_value\n",
        "\n",
        "\n",
        "best_value = find_contract_best_value(contract)\n",
        "\n",
        "\n",
        "# rendering:----------------------------\n",
        "\n",
        "\n",
        "def _render_violations(ranges_by_group, best_value):\n",
        "  for group_key in ranges_by_group:\n",
        "    group = ranges_by_group[group_key]\n",
        "    #   print(group['name'])\n",
        "    display(HTML(as_headline_2(group['name'])))\n",
        "\n",
        "    for rk in group['ranges']:\n",
        "      r = group['ranges'][rk]\n",
        "      display(HTML(r.check_contract_value(best_value, currency_converter, renderer)))\n",
        "\n",
        "\n",
        "print(\"–°—É–º–º–∞ –î–æ–≥–æ–≤–æ—Ä–∞:\")\n",
        "renderer.render_values([best_value])\n",
        "renderer.render_color_text(best_value.value.context[0], best_value.value.context[1], _range=[0, 1])\n",
        "\n",
        "_render_violations(\n",
        "  find_ranges_by_group(charter_constraints),\n",
        "  best_value)\n",
        "\n",
        "display(HTML(renderer.render_constraint_values(charter_constraints)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T1D7Zdk9L5GY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#~~~~ Garbage, to be removed"
      ]
    },
    {
      "metadata": {
        "id": "ItuAG1-lClzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4fYh_qZL4Kb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijcdJr7MCXgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# violations "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiDHL0oNGfOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.core.display import display, HTML\n",
        "def render_subj(self, doc):\n",
        "      from demo import subject_types_dict\n",
        "      subj=doc.subject\n",
        "      s_name=subject_types_dict[ subj[0]].upper()\n",
        "      \n",
        "      display(HTML(f'–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:<h3 style=\"margin:0\">{s_name}<sup> {subj[0]}</sup> </h3> confidence:{subj[1]:20,.2f}'))\n",
        "\n",
        "r=GLOBALS__['renderer']\n",
        "# GLOBALS__['renderer'].render_subj = render_subj \n",
        "\n",
        "import types\n",
        "r.render_subj = types.MethodType( render_subj, r )\n",
        "\n",
        "contract = GLOBALS__['ContractAnlysingContext'].contract\n",
        "GLOBALS__['renderer'].render_subj(contract)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KcCp4a1F-gl0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = GLOBALS__['CharterAnlysingContext'].doc\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "def render_contents(doc):\n",
        "  html='<h3>–í—ã—è–≤–ª–µ–Ω–Ω–æ–µ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞</h4>'\n",
        "  html+=\"<ul>\"\n",
        "  for i in doc.structure.headline_indexes:\n",
        "    line = doc.structure.structure[i].to_string(doc.tokens_cc)\n",
        "    html+=f'<li> {line} <sup>line {i}</sup></li>'\n",
        "  html+=\"</ul>\"\n",
        " \n",
        "  \n",
        "  display(HTML(html))\n",
        "    \n",
        "render_contents(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHc0PpyRMq5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(GLOBALS__['ContractAnlysingContext'].contract)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P_2rjssX0KUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# constraints = GLOBALS__['CharterAnlysingContext'].constraints\n",
        "\n",
        "# GLOBALS__['renderer'].render_values(GLOBALS__['ContractAnlysingContext'].contract_values)\n",
        "# GLOBALS__['renderer'].render_charter_parsing_results(GLOBALS__['CharterAnlysingContext'].org, GLOBALS__['CharterAnlysingContext'].constraints)\n",
        "\n",
        "\n",
        "# # for i in \n",
        "\n",
        "# for headkey in constraints:\n",
        "#   cc = constraints[headkey]\n",
        "#   print (cc)\n",
        "#   print (cc['section'])\n",
        "#   print (cc['caption'])\n",
        "  \n",
        "#   for s in cc['sentences']:\n",
        "#     print ('\\t\\t',s['constraints'])\n",
        "#     c = s['constraints']\n",
        "#     for vc in c:\n",
        "#       print(f'\\t\\t\\t {vc.value} \\t {vc.sign} \\t {vc.currency}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}