# -*- coding: utf-8 -*-
"""processing Charters: Value & Org Extraction:batch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JcmP4GgCmHJgMU2a3aIjbBY4uBqQgjoh

# Init
"""

import tensorflow as tf
import tensorflow_hub as hub

print(tf.__version__)
elmo = hub.Module('http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-twitter_2013-01_2018-04_600k_steps.tar.gz', trainable=False) #twitter
# elmo = hub.Module('http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz', trainable=False) #Russian WMT News
# elmo = hub.Module('http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz', trainable=False) #wiki

"""## Import from GitHub"""

!wget https://raw.githubusercontent.com/compartia/nlp_tools/resonance/text_tools.py
!wget https://raw.githubusercontent.com/compartia/nlp_tools/resonance/embedding_tools.py
!wget https://raw.githubusercontent.com/compartia/nlp_tools/resonance/ml_tools.py
!wget https://raw.githubusercontent.com/compartia/nlp_tools/resonance/text_normalize.py  
!wget https://raw.githubusercontent.com/compartia/nlp_tools/resonance/patterns.py  
# !wget https://raw.githubusercontent.com/compartia/nlp_tools/protocols/split.py  
  
  
from patterns import *
from text_tools import *
from text_normalize import *
from embedding_tools import *
from ml_tools import *
# from split import *

"""# Code (common)"""

import glob, os


!pip install docx2txt
!sudo apt-get install antiword
  

import docx2txt, sys, os
from google.colab import files



def read_doc(fn):
  
  text = ''
  try:
    text = docx2txt.process(fn)
  except:
    print("Unexpected error:", sys.exc_info())
    os.system('antiword -w 0 "' + fn + '" > "' + fn + '.txt"')
    with open(fn + '.txt') as f:
      text = f.read()

  return text




#-------

!sudo apt-get install antiword
!pip install docx2txt
import docx2txt, sys, os

from google.colab import files

 

def interactive_upload():
  print('select .docx files:')
  uploaded = files.upload()
  docs=[]
  for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))

    with open(fn, "wb") as df:
      df.write(uploaded[fn])
      df.close()

    # extract text
  
    text = ''
    try:
      text = docx2txt.process(fn)
    except:
      print("Unexpected error:", sys.exc_info())
      os.system('antiword -w 0 "' + fn + '" > "' + fn + '.txt"')
      with open(fn + '.txt') as f:
        text = f.read()
      #os.remove(fn+'.txt') #fn.txt was just to read, so deleting  
  
#     print(text)
    docs.append(text)
    return docs

"""### Profiling"""

def profile(fn):
  @wraps(fn)
  def with_profiling(*args, **kwargs):
    start_time = time.time()

    ret = fn(*args, **kwargs)

    elapsed_time = time.time() - start_time

    if fn.__name__ not in PROF_DATA:
      PROF_DATA[fn.__name__] = [0, []]
    PROF_DATA[fn.__name__][0] += 1
    PROF_DATA[fn.__name__][1].append(elapsed_time)

    return ret

  return with_profiling


def print_prof_data():
  for fname, data in PROF_DATA.items():
    max_time = max(data[1])
    avg_time = sum(data[1]) / len(data[1])
    print("Function {} called {} times. ".format(fname, data[0]))
    print('Execution time max: {:.4f}, average: {:.4f}'.format(max_time, avg_time))


def clear_prof_data():
  global PROF_DATA
  PROF_DATA = {}

"""### Embedder"""

class ElmoEmbedder(AbstractEmbedder):

  def __init__(self, elmo):
    self.elmo = elmo

  def embedd_tokenized_text(self, words, lens):
    with tf.Session() as sess:
      embeddings = self.elmo(
        inputs={
          "tokens": words,
          "sequence_len": lens
        },
        signature="tokens",
        as_dict=True)["elmo"]

      sess.run(tf.global_variables_initializer())
      out = sess.run(embeddings)

    return out, words

  def get_embedding_tensor(self, str, type="elmo", signature="default"):
    embedding_tensor = self.elmo(str, signature=signature, as_dict=True)[type]

    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      embedding = sess.run(embedding_tensor)

    return embedding


embedder = ElmoEmbedder(elmo)

"""## Rendering"""

import matplotlib as mpl
import matplotlib.pyplot as plt

from IPython.core.display import display, HTML


def to_color_text(tokens, weights, colormap='coolwarm', print_debug=False, _range=None):
  #   weights = _weights *-1
  if len(weights) != len(tokens):
    raise ValueError("number of weights differs weights={} tokens={}".format(len(weights), len(tokens)))

  #   if()
  vmin = weights.min()
  vmax = weights.max()

  if _range is not None:
    vmin = _range[0]
    vmax = _range[1]

  if print_debug:
    print(vmin, vmax)

  norm = mpl.colors.Normalize(vmin=vmin - 0.5, vmax=vmax)
  html = ""
  cmap = mpl.cm.get_cmap(colormap)

  for d in range(0, len(weights)):

    html += '<span title="{} {:.4f}" style="background-color:{}">{} </span>'.format(
      d,
      weights[d],
      mpl.colors.to_hex(cmap(norm(weights[d]))),
      tokens[d])

    #     html+='<span style="background-color:' +mpl.colors.to_hex(cmap(norm(weights[d]) ))+ '">' + str(tokens[d]) + " </span>"
    if tokens[d] == '\n':
      html += "<br>"

  return html


def render_color_text(tokens, weights, colormap='coolwarm', print_debug=False, _range=None):
  html = to_color_text(tokens, weights, colormap, print_debug, _range)
  display(HTML(html))


def winning_patterns_to_html(_tokens, ranges, winning_patterns, _range,
                             colormaps=['Reds', 'Purples', 'Blues', 'Greens', 'Greys']):
  vmin = -ranges[1]
  vmax = -ranges[0]

  #     print("winning_patterns_to_html _range", _range, "min max=", ranges)

  norm = mpl.colors.Normalize(vmax=vmax, vmin=vmin)

  cmaps = []

  #     print (colormaps)
  for n in colormaps:
    cmap = mpl.cm.get_cmap(n)
    cmaps.append(cmap)

  html = ""

  for d in _range:
    winning_pattern_i = winning_patterns[d][0]
    colormap = cmaps[winning_pattern_i % len(colormaps)]
    normed = norm(-winning_patterns[d][1])
    color = mpl.colors.to_hex(colormap(normed))
    html += '<span title="' + '{} {:.2f}'.format(d, winning_patterns[d][
      1]) + '" style="background-color:' + color + '">' + str(
      _tokens[d]) + " </span>"
    if _tokens[d] == '\n':
      html += "<br>"

  return html


def _render_doc_subject_fragments(doc):
  #     print(doc.per_subject_distances)

  _html = ""
  if doc.per_subject_distances is not None:

    type = "Договор  благотворительного пожертвования"
    if doc.per_subject_distances[0] > doc.per_subject_distances[1]:
      type = "Договор возмездного оказания услуг"

    _html += "<h3>" + type + "</h3>"

    colormaps = ['PuRd'] * 5 + ['Blues'] * 7 + ['Greys']

    _html += "<h4> Предмет договора:</h4>"

    for region in [doc.subj_range]:
      _html += winning_patterns_to_html(_tokens=doc.tokens, ranges=doc.subj_ranges,
                                        winning_patterns=doc.winning_subj_patterns, _range=region,
                                        colormaps=colormaps)

  return _html





def sum_to_html(result, prefix=''):
  html = ""
  if result is None:
    html += '<h3 style="color:red">СУММА НЕ НАЙДЕНА</h3>'
  else:
    html += '<h3>{}{} {:20,.2f}</h3>'.format(prefix, currencly_map[result[1]], result[0])
  return html


def print_results(_doc, results=None):
  if results is None:
    results = _doc.found_sum

  result, (start, end), sentence, meta = results

  html = "<hr>"

  html += _render_doc_subject_fragments(_doc)

  html += sum_to_html(result)

  for key in meta.keys():
    html += '<div style="font-size:9px">' + str(key) + " = " + str(meta[key]) + "</div>"

  display(HTML(html))
  render_color_text(_doc.tokens[start:end], _doc.sums[start:end])


def render_sections(doc, weights):
  if weights is None:
    weights = np.zeros(len(doc.tokens))
    for subdoc in doc.subdocs:
      weights[subdoc.start] = 1

  fig = plt.figure(figsize=(20, 6))
  ax = plt.axes()
  ax.plot(weights, alpha=0.5, color='green', label='Sections');
  plt.title('sectoins')

  for subdoc in doc.subdocs:
    print(subdoc.filename, '-' * 20)
    render_color_text(subdoc.tokens, weights[subdoc.start:subdoc.end], _range=[0, 1])

"""## Legal Doc Classes"""

import time
from functools import wraps
from typing import List

from patterns import *
from text_normalize import *
from text_tools import *

PROF_DATA = {}

currencly_map = {
  'доллар': 'USD',
  'евро': 'EUR',
  'руб': 'RUR'
}





class LegalDocument(EmbeddableText):

  def __init__(self, original_text=None):
    self.original_text = original_text
    self.filename = None
    self.tokens = None
    self.embeddings = None
    self.normal_text = None
    self.distances_per_pattern_dict = None

    self.right_padding = 10

    # subdocs
    self.start = None
    self.end = None

  def find_sum_in_section(self):
    raise Exception('not implemented')

  def find_sentence_beginnings(self, best_indexes):
    return [find_token_before_index(self.tokens, i, '\n', 0) for i in best_indexes]

  @profile
  def calculate_distances_per_pattern(self, pattern_factory: AbstractPatternFactory, dist_function=DIST_FUNC):
    distances_per_pattern_dict = {}
    for pat in pattern_factory.patterns:
      dists = pat._eval_distances_multi_window(self.embeddings, dist_function)
      if self.right_padding > 0:
        dists = dists[:-self.right_padding]
      # TODO: this inversion must be a part of a dist_function
      dists = 1.0 - dists
      distances_per_pattern_dict[pat.name] = dists
      dists.flags.writeable = False

      # print(pat.name)

    self.distances_per_pattern_dict = distances_per_pattern_dict
    return self.distances_per_pattern_dict

  def subdoc(self, start, end):

    assert self.tokens is not None
    #         assert self.embeddings is not None
    #         assert self.distances_per_pattern_dict is not None

    klazz = self.__class__
    sub = klazz("REF")
    sub.start = start
    sub.end = end
    sub.right_padding = 0

    if self.embeddings is not None:
      sub.embeddings = self.embeddings[start:end]

    if self.distances_per_pattern_dict is not None:
      sub.distances_per_pattern_dict = {}
      for d in self.distances_per_pattern_dict:
        sub.distances_per_pattern_dict[d] = self.distances_per_pattern_dict[d][start:end]

    sub.tokens = self.tokens[start:end]
    return sub

  def split_into_sections(self, caption_pattern_prefix='p_cap_', relu_th=0.5, soothing_wind_size=22):
    """
        this works only for documents where captions are not unique

        :param caption_pattern_prefix: pattern name prefix
        :param relu_th: ReLu threshold
        :param soothing_wind_size: smoothing coefficient (like average window size) TODO: rename
        :return:
        """

    print("WARNING: split_into_sections method is deprecated")

    tokens = self.tokens
    if (self.right_padding > 0):
      tokens = self.tokens[:-self.right_padding]
    # l = len(tokens)

    distances_to_pattern = rectifyed_mean_by_pattern_prefix(self.distances_per_pattern_dict, caption_pattern_prefix,
                                                            relu_th)

    distances_to_pattern = normalize(distances_to_pattern)

    distances_to_pattern = smooth(distances_to_pattern, window_len=soothing_wind_size)

    sections = extremums(distances_to_pattern)
    # print(sections)
    sections_starts = [find_token_before_index(self.tokens, i, '\n', 0) for i in sections]
    # print(sections_starts)
    sections_starts = remove_similar_indexes(sections_starts)
    sections_starts.append(len(tokens))
    # print(sections_starts)

    # RENDER sections
    self.subdocs = []
    for i in range(1, len(sections_starts)):
      s = sections_starts[i - 1]
      e = sections_starts[i]
      subdoc = self.subdoc(s, e)
      self.subdocs.append(subdoc)
      # print('-' * 20)
      # render_color_text(subdoc.tokens, captions[s:e])

    return self.subdocs, distances_to_pattern

  def normalize_sentences_bounds(self, text):
    """
        splits text into sentences, join sentences with \n
        :param text:
        :return:
        """
    sents = ru_tokenizer.tokenize(text)
    for s in sents:
      s.replace('\n', ' ')

    return '\n'.join(sents)

  def preprocess_text(self, text):
    a = text
    #     a = remove_empty_lines(text)
    a = normalize_text(a, replacements_regex)
    a = self.normalize_sentences_bounds(a)

    return a

  def read(self, name):
    print("reading...", name)
    self.filename = name
    txt = ""
    with open(name, 'r') as f:
      self.set_original_text(f.read())

  def set_original_text(self, txt):
    self.original_text = txt
    self.tokens = None
    self.embeddings = None
    self.normal_text = None

  def tokenize(self, _txt=None):
    if _txt is None: _txt = self.normal_text

    _words = tokenize_text(_txt)

    sparse_words = []
    end = len(_words)
    last_cr_index = 0
    for i in range(end):
      if (_words[i] == '\n') or i == end - 1:
        chunk = _words[last_cr_index:i + 1]
        chunk.extend([TEXT_PADDING_SYMBOL] * self.right_padding)
        sparse_words += chunk
        last_cr_index = i + 1

    return sparse_words

  def parse(self, txt=None):
    if txt is None: txt = self.original_text
    self.normal_text = self.preprocess_text(txt)

    self.tokens = self.tokenize()
    return self.tokens
    # print('TOKENS:', self.tokens[0:20])

  def embedd(self, pattern_factory):
    max_tokens = 6000
    if len(self.tokens) > max_tokens:
      self._embedd_large(pattern_factory.embedder, max_tokens)
    else:
      self.embeddings = self._emb(self.tokens, pattern_factory.embedder)

    print_prof_data()

  @profile
  def _emb(self, tokens, embedder):
    embeddings, _g = embedder.embedd_tokenized_text([tokens], [len(tokens)])
    embeddings = embeddings[0]
    return embeddings

  @profile
  def _embedd_large(self, embedder, max_tokens=6000):

    overlap = int(max_tokens / 5)  # 20%

    number_of_windows = 1 + int(len(self.tokens) / max_tokens)
    window = max_tokens

    print(
      "WARNING: Document is too large for embedding: {} tokens. Splitting into {} windows overlapping with {} tokens ".format(
          len(self.tokens), number_of_windows, overlap))
    start = 0
    embeddings = None
    tokens = []
    while start < len(self.tokens):
      #             start_time = time.time()
      subtokens = self.tokens[start:start + window + overlap]
      print("Embedding region:", start, len(subtokens))

      sub_embeddings = self._emb(subtokens, embedder)

      sub_embeddings = sub_embeddings[0:window]
      subtokens = subtokens[0:window]

      if embeddings is None:
        embeddings = sub_embeddings
      else:
        embeddings = np.concatenate([embeddings, sub_embeddings])
      tokens += subtokens

      start += window
      #             elapsed_time = time.time() - start_time
      #             print ("Elapsed time %d".format(t))
      print_prof_data()

    self.embeddings = embeddings
    self.tokens = tokens


class LegalDocumentLowCase(LegalDocument):

  def __init__(self, original_text):
    LegalDocument.__init__(self, original_text)

  def preprocess_text(self, text):
    a = text
    #     a = remove_empty_lines(text)
    a = normalize_text(a,
                       dates_regex + abbreviation_regex + fixtures_regex +
                       spaces_regex + syntax_regex + numbers_regex +
                       formatting_regex + tables_regex)

    a = self.normalize_sentences_bounds(a)

    return a.lower()


class ContractDocument(LegalDocumentLowCase):
  def __init__(self, original_text):
    LegalDocumentLowCase.__init__(self, original_text)
    


def rectifyed_sum_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th=0):
  c = 0
  sum = None

  for p in distances_per_pattern_dict:
    if p.startswith(prefix):
      x = distances_per_pattern_dict[p]
      if sum is None:
        sum = np.zeros(len(x))

      sum += relu(x, relu_th)
      c += 1
  #   deal/=c
  return sum, c


def mean_by_pattern_prefix(distances_per_pattern_dict, prefix):
  #     print('mean_by_pattern_prefix', prefix, relu_th)
  sum, c = rectifyed_sum_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th=0)
  return normalize(sum)


def rectifyed_normalized_mean_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th=0.5):
  return normalize(rectifyed_mean_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th))


def rectifyed_mean_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th=0.5):
  #     print('mean_by_pattern_prefix', prefix, relu_th)
  sum, c = rectifyed_sum_by_pattern_prefix(distances_per_pattern_dict, prefix, relu_th)
  sum /= c
  return sum


def remove_similar_indexes(indexes, min_section_size=20):
  if len(indexes) < 2:
    return indexes

  indexes_zipped = []
  indexes_zipped.append(indexes[0])

  for i in range(1, len(indexes)):
    if indexes[i] - indexes[i - 1] > min_section_size:
      indexes_zipped.append(indexes[i])
  return indexes_zipped



class BasicContractDocument(LegalDocumentLowCase):

  def __init__(self, original_text=None):
    LegalDocumentLowCase.__init__(self, original_text)

  def get_subject_ranges(self, indexes_zipped, section_indexes: List):

    # res = [None] * len(section_indexes)
    # for sec in section_indexes:
    #     for i in range(len(indexes_zipped) - 1):
    #         if indexes_zipped[i][0] == sec:
    #             range1 = range(indexes_zipped[i][1], indexes_zipped[i + 1][1])
    #             res[sec] = range1
    #
    #     if res[sec] is None:
    #         print("WARNING: Section #{} not found!".format(sec))
    #
    # return res

    subj_range = None
    head_range = None
    for i in range(len(indexes_zipped) - 1):
      if indexes_zipped[i][0] == 1:
        subj_range = range(indexes_zipped[i][1], indexes_zipped[i + 1][1])
      if indexes_zipped[i][0] == 0:
        head_range = range(indexes_zipped[i][1], indexes_zipped[i + 1][1])
    if head_range is None:
      print("WARNING: Contract type might be not known!!")
      head_range = range(0, 0)
    if subj_range is None:
      print("WARNING: Contract subject might be not known!!")
      if len(self.tokens) < 80:
        _end = len(self.tokens)
      else:
        _end = 80
      subj_range = range(0, _end)
    return head_range, subj_range

  def find_subject_section(self, pattern_fctry: AbstractPatternFactory, numbers_of_patterns):

    self.split_into_sections(pattern_fctry.paragraph_split_pattern)
    indexes_zipped = self.section_indexes

    head_range, subj_range = self.get_subject_ranges(indexes_zipped, [0, 1])

    distances_per_subj_pattern_, ranges_, winning_patterns = pattern_fctry.subject_patterns.calc_exclusive_distances(
      self.embeddings,
      text_right_padding=0)
    distances_per_pattern_t = distances_per_subj_pattern_[:, subj_range.start:subj_range.stop]

    ranges = [np.nanmin(distances_per_subj_pattern_[:-TEXT_PADDING]),
              np.nanmax(distances_per_subj_pattern_[:-TEXT_PADDING])]

    weight_per_pat = []
    for row in distances_per_pattern_t:
      weight_per_pat.append(np.nanmin(row))

    print("weight_per_pat", weight_per_pat)

    _ch_r = numbers_of_patterns['charity']
    _co_r = numbers_of_patterns['commerce']

    chariy_slice = weight_per_pat[_ch_r[0]:_ch_r[1]]
    commerce_slice = weight_per_pat[_co_r[0]:_co_r[1]]

    min_charity_index = min_index(chariy_slice)
    min_commerce_index = min_index(commerce_slice)

    print("min_charity_index, min_commerce_index", min_charity_index, min_commerce_index)
    self.per_subject_distances = [
      np.nanmin(chariy_slice),
      np.nanmin(commerce_slice)]

    self.subj_range = subj_range
    self.head_range = head_range

    return ranges, winning_patterns

  # TODO: remove
  def __find_sum(self, pattern_factory):

    min_i, sums_no_padding, confidence = pattern_factory.sum_pattern.find(self.embeddings, self.right_padding)

    self.sums = sums_no_padding
    sums = sums_no_padding[:-TEXT_PADDING]

    meta = {
      'tokens': len(sums),
      'index found': min_i,
      'd-range': (sums.min(), sums.max()),
      'confidence': confidence,
      'mean': sums.mean(),
      'std': np.std(sums),
      'min': sums[min_i],
    }

    start, end = get_sentence_bounds_at_index(min_i, self.tokens)
    sentence_tokens = self.tokens[start + 1:end]

    f, sentence = extract_sum_from_tokens(sentence_tokens)

    self.found_sum = (f, (start, end), sentence, meta)

  #     return

  def analyze(self, pattern_factory):
    self.embedd(pattern_factory)
    self._find_sum(pattern_factory)

    self.subj_ranges, self.winning_subj_patterns = self.find_subject_section(
      pattern_factory, {"charity": [0, 5], "commerce": [5, 5 + 7]})


# SUMS -----------------------------

def extract_sum(sentence: str):
  currency_re = re.compile(r'((^|\s+)(\d+[., ])*\d+)(\s*([(].{0,100}[)]\s*)?(евро|руб|доллар))')
  currency_re_th = re.compile(
    r'((^|\s+)(\d+[., ])*\d+)(\s+(тыс\.|тысяч.{0,2})\s+)(\s*([(].{0,100}[)]\s*)?(евро|руб|доллар))')
  currency_re_mil = re.compile(
    r'((^|\s+)(\d+[., ])*\d+)(\s+(млн\.|миллион.{0,3})\s+)(\s*([(].{0,100}[)]\s*)?(евро|руб|доллар))')

  r = currency_re.findall(sentence)
  f = None
  try:
    number = to_float(r[0][0])
    f = (number, r[0][5])
  except:
    r = currency_re_th.findall(sentence)

    try:
      number = to_float(r[0][0]) * 1000
      f = (number, r[0][5])
    except:
      r = currency_re_mil.findall(sentence)
      try:
        number = to_float(r[0][0]) * 1000000
        f = (number, r[0][5])
      except:
        pass

  return f


def extract_sum_from_tokens(sentence_tokens: List):
  sentence = untokenize(sentence_tokens).lower().strip()
  f = extract_sum(sentence)
  return f, sentence


def _extract_sums_from_distances(doc, x):
  maximas = extremums(x)

  results = []
  for max_i in maximas:
    start, end = get_sentence_bounds_at_index(max_i, doc.tokens)
    sentence_tokens = doc.tokens[start + 1:end]

    f, sentence = extract_sum_from_tokens(sentence_tokens)

    if f is not None:
      result = {
        'sum': f,
        'region': (start, end),
        'sentence': sentence,
        'confidence': x[max_i]
      }
      results.append(result)

  return results


def _extract_sum_from_distances____(doc, sums_no_padding):
  max_i = np.argmax(sums_no_padding)
  start, end = get_sentence_bounds_at_index(max_i, doc.tokens)
  sentence_tokens = doc.tokens[start + 1:end]

  f, sentence = extract_sum_from_tokens(sentence_tokens)

  return (f, (start, end), sentence)


def extract_sum_from_doc(doc: LegalDocument, attention_mask=None, relu_th=0.5):
  sum_pos, _c = rectifyed_sum_by_pattern_prefix(doc.distances_per_pattern_dict, 'sum_max', relu_th=relu_th)
  sum_neg, _c = rectifyed_sum_by_pattern_prefix(doc.distances_per_pattern_dict, 'sum_max_neg', relu_th=relu_th)

  sum_pos -= sum_neg

  sum_pos = smooth(sum_pos, window_len=8)
  #     sum_pos = relu(sum_pos, 0.65)

  if attention_mask is not None:
    sum_pos *= attention_mask

  sum_pos = normalize(sum_pos)

  return _extract_sums_from_distances(doc, sum_pos), sum_pos


class ProtocolDocument(LegalDocumentLowCase):

  def __init__(self, original_text=None):
    LegalDocumentLowCase.__init__(self, original_text)

  def make_solutions_mask(self):

    section_name_to_weight_dict = {}
    for i in range(1, 5):
      cap = 'p_cap_solution{}'.format(i)
      section_name_to_weight_dict[cap] = 0.5

    mask = mask_sections(section_name_to_weight_dict, self)
    mask += 0.5
    if self.right_padding > 0:
      mask = mask[0:-self.right_padding]

    mask = smooth(mask, window_len=12)
    return mask

  def find_sum_in_section____(self):
    assert self.subdocs is not None

    sols = {}
    for i in range(1, 5):
      cap = 'p_cap_solution{}'.format(i)

      solution_section = find_section_by_caption(cap, self.subdocs)
      sols[solution_section] = cap

    results = []
    for solution_section in sols:
      cap = sols[solution_section]
      #             print(cap)
      # TODO:
      # render_color_text(solution_section.tokens, solution_section.distances_per_pattern_dict[cap])

      x = extract_sum_from_doc(solution_section)
      results.append(x)

    return results


# Support masking ==================

def find_section_by_caption(cap, subdocs):
  solution_section = None
  mx = 0;
  for subdoc in subdocs:
    d = subdoc.distances_per_pattern_dict[cap]
    _mx = d.max()
    if _mx > mx:
      solution_section = subdoc
      mx = _mx
  return solution_section


def mask_sections(section_name_to_weight_dict, doc):
  mask = np.zeros(len(doc.tokens))

  for name in section_name_to_weight_dict:
    section = find_section_by_caption(name, doc.subdocs)
    #         print([section.start, section.end])
    mask[section.start:section.end] = section_name_to_weight_dict[name]
  return mask


# Charter Docs


class CharterDocument(LegalDocumentLowCase):
  def __init__(self, original_text):
    LegalDocumentLowCase.__init__(self, original_text)
    self.right_padding = 15

  def preprocess_text(self, text):
    a = text
    #     a = remove_empty_lines(text)
    a = normalize_text(a,
                       dates_regex + abbreviation_regex + fixtures_regex + spaces_regex + syntax_regex + cleanup_regex + numbers_regex)
    a = self.normalize_sentences_bounds(a)

    return a.lower()


def max_by_pattern_prefix(distances_per_pattern_dict, prefix, attention_vector=None):
  ret = {}

  for p in distances_per_pattern_dict:
    if p.startswith(prefix):
      x = distances_per_pattern_dict[p]

      if attention_vector is not None:
        x = np.array(x)
        x += attention_vector

      ret[p] = x.argmax()

  return ret


def split_into_sections(doc, caption_indexes):
  sorted_keys = sorted(caption_indexes, key=lambda s: caption_indexes[s])

  doc.subdocs = []
  for i in range(1, len(sorted_keys)):
    key = sorted_keys[i - 1]
    next_key = sorted_keys[i]
    start = caption_indexes[key]
    end = caption_indexes[next_key]
    print(key, [start, end])

    subdoc = doc.subdoc(start, end)
    subdoc.filename = key
    doc.subdocs.append(subdoc)


def split_doc(doc, caption_prefix, attention_vector=None):
  raise Exception('deprecated')
  caption_indexes = max_by_pattern_prefix(doc.distances_per_pattern_dict, caption_prefix, attention_vector)
  for k in caption_indexes:
    caption_indexes[k] = find_token_before_index(doc.tokens, caption_indexes[k], '\n', 0)
  caption_indexes['__start'] = 0
  caption_indexes['__end'] = len(doc.tokens)

  split_into_sections(doc, caption_indexes)

"""# Charter parsing-related code

### Constants
"""

head_types = ['directors', 'all', 'gen', 'pravlenie']

head_types_dict = {  'directors':'Совет директоров', 
                     'all':'Общее собрание участников/акционеров', 
                     'gen':'Генеральный директор', 
#                      'shareholders':'Общее собрание акционеров', 
                     'pravlenie':'Правление общества',
                     'unknown':'*Неизвестный орган управления*'}

head_types_colors = {  'directors':'crimson', 
                     'all':'orange', 
                     'gen':'blue', 
                     'shareholders':'#666600', 
                     'pravlenie':'#0099cc',
                     'unknown':'#999999'}


org_types={
    'org_ao':'Акционерное общество', 
    'org_zao':'Закрытое акционерное общество', 
    'org_oao':'Открытое акционерное общество', 
    'org_ooo':'Общество с ограниченной ответственностью'}

"""## Tools"""

def make_soft_attention_vector(doc, pattern_prefix, relu_th=0.5, blur=60, norm=True):
  assert doc.distances_per_pattern_dict is not None
  attention_vector, _c = rectifyed_sum_by_pattern_prefix(doc.distances_per_pattern_dict, pattern_prefix,
                                                         relu_th=relu_th)
  attention_vector = relu(attention_vector, relu_th=relu_th)

  attention_vector = smooth(attention_vector, window_len=blur)
  attention_vector = smooth(attention_vector, window_len=blur)
  if norm:
    attention_vector = normalize(attention_vector)
  return attention_vector


def soft_attention_vector(doc, pattern_prefix, relu_th=0.5, blur=60, norm=True):
  
  assert doc.distances_per_pattern_dict is not None
  attention_vector, c = rectifyed_sum_by_pattern_prefix(doc.distances_per_pattern_dict, pattern_prefix, relu_th=relu_th)
#   print('soft_attention_vector', c)
  assert c > 0
  attention_vector = relu(attention_vector, relu_th=relu_th)

  attention_vector = smooth(attention_vector, window_len=blur)
  attention_vector = smooth(attention_vector, window_len=blur)
  attention_vector/=c
  if norm:
    attention_vector = normalize(attention_vector)
  return attention_vector



def find_ner_end(tokens, start):
  for i in range(start, len(tokens)):
    if tokens[i] == '"':
      return i

    if tokens[i] == '»':
      return i

    if tokens[i] == '\n':
      return i

    if tokens[i] == '.':
      return i

    if tokens[i] == ';':
      return i

  return min(len(tokens), start + 20)



def _find_sentences_by_attention_vector(doc, attention_vector):

  maxes = extremums(attention_vector)[1:]   
#   print('_find_sentences_by_attention_vector',maxes)
  maxes = doc.find_sentence_beginnings(maxes) 
  maxes = remove_similar_indexes(maxes, 6)
    
  res=[]
  for i in maxes:  
    s,e = get_sentence_bounds_at_index(i+1, doc.tokens)
    if e-s > 0:
      res.append((s,e))

#   print('_find_sentences_by_attention_vector', res)
  return res, attention_vector, maxes

"""## 1.  Patterns Factory 1"""

class CharterPatternFactory(AbstractPatternFactory):

  def create_pattern(self, pattern_name, ppp):
    _ppp = (ppp[0].lower(), ppp[1].lower(), ppp[2].lower())
    fp = FuzzyPattern(_ppp, pattern_name)
    self.patterns.append(fp)
    self.patterns_dict[pattern_name] = fp
    return fp

  def __init__(self, embedder):
    AbstractPatternFactory.__init__(self, embedder)

    self.patterns_dict = {}

    #     self._build_paragraph_split_pattern()
    self._build_order_patterns()
    self._build_head_patterns()
    self._build_sum_margin_extraction_patterns()
    self.embedd()

  def _build_head_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    cp('d_head_all.0', ('к компетенции', 'общего собрания акционеров', 'относятся следующие вопросы'))
    cp('d_head_all.1', ('к компетенции', 'общего собрания участников', 'общества относятся следующие вопросы'))
    cp('d_head_all.2', ('', 'общее собрание участников', 'высшим органом управления общества является общее собрание участников .'))
        
    cp('d_head_directors.1', ('к компетенции', 'совета директоров', 'общества относятся следующие вопросы'))
    
    cp('d_head_pravlenie.1', ('к компетенции', 'правления общества', 'общества относятся следующие вопросы'))
   
    cp('d_head_gen.1', ('к компетенции', 'генерального директора', 'относятся все вопросы руководства текущей деятельностью общества'))
    cp('d_head_gen.1', ('\n', 'генеральный директор', 'общества \n'))

    
    
    cp('negation.1', ('', 'кроме', ''))
    cp('negation.2', ('', 'не', ''))
    cp('negation.3', ('', 'за исключением', ''))
    
        
    cp('organs_1', ('\n', 'органы управления', '.\n органами управления общества являются'))


    
  def _build_order_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    self.create_pattern('d_order_1', ('Порядок', 'одобрения сделок', 'в совершении которых имеется заинтересованность'))
    self.create_pattern('d_order_2', ('', 'принятие решений', 'о совершении сделок'))
    self.create_pattern('d_order_3',
                        ('', 'одобрение заключения', 'изменения или расторжения какой-либо сделки Общества'))
    self.create_pattern('d_order_4', ('', 'Сделки', 'стоимость которой равна или превышает'))
    self.create_pattern('d_order_5', ('', 'Сделки', 'стоимость которой составляет менее'))

    
    
  def _build_sum_margin_extraction_patterns(self):
    suffix = 'млн. тыс. миллионов тысяч рублей долларов копеек евро'
    prefix = 'совершение сделок '

    # less than
    self.create_pattern('sum__lt_1', (prefix + 'стоимость', 'не более 0', suffix))
    self.create_pattern('sum__lt_2', (prefix + 'цена', 'не больше 0', suffix))
    self.create_pattern('sum__lt_3', (prefix + 'стоимость', '< 0', suffix))
    self.create_pattern('sum__lt_4', (prefix + 'цена', 'менее 0', suffix))
    self.create_pattern('sum__lt_4.1', (prefix + 'цена', 'ниже 0', suffix))
    self.create_pattern('sum__lt_5', (prefix + 'стоимость', 'не может превышать 0', suffix))
    self.create_pattern('sum__lt_6', (prefix + 'лимит соглашения', '0', suffix))
    self.create_pattern('sum__lt_7', (prefix + 'верхний лимит стоимости', '0', suffix))
    self.create_pattern('sum__lt_8', (prefix, 'максимум 0', suffix))
    self.create_pattern('sum__lt_9', (prefix, 'до 0', suffix))
    self.create_pattern('sum__lt_10', (prefix, 'но не превышающую 0', suffix))
    self.create_pattern('sum__lt_11', (prefix, 'совокупное пороговое значение 0', suffix))

    # greather than
    self.create_pattern('sum__gt_1', (prefix + 'составляет', 'более 0', suffix))
    self.create_pattern('sum__gt_2', (prefix + '', 'превышает 0', suffix))
    self.create_pattern('sum__gt_3', (prefix + '', 'свыше 0', suffix))
    self.create_pattern('sum__gt_4', (prefix + '', 'сделка имеет стоимость, равную или превышающую 0', suffix))

    self.create_pattern('sumneg_1', ("Размер Уставного капитала Общества составляет", '0', suffix))
 


CharterPF = CharterPatternFactory(embedder)

"""## NER

### 2. NER Pattern factory
"""



class NerPatternFactory(AbstractPatternFactory):

  def create_pattern(self, pattern_name, ppp):
    _ppp = (ppp[0].lower(), ppp[1].lower(), ppp[2].lower())
    fp = FuzzyPattern(_ppp, pattern_name)
    self.patterns.append(fp)
    self.patterns_dict[pattern_name] = fp
    return fp

  def __init__(self, embedder):
    AbstractPatternFactory.__init__(self, embedder)

    self.patterns_dict = {}

    self._build_ner_patterns()
    self.embedd()

    self.ner_pat = None
    try:
      self.ner_pat = self.make_average_pattern('ner_org_')
    except:
      print('NerPatternFactory: error')

  def _build_ner_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    for o_type in org_types.keys():
      cp(o_type, ('', org_types[o_type], '"'))


    cp('ner_org_2', ('Полное фирменное', 'наименование', 'общества на русском языке:'))

    cp('nerneg_1', ('общество имеет', 'круглую печать', ''))
    cp('nerneg_2', ('', 'сокращенное', ''))
    cp('nerneg_3', ('на', 'английском', 'языке'))
    cp('nerneg_4', ('', 'место нахождения общества', ''))


NerPF = NerPatternFactory(embedder)

"""### Detection utils"""

def _get_document_head_having_org_name(txt):
  doc = CharterDocument(txt)
  doc.parse()
  index = find_token_after_index(doc.tokens, 0, "наименование")
  index = max(0, index - 300)

  head = doc.subdoc(index, index + 3000)  # XXX: make smarter range
  return head


def _detect_org_name_section(head, render=False):
  head.calculate_distances_per_pattern(NerPF)

  distances = relu(1 - NerPF.ner_pat._find_patterns(head.embeddings), relu_th=0.3)
  distances = distances[0:-20]

  attention_vector_neg = make_soft_attention_vector(head, 'nerneg_', blur=80)
  attention_vector_neg = attention_vector_neg[0:-20]
  attention_vector_neg = normalize(1 - attention_vector_neg)  # normalize(attention_vector_neg * -1)

  #   attention_vector_org = make_soft_attention_vector(head, 'org_', blur=20) 
  distances = normalize(distances)
  distances *= attention_vector_neg
  distances = normalize(distances)

  head.distances_per_pattern_dict['attention_vector'] = attention_vector_neg
  #   

  idx = np.argmax(distances)
  start = idx - 40
  end = idx + 60

  if render:
    render_color_text(head.tokens[0:1700], distances[0:1700], _range=[0, 1])

  section = head.subdoc(start, end)

  return section


def _detect_org_type_and_name(section, render=False):
  s_attention_vector_neg = section.distances_per_pattern_dict['attention_vector']

  dict_org = {}
  best_type = None
  max = 0
  for org_type in org_types.keys():

    vector = section.distances_per_pattern_dict[org_type] * s_attention_vector_neg
    idx = np.argmax(vector)
    val = section.distances_per_pattern_dict[org_type][idx]
    if val > max:
      max = val
      best_type = org_type

    dict_org[org_type] = [idx, val]

  if render:
    print('_detect_org_type_and_name', dict_org)
    
  return dict_org, best_type




def detect_ners(head, render=False):
  section = _detect_org_name_section(head, render)
  dict_org, best_type = _detect_org_type_and_name(section)

  #   s_attention_vector_neg = normalize(s_attention_vector_neg)

  if render:
    render_color_text(section.tokens, section.distances_per_pattern_dict[best_type], _range=[0, 1])
  #   render_color_text(section.tokens, s_attention_vector_neg, _range=[0,1])

  start = dict_org[best_type][0]
  start = start + len(NerPF.patterns_dict[best_type].embeddings)
  end = 1 + find_ner_end(section.tokens, start)

  section_name = section.subdoc(start, end)

  
  if render:
    print("*" * 40)
    print("Org name:")
  
  
  s_attention_vector_neg = section_name.distances_per_pattern_dict['attention_vector']

  if render:   
    render_color_text(section_name.tokens, s_attention_vector_neg * section_name.distances_per_pattern_dict[best_type],
                      _range=[0, 1])
    print('Org type:', org_types[best_type], dict_org[best_type])

  #   fig = plt.figure(figsize=(20, 6))
  #   ax = plt.axes()

  #   ax.plot(distances  , alpha=0.4, color='black');
  #   ax.plot(attention_vector_neg  , alpha=0.4, color='magenta');
  #   ax.plot( smooth(normalize (distances - attention_vector_neg))  , alpha=0.9, color='red');
  
  rez={
      'type': best_type,
      'name': untokenize(section_name.tokens),
      'type_name': org_types[best_type],
      'tokens': section.tokens,
      'attention_vector': section.distances_per_pattern_dict['attention_vector'] * section.distances_per_pattern_dict[best_type]
  }

  return rez

# process_find_org_name(stext)

"""## Margin values detection

### Split doc into section (поиск разделов )
"""

def find_charter_head_sections(charter, verbose=False):
  indexes = {}
  indexes['__end'] = [len(charter.tokens)-charter.right_padding, 0]
  indexes['unknown'] = [0, 0]
  weights ={}
  
  
  order_v = make_soft_attention_vector(charter, 'd_order_2', relu_th=0.35, blur=60)
  negation_v = soft_attention_vector(charter, 'negation', relu_th=0.4, blur=60)
#   organs_v = soft_attention_vector(charter, 'organs_', relu_th=0.4, blur=110)
  
 
  
  
  weights['unknown'] = order_v
  
  
  
  for head_type in head_types: 
    a = rectifyed_mean_by_pattern_prefix(charter.distances_per_pattern_dict, 'd_head_'+head_type, relu_th=0.4)  
    a = smooth(a, window_len=6)

    a -= order_v/2
#     a -= organs_v
    a -= negation_v/2

    weights[head_type] = normalize(a)

    max_id = np.argmax(a)
    indexes[head_type] = [find_token_before_index(charter.tokens, max_id, "\n"), a[max_id]]

  if verbose:  
    print('indexes=',indexes)  

  # clean collisions:
  for i in indexes:

    if indexes[i] is not None:
      i1, v1 = indexes[i]
      for j in indexes:
        if indexes[j] is not None:
          i2, v2 = indexes[j]
          if i!=j and i1 == i2:
            if verbose:  
              print('COLLIDES', i,j, v1, v2)
            if v1 > v2:
               indexes[j] = None

  if verbose:  
    print('indexes=',indexes)  


  # find sections bounds
  bounds={}
  for i in indexes:   
    if indexes[i] is not None:
      start,v1 = indexes[i]

      mindelta=2**20
      for j in indexes:
        if i!=j:
          if indexes[j] is not None:
            end,v2 = indexes[j]
            delta = end - start
            if delta > 0:
              if delta < mindelta:

                bounds[i] = [start, end]
                mindelta = delta

                
  if verbose:  
    print (bounds)
    
  return bounds, weights

"""### Find constraint values"""

def estimate_threshold(a, min_th=0.3):
  return max(min_th, np.max(a) * 0.7)


def extract_sums_from_tokens(tokens: List, x, verbose=False):
  maximas = extremums(x)

  results = []
  for max_i in maximas:
    start = max_i - 4
    end = min(max_i+15, len(tokens))
    sentence_tokens = tokens[start:end]
    
    if verbose:
      print ('extract_sums_from_tokens', untokenize(sentence_tokens))

    f, sentence = extract_sum_from_tokens(sentence_tokens)

    if f is not None:
      result = {
        'sum': f,
        'region': (start, end),
        'sentence': sentence,
        'confidence': x[max_i]
      }
      results.append(result)

  return results, maximas


def highlight_margin_numbers(_doc, ctx=None, relu_threshold=0.5):
  attention_vector = make_soft_attention_vector(_doc, 'sum_', relu_th=0.35, blur=10)
  attention_vector += make_soft_attention_vector(_doc, 'd_order_', relu_th=0.35, blur=120)

  if ctx is not None:
    attention_vector += ctx

  #   attention_vector_n=normalize(attention_vector)
  estimated_threshold = estimate_threshold(attention_vector) * 0.6
#   print('highlight_margin_numbers estimated_threshold=', estimated_threshold)
  attention_vector_r = relu(attention_vector, relu_th=estimated_threshold)

  return attention_vector_r, attention_vector



def _extract_constraint_values_from_region(subdoc):
  
  sentences = []
  
  
  vector = subdoc.distances_per_pattern_dict['values']
  _sentences_having_currency_numbers, attention_vector, _s_indexes = _find_sentences_by_attention_vector(subdoc, vector)

 
  for i in range(len(_s_indexes)):
    s, e = _sentences_having_currency_numbers[i]

    section = subdoc.subdoc(s, e)
    results, m = extract_sums_from_tokens(section.tokens, section.distances_per_pattern_dict['values'])

    sentence ={}
    sentence['quote']=untokenize(section.tokens)
    constraints = []
    sentence['constraints'] = constraints
    sentence['subdoc'] = section


    for r in results:
      constraints.append( r['sum'] )
      
    sentences.append(sentence)
  return sentences
  
      
  
def extract_constraint_values_from_bounding_boxes(bounds, _doc):
  
  rez ={}

  for head_type in bounds:
#     print('.'*10)
#     print('head_type=',head_type.upper())
    
    rez[head_type] = {}
    r_by_head_type = rez[head_type]
    r_by_head_type['section'] = head_types_dict[head_type]
#     r_by_head_type['constraints'] = []

    b = bounds[head_type]
    r_by_head_type['bounding box'] = b
    r_by_head_type['sentences'] = []
    
    subdoc = _doc.subdoc(b[0], b[1])
    vector, vector_soft = highlight_margin_numbers(subdoc, ctx=None, relu_threshold=0.4)

    subdoc.distances_per_pattern_dict['values'] = vector
 
    
    r_by_head_type['sentences'] =  _extract_constraint_values_from_region(subdoc) 
    
    
  return rez



"""#### Rendering"""

def _render_sentence (sentence):
  html=""
  constraints = sentence['constraints']
  for c in constraints:          
    prefix = "" #" > "
    html += sum_to_html(c, prefix)

#   html+=sentence['quote']
  if len(constraints)>0:
    html += '<div style="border-bottom:1px solid #ccc; margin-top:1em"></div>'
    section = sentence['subdoc']
    html += to_color_text(section.tokens, section.distances_per_pattern_dict['values'], _range=[-0.1,1.5])
  return html
          
          
  
def render_constraint_values (rz):

  html=''
  for head_type in rz.keys():
    
    r_by_head_type = rz[head_type]


    html += '<hr style="margin-top: 45px">'
    html += '<i style="padding:0; margin:0">решения о пороговых суммах, которые принимает</i><h2 style="color:{}; padding:0;margin:0">{}</h2>'.format(
      head_types_colors[head_type],
      head_types_dict[head_type])

        
    sentences = r_by_head_type['sentences']
    html += '<div style="padding-left:80px">'

    if True:
      if len(sentences)>0:
        for sentence in sentences:
          html+=_render_sentence(sentence)

      else:
        html+='<h4 style="color:crimson">Пороговые суммы не найдены или не заданы</h4>'

    html += '</div>'
     
  return html

"""### All together"""

def process_charter(txt, verbose=False):
  
  # Detect 
  head = _get_document_head_having_org_name(txt)
  head.embedd(NerPF)
  
  org = detect_ners(head, render=verbose)
  
  
  charter = CharterDocument(txt)
  charter.parse()
  charter.embedd(CharterPF)
  charter.calculate_distances_per_pattern(CharterPF)
  

  _doc = charter.subdoc(20, len(charter.tokens))
  _bounds, weights = find_charter_head_sections(_doc)
  
  rz = extract_constraint_values_from_bounding_boxes(_bounds, _doc)

#   print ('*** found constraints ***', rz)

  
      
  return org, rz, charter

"""# BATCH"""

# txt = docs[0]
# tf.logging.set_verbosity('FATAL')
# org, rz, charter = process_charter(txt, verbose=False)
# html = render_charter_parsing_results(org, rz)

import glob, os


!pip install docx2txt
!sudo apt-get install antiword
  

import docx2txt, sys, os
from google.colab import files
from google.colab import drive

drive.mount('/content/gdrive', force_remount=True)



from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials

gc = gspread.authorize(GoogleCredentials.get_application_default())




# ----------------


def read_charters():
  texts = {}
  for file in glob.glob("/content/gdrive/My Drive/GazpromOil/Charters/*.doc"):
    try:
      text = read_doc(file)
      texts[file] = text
      print("good:", file)
    except:
      print('WRONG FILE!!', file)

  for file in glob.glob("/content/gdrive/My Drive/GazpromOil/Charters/*.docx"):
    try:
      text = read_doc(file)
      texts[file] = text
      print("good:", file)
    except:
      print('WRONG FILE!!', file)
      
  return texts


charters = read_charters()

#-------------------


#@title  Заполнение таблицы именами файлов { vertical-output: true, form-width: "250px", display-mode: "both" }

sh_name = 'Charter test results'
worksheet = gc.open(sh_name).sheet1  



populate_names = False  #@param {type: "boolean"}

if populate_names:
  worksheet.update_cell( 1, 1, "doc name")

  _row=2  

  # Populate document names
  for filename in sorted(charters):
    head, tail = os.path.split(filename)  
    worksheet.update_cell( _row, 1, tail)

    _row+=1
    
#-------------------
def process_find_org_name(txt):
  
  doc = CharterDocument(txt)
  doc.parse()
  index = find_token_after_index(doc.tokens, 0, "наименование" )
  index = max(0, index-300)

  head = doc.subdoc(index,index+3000) #XXX: make smarter range
  head.embedd(NerPF)
  orginfo  = detect_ners(head)
  return orginfo


def _populate_rz(rz, the_row, worksheet, col):
  for head_type in rz:
    worksheet.update_cell( the_row, col+3, '')
    worksheet.update_cell( the_row, col+4, '')
    worksheet.update_cell( the_row, col+5, '')
    worksheet.update_cell( the_row, col+6, '')
    worksheet.update_cell( the_row, col+7, head_types_dict[head_type])

    r_by_head_type = rz[head_type]




    sentences = r_by_head_type['sentences']
    for sentence in sentences:
      
      constraints = sentence['constraints']
      
      if len(constraints) > 0:
        worksheet.update_cell( the_row, col+10, sentence['quote'])
        
        
        for c in constraints:          
          worksheet.update_cell( the_row, col+8, c[0])
          worksheet.update_cell( the_row, col+9, currencly_map [c[1]] )
    #     if len(constraints)>0:
          the_row +=1 

    the_row +=1 
    
  return the_row

#----------------------



start_from_row = 8  #@param {type: "integer"}

  
# _row=3    

the_row=start_from_row
clear_prof_data
col=1
for _row in range(start_from_row, 2+len(charters)):
  
  
  print("-"*120)
  short_fn = worksheet.cell( _row, 1).value
  
  try:
    the_row = int (worksheet.cell( _row, 2).value)
  except:
    worksheet.update_cell( _row, 2, the_row)

       
  
  filename = '/content/gdrive/My Drive/GazpromOil/Charters/' + short_fn
  print(_row, filename)
  
  txt = charters[filename]
  orginfo = process_find_org_name(txt)
  
   
  worksheet.update_cell( the_row, col+2, short_fn)
  worksheet.update_cell( the_row, col+3, time.strftime("%Y-%m-%d %H:%M"))
  
  worksheet.update_cell( the_row, col+4, orginfo['name'])
  worksheet.update_cell( the_row, col+5, orginfo['type_name'])
  worksheet.update_cell( the_row, col+6, orginfo['type'])
  
  the_row+=1
  charter = CharterDocument(txt)
  charter.parse()
  charter.embedd(CharterPF)
  charter.calculate_distances_per_pattern(CharterPF)
  

  _doc = charter.subdoc(20, len(charter.tokens))
  _bounds, weights = find_charter_head_sections(_doc)
  
  rz = extract_constraint_values_from_bounding_boxes(_bounds, _doc)
  
  the_row = 1 + _populate_rz(rz, the_row, worksheet, col)
  
  
  the_row+=1
  worksheet.update_cell( _row+1, 2, the_row)